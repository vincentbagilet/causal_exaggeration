---
title: "Exaggeration across methods in top economic journals"
description: "In this document, leveraging data form [Brodeur et al. (2020)](https://www.aeaweb.org/articles?id=10.1257/aer.20190687), I examine potential exaggeration in published economics papers and investigate whether we can expect to see more exaggeration of significant results for some of the causal identification strategies than for the others."
output: 
  distill::distill_article:
    toc: true
    float: true
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r setup_comparison, include=FALSE, results='hide', warning=FALSE}
library(knitr)
opts_chunk$set(fig.path = "images/comparison/",
               cache.path = "cache/",
               cache = FALSE,
               echo = TRUE, #set to false to hide code
               message = FALSE,
               warning = FALSE,
               out.width = "85%",
               dpi = 300,
               fig.align = "center",
               dev.args = list(bg="transparent"))  
```  

```{r packages_comparison, message=FALSE, warning=FALSE, code_folding='Show packages used in this document'}
library(tidyverse) 
library(knitr) 
library(mediocrethemes)
library(here)
library(retrodesign)
library(haven)
library(DT)

set_mediocre_all(pal = "coty")
```

## Data and method

### Methods and limitations

[Brodeur et al. (2020)](https://www.aeaweb.org/articles?id=10.1257/aer.20190687) assesses selection on significance in the universe of articles published in 2015 and 2018 in the 25 top economics journals. I leverage this data to investigate statistical power and exaggeration in this literature and then compare them across methods. 

To do so, I first run a naive analysis: I **explore whether the design of each study would enable to retrieve the true effect if it was indeed twice smaller that the one found in the study**.

There is no *a priori* reason to believe that the magnitude of the true effect of a specific study would be half of that of the estimate and not the estimate obtained. I am not claiming that these would be the true effect. Rather, I wonder what would be the power and exaggeration under this reasonable assumption, [Ioannidis et al. (2017)](https://onlinelibrary.wiley.com/doi/abs/10.1111/ecoj.12461) finding a typical exaggeration of two in the economics literature. This approach is also to some extent conservative: hypothesized effect sizes based on exaggerated estimates will be too large and will thus minimize exaggeration.

Note that [Gelman and Carlin (2014)](https://journals.sagepub.com/doi/10.1177/1745691614551642), the seminal article on exaggeration, warns against using estimates from the study as true effects since they might be exaggerated. It instead recommends using estimate from other studies or meta analyses. Since meta-analyses estimates are not readily available for the studies in Brodeur et al. (2020), I however run an analysis where hypothetical true effects are based on the published estimates. This might still be informative if we see important disparities across methods.

Statistical significance is evaluated using p-values and t-statistics in the analysis. This allows avoiding issues if the null is not a null of no effect.

Brodeur et al. (2020) assesses whether significant results are favoured. They find that yes. Therefore, it may make sense to focus on significant estimates only when doing my analysis. Including non-significant estimates would worsen the results since these results have definitely not been selected on significance. Their significance was maybe not even an objective (in the case of placebo tests or when trying to show an absence of effect). Yet, not all significant results were selected on significance. 

### Data

To run the analysis, I first retrieve the Brodeur et al. (2020) data from [OPENIPCSR](https://www.openicpsr.org/openicpsr/project/120246/version/V2/view) and store it locally. I load it and wrangle it. The authors fixed some of their data after the publication of the article (changes explained in the `Changelog.txt` file). I keep the most recent data.

```{r wrangle_brodeur}
data_brodeur_raw <- read_dta(here("inputs", "brodeur_v2", "data", "MM Data.dta"))

data_brodeur <- data_brodeur_raw |> 
  mutate(
    estimate = ifelse(!is.na(mu_new), mu_new, mu_orig),
    estimate = abs(estimate), #sign does not matter here
    se = ifelse(!is.na(sd_new), sd_new, sd_orig)
  ) |> 
  select(method, estimate, se, p_value = pv) |> 
  filter(se > 0)

data_brodeur_signif <- data_brodeur |> 
  filter(p_value <= 0.05)
```

### Brief exploratory data analysis

Then, I quickly explore the raw data set. There is a total of `r nrow(data_brodeur_raw)` estimates from `r length(unique(data_brodeur_raw$title))` articles. `r nrow(data_brodeur_signif)` of these estimates are significant at the 5% confidence level. The distribution of the number of articles across journals is as follows:

```{r explore_nb_journal, code_folding='Show the code used to generate the table'}
data_brodeur_raw |> 
  select(journal, title) |> 
  distinct() |> 
  count(journal) |> 
  arrange(desc(n)) |> 
  DT::datatable(colnames = c("Journal", "Number of articles"))
```

I then display the number of estimates by causal method:

```{r explore_nb_method, code_folding='Show the code used to generate the table'}
data_brodeur_raw |> 
  count(method) |> 
  cbind(data_brodeur_raw |> 
          filter(pv <= 0.05) |> 
          count(method) |> 
          select(n_signif = n)
  ) |> 
  kable(col.names = c("Method", "Number of estimates", "Number of significant estimates"))
```

## Overall power assessment

I then explore the power of the literature to retrieve effects that are equal to half of the published estimate, using the `retrodesign` package. I restrict the analysis to the `r nrow(data_brodeur_signif)` statistically significant estimates. 

```{r power_brodeur}
power_brodeur <- data_brodeur_signif |> 
  mutate(
    retro = map2(estimate/2, se, \(x, y) retro_design_closed_form(x, y)), 
    #retro_design returns a list with power, type_s, type_m
  ) |> 
  unnest_wider(retro) |> 
  mutate(power = power * 100) |> 
  rename(exagg = type_m) 
```

The median power, under this assumption is `r round(median(power_brodeur$power))`% and the median exaggeration `r round(median(power_brodeur$exagg), 1)`. The overall distribution of power is:

```{r graph_distrib_power, code_folding=TRUE}
power_brodeur |> 
  ggplot(aes(x = power)) + 
  # geom_density() + 
  geom_histogram(bins = 100) + 
  labs(
    title = "Distribution of power in the causal inference literature",
    subtitle = "Assuming a true effect size equal to half of the obtained estimate",
    x = "Power",
    y = "Count"
  )
```

Only `r round(mean(power_brodeur$power > 80)*100)`% of the studies would have a power greater than the conventional 80% threshold. For one quarter of the studies, exaggeration would be greater than `r round(quantile(power_brodeur$exagg, 0.75), 1)`.

Importantly, type-S does not seem to be a important concern here, even though it might be for some estimates:

```{r graph_distrib_type_s, code_folding=TRUE}
power_brodeur |> 
  ggplot(aes(x = type_s*100)) + 
  # geom_density() + 
  geom_histogram(bins = 100) + 
  labs(
    title = "Distribution of type-S in the causal inference literature",
    subtitle = "Assuming a true effect size equal to half of the obtained estimate",
    x = "Type S (%)",
    y = "Count"
  )
```

## Comparison across methods

There are two ingredients in the recipe for exaggeration:

1. Publication bias
1. Low statistical power

A low statistical power leads the estimator to be imprecise and statistically significant estimates to be located in the tail of the distribution of estimates and to thus overestimate true effects. Publication bias leads to a larger probability of publishing estimates that come from this non-representative sample of estimates. In the absence of publication bias, low statistical power would not create exaggeration.

In order to explore whether some causal identification methods are more prone to exaggeration than others, we can thus explore which ones are more subject to each of these ingredients. 

### Differences of publication bias across methods

For publication bias, the question has been investigated by [Brodeur et al. (2020)](https://www.aeaweb.org/articles?id=10.1257/aer.20190687). They find that IV and to a lesser extent DiD are particularly problematic in this regards. RDD and RCT seem to be less prone to this issue.

### Differences of statistical power across methods

#### Exaggeration and power

I compute the median power and exaggeration for each identification strategy.

```{r median_power_brodeur, code_folding='Show the code used to generate the table'}
power_brodeur |> 
  group_by(method) |> 
  summarise(
    median_power = median(power),
    median_exagg = median(exagg)
  ) |> 
  rename_with(\(x) str_to_title(str_replace_all(x, "_", " "))) |> 
  kable()
```

There does not seem to be substantial differences in medians. If anything, RCT seem to perform slightly less well than the others. 

I also compute the proportion of studies that would have a power greater that the usual 80% threshold: `r mean(power_brodeur$power > 80)*100`%. The decomposition across methods is the following: 

```{r adequate_power_brodeur, code_folding='Show the code used to generate the table'}
power_brodeur |> 
  group_by(method) |> 
  summarise(
    percent_adequate_power = mean(power > 80)*100
  ) |> 
  rename_with(\(x) str_to_title(str_replace_all(x, "_", " "))) |> 
  kable()
```

I then plot the corresponding distributions of power and exaggeration, split by method

```{r graph_power_brodeur, code_folding='Show the code used to generate the graph', fig.asp=0.7, dpi=300}
power_brodeur |> 
  ggplot(aes(x = power)) +
  geom_histogram(bins = 40) +
  facet_wrap(~method, scales = "free_y") + 
  labs(
    # title = "Distribution of hypothetical statistical power",
    # subtitle = "Comparison across identification strategies",
    x = "Power (%)",
    y = "Count",
    caption = "Hypothetical true effect equal to half of the estimate"
  ) 
  # theme(legend.position = "none")

ggsave(here("images", "comparison", "power_brodeur.pdf"), height = 5, width = 8)

power_brodeur |> 
  ggplot(aes(x = power, fill = method, color = method)) +
  geom_density(alpha = 0.1, adjust = 0.5) +
  # facet_wrap(~method, scales = "free_y") + 
  labs(
    title = "Distribution of hypothetical statistical power",
    subtitle = "Comparison across identification strategies",
    x = "Power (%)",
    y = "Density", 
    caption = "Hypothetical true effect equal to half of the estimate",
    fill = NULL,
    color = NULL
  )

power_brodeur |> 
  filter(exagg < 3) |> 
  ggplot(aes(x = exagg, fill = method, color = method)) +
  geom_density(alpha = 0.1) +
  # facet_wrap(~method) + 
  labs(
    # title = "Distribution of hypothetical exaggeration",
    # subtitle = "Comparison across identification strategies",
    x = "Exaggeration ratio",
    y = "Density", 
    caption = "Hypothetical true effect equals to half of the estimate
    Estimates with an exaggeration ratio greater than 3 are filtered out",
    fill = NULL,
    color = NULL
  )

ggsave(here("images", "comparison", "exagg_brodeur.pdf"), height = 5, width = 8)
```

The results are rather similar across strategies. If anything, we notice that IV and RCT perform slightly less well than DID and RDD.

However, if there is initially more exaggeration for one of the methods, considering hypothetical true effect sizes equal to half of the observed estimates will be dismissive since we would not be comparing similar things.

I also look at the distribution of the Signal-To-Noise Ratios in the samples;

```{r}
data_brodeur |> 
  mutate(snr = abs(estimate/se)) |> 
  ggplot(aes(x = snr)) + 
  geom_histogram(bins = 200) + 
  scale_x_log10() + 
  facet_wrap(~ method) +
  labs(
    title = "Distribution of SNR",
    subtitle = "Comparison across methods",
    x = "Signal-to-Noise Ratio",
    y = "Count"
  )
```








