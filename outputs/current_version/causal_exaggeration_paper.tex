\documentclass[usletter, 12pt]{article}
 
\usepackage{VB}
\usepackage{cleveref}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{caption}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}


%\doublespacing

\begin{document}

\lhead{\textsc{Causal Exaggeration}}

\selectlanguage{english}
	
	\title{Causal Exaggeration: \\ Unconfounded but Inflated Causal Estimates}

	\author{Vincent Bagilet
		\thanks{Columbia University, New York, USA. Email: \url{vincent.bagilet@columbia.edu}. 
			A previous version of this paper (\textit{CEEP Working Paper Series}, 20) was co-authored with Léo Zabrocki-Hallak; I cannot thank him enough for his invaluable and far-reaching contributions to the project. I am very grateful to Jeffrey Shrader for his guidance and thank Sylvain Chabé-Ferret, Clément De Chaisemartin, Jesse McDevitt-Irwin, David McKenzie, José Luis Montiel Olea, Hélène Ollivier, Suresh Naidu, Claire Palandri, Julian Reif, Stephan Thies and Roberto Zuniga Valladares for helpful comments, as well as lab members at Columbia and seminars participants at Columbia, IPWSD, the Paris School of Economics and the Toulouse School of Economics.}
	}
	
	\institution{Sustainable Development PhD Program, Columbia University}

	\date{January 19, 2023}
	
	\maketitle
	
	\begin{center}
		\large \textsc{\textbf{Abstract}}\\
	\end{center}
	
	\input{abstract/causal_exaggeration_abstract.tex}
		
			
	\begin{center}
		\href{https://github.com/vincentbagilet/causal_exaggeration/blob/main/outputs/current_version/causal_exaggeration_paper.pdf}{Link to the most recent version of the paper}
	\end{center}
	
	
	\newpage
	
%-----------------------------------------------------------------------------

% INTRODUCTION

%-----------------------------------------------------------------------------
	
	\section{Introduction}
	
		\input{intro/causal_exaggeration_intro_text.tex}
	
	

%-----------------------------------------------------------------------------

% MATHS

%-----------------------------------------------------------------------------
	
	\section{Mathematical derivation} \label{maths}
						
		In this section, I formally prove the existence of the confounding-exaggeration trade-off  and describe its drivers in a simple setting.\footnote{A more detailed version of this mathematical derivation is available on the \href{https://vincentbagilet.github.io/causal_exaggeration/Maths/math_causal_exaggeration.pdf}{companion website}.} To do so, I first define an exaggeration ratio  and show that it increases with the variance of normally distributed estimators. This leads me to computing the asymptotic distributions of a series of estimators in order to prove their normality and study drivers of their variances, and ultimately of their exaggeration ratios. Finally, I show that, for any magnitude of OVB, exaggeration can be greater when using a causal inference method than the overall bias combining exaggeration and OVB in the naive regression.
		
%%%%%%%%%%%%%%  Exagg ratio  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		\subsection{Properties of the exaggeration ratio}\label{general_exagg}
		
			Following  \cite{gelman_beyond_2014}, we can define the exaggeration ratio $E$, as the expectation of the absolute value of significant estimates over the absolute value of the true effect. For an estimator $\hat{\beta}$ of a true effect $\beta$, with standard deviation $\sigma$ and a two-sided hypothesis test of size $\alpha$ with threshold value $z_{\alpha}$, let
			
			%Let's define an exaggeration ratio $E$, as the expected value of significant estimates over the true effect\footnote{Note that \cite{gelman_beyond_2014} define exaggeration as the expected value of the \textit{absolute value} of the estimate: $\mathbb{E}[ | \hat{\beta}| | \beta, \sigma, | \hat{\beta} | > z_{\alpha} \sigma ] / | \beta |$. Problematically, this expression, when not conditioning on significance, \textit{i.e.} $\mathbb{E}[ | \hat{\beta}| | \beta, \sigma ] / | \beta |$, is not equal to 1 for unbiased estimators, if statistical power is low. The alternative definition I use satisfies this key condition. My exaggeration ratio is also smaller or equal to \citeauthor{gelman_beyond_2014}'s and thus more conservative.}. Formally, we have:
		 
				 \begin{equation}\label{exagg_general}
				 	E(\hat{\beta}, \sigma, \beta, z_{\alpha}) =
					 	\dfrac{\mathbb{E}\left[ | \hat{\beta} | \big| \beta, \sigma, |\hat{\beta}| > z_{\alpha} \sigma \right]}{| \beta |} 
				\end{equation}
				
			\cite{lu_note_2019} and \cite{zwet_significance_2021} showed that, for given test and true effect sizes, the exaggeration ratio increases with the variance of a unbiased normally distributed estimator. We can extend this proof to biased estimators and get that:\footnote{All the proofs of the lemma and theorems are in appendix \ref{maths_proofs}.}\label{lemma_evol_exagg}
			
			\begin{lemma}
			
				For an estimator $\hat{\beta_{b}}  \sim \mathcal{N}(\beta + b, \sigma^{2})$ of a true effect of magnitude $\beta$ and a fixed bias $b$ of the same sign as and independent from the true effect,
				
				\begin{itemize}
					\item $E$ is a decreasing function of the Signal-to-Noise Ratio (SNR), $\frac{\beta}{\sigma}$, and only depends on $\sigma$ through this SNR. 
					\item  $\lim_{\sigma\to \infty} E(\hat{\beta_{b}}, \sigma, \beta, z_{\alpha}) = +\infty$.
				\end{itemize}
			\end{lemma}
			
				Figure \ref{fig:graph_exag} provides a clear intuition for these results in the unbiased case. Note that here, we focus on cases in which the bias is in the same direction as the true effect so that exaggeration from causal inference methods and OVB do not cancel each other.\\
				 
				 Based on lemma \ref{lemma_evol_exagg}, to study how exaggeration evolves with the IV strength in an IV setting, the number of exogenous shocks in a reduced form and the correlation between the explanatory variable of interest and the omitted variable of interest, we only have to show asymptotic normality and study how the variance of these estimators evolves with these parameters.
				 				 
		\subsection{Setting and data generating process}\label{maths_dgp}
		
			Consider a usual linear homoskedastic regression model with an omitted variable. For any individual $i \in \{1, ..., n\}$, we write:
				~
				\begin{equation}\label{maths_dgp_y}
					y_i = \beta_{0} + \beta_{1}x_{i} + \delta w_{i} + u_i
				\end{equation}
				
				where $y$ is the outcome, $x$ the explanatory variable,  $w$ an unobserved omitted variable, $u$ an unobserved error term.  $(\beta_0, \beta_1, \delta) \in \mathbb{R}^{3}$ are unknown parameters. $\beta_1$ is the parameter of interest.\\
			
			Assume homogeneous treatment effects and homoskedasticity, along with the usual OLS assumptions (\textit{i.i.d.} observations, finite second moments, positive-definiteness of $\mathbb{E}[\text{x}_i \text{x}_i']$---with $\text{x}_i = (1, x_i)'$--- and $u_{i}$ conditional mean-zero and uncorrelated with $x_i$ and $w_i$). Assume that $w_{i}$ is unobserved, correlated with $x_{i}$ and that $\delta \neq 0$. To simplify the derivations, I further assume that the unobserved variable is centred, \textit{i.e.} $\mathbb{E}[w_i] = 0$. I also assume that the variance of the component of $w_{i}$ that is orthogonal to $x_{i}$ (denoted $w_{i}^{\perp x}$) does not vary with $x_{i}$, \textit{i.e.}, $\text{Var}(w_{i}^{\perp x}|x_{i}) = \text{Var}(w_{i}^{\perp x})$. Consider the following data generating process for $x_i$:
				~
				\begin{equation}\label{maths_dgp_x}
					x_i = \mu_{x} + \gamma w_{i} + \epsilon_i
				\end{equation}
				
				where $\gamma \in \mathbb{R}^{*}$ since $x$ and $w$ are correlated. Set $\rho_{xw} = \text{corr}(x, w) =  \frac{\gamma \sigma_{w}}{\sigma_{x}}$. In the IV and reduced form sections, I further assume that there exists a valid instrumental variable $z_i$ for $x_i$, \textit{i.e.} that $\mu_x + \epsilon_{i} = \pi_0 + \pi_1 z_i + e_{i}$  where $(\pi_0, \pi_1) \in \mathbb{R}^{2}$ are unknown parameters. The existence or not of this valid instrument does not affect the results in the controlled and \textsc{OVB} cases. Since the instrument is valid, it satisfies exogeneity, \textit{ie} $\mathbb{E}[\text{z}_{i}u_{i}] = 0$ and  $\mathbb{E}[z_{i}w_{i}] = 0$, relevance, \textit{ie} $\text{rank}(\mathbb{E}[\text{z}_{i}\text{x}_i']) = 2$,  and positive-definiteness of $\mathbb{E}[\text{z}_i \text{z}_i']$. The data generating process for $x_i$ becomes:			
				~
				\begin{equation}\label{maths_dgp_x_iv}
					x_i = \pi_0 + \pi_1 z_i + \gamma w_{i} + e_{i}
				\end{equation}
				
				I assume that $e_{i}$ is uncorrelated with $z_i$ and $w_{i}$, \textit{ie} $\mathbb{E}[z_ie_{i}] = 0$ and $\mathbb{E}[w_ie_{i}] = 0$. I also assume homoskedasticity for this term, such that $\mathbb{E}[e_{i}^{2} | z_{i}, w_{i}] = \sigma_{e}^{2}$ is constant.\\ %It implies that  $\mathbb{E}[e_{i}^{2} | z_{i}] = \sigma_{e}^{2}$ (since $\mathbb{E}[e_{i}^{2} | z_{i}] =  \mathbb{E}[\mathbb{E}[e_{i}^{2} | z_{i}, w_{i}] | z_{i}] = \mathbb{E}[\sigma_{e}^{2} | z_{i}] = \sigma_{e}^{2})$.\\
				
				Overall, this DGP is close to the usual textbook one but with an additional omitted variable. The Directed Acyclic Graph (DAG) in figure \ref{DAG} represents the data generating process.
			
			 \begin{figure}[!h] 
                    			\begin{center}
                    				\caption{DAG of the data generating process}
                    				\label{DAG}
                    				\includegraphics[width=0.6\linewidth]{images/DAG_maths.png}
                                   \caption*{\footnotesize \textit{Notes}: for clarity the error terms are represented in this graph, in beige. Model parameters are noted as edges labels.}
                                    \end{center}
				\vspace{-1cm}
                    		\end{figure} 

				
%%%%%%%%%%%%  ASYMPTOTIC DISTRIB  %%%%%%%%%%%%%%%%%%%%%%%%
		
		\subsection{Asymptotic distributions of the estimators}\label{maths_asymptotics}			 
			
			I now derive the asymptotic distributions of the various estimators. For each model, the goal is to show asymptotic normality and to study the evolution of estimator variances with the value of the parameter of interest, \textit{i.e.}, a measure of the correlation between $x$ and $w$ ($\gamma$) in the controlled case, of the IV strength ($\pi_{1}$) in the IV case and of the number of exogenous shocks ($\sigma_{z}^{2}$ when $z$ is a dummy) in the reduced form case.  %Since variables and parameters are intertwined, modifying the value of one of them can affect the value of the others. 
			In order for the variation of one factor not to impact other factors of interest, I consider the variances of the variables ($\sigma_{y}^{2}, \sigma_{x}^{2}, \sigma_{w}^{2} \text{ and } \sigma_{z}^{2}$) as fixed but adjust for the variances of the error terms ($\sigma_{u}^{2} \text{ and } \sigma_{\epsilon}^{2}$) when varying the values of one of the parameters ($\gamma, \delta \text{ and } \pi_{1}$). This corresponds to thinking in terms of shares of the variance of $x$ and $y$ explained by ``defined'' variables (\textit{i.e.}, observed variables and $w$) \textit{versus} by residuals. Finally note that comparison between cases with and without OVB for different parameter values is only relevant if varying the parameter of interest does not affect the OVB. I thus make comparative statics analyses at bias fixed, \textit{i.e.}, as shown below, for $\gamma \delta = \kappa = cst$.
			
%%%%%%%%%%%%%%  OVB  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

			\subsubsection{Naive regression (OVB)}\label{formal_proof_ovb}
			
%				First, consider a ``naive'' regression of $y$ on $x$ (with $w$ omitted). The formula for the bias can easily computed using textbook algebra. The intuition for the formula of the main parameter of interest, the asymptotic variance, has been discussed in the introduction: as in the standard case, the variance of the estimator is given by the ratio of the variance in $y$ that is not explained by $x$ ($\sigma^{2}_{y^{\perp x}}$), over the variance of $x$ multiplied by the number of observations. The only difference with the standard case is the presence of the omitted variable $w$. From equation \ref{maths_dgp_y}, it is clear that $\sigma^{2}_{y^{\perp x}} = \sigma^{2}_{u^{\perp x}} + \delta^{2}\sigma^{2}_{w^{\perp x}}$. A more rigorous derivation described in appendix \ref{maths_proofs} leads to the following lemma:
%				%Since, $w$ is unobserved, we consider the projection of $y$ on $X$ only, where $X = (\text{x}_{1}', ..., \text{x}_{n}')'$ with $\forall i, \text{x}_{i}' = (1, x_{i})$
%%					~
%%					\begin{equation}\label{maths_eq_ovb}
%%						y = X\bm{\beta}_{\textsc{ovb}} + u_{\textsc{ovb}}
%%					\end{equation}
%%				
%%				with, by definition of the projection, $\mathbb{E}[X'u_{\textsc{ovb}}] = 0$.
%						
%				\begin{lemma}\label{lemma_ovb}
%					Based on the data generating process described in section \ref{maths_dgp}, for $\hat{\beta}_{\textsc{ovb}}$ the OLS estimate of $\beta_{1}$ in the regression of $y$ on $x$, $\hat{\beta}_{\textsc{ovb}} \overset{d}{\to} \mathcal{N}(\beta_{1} + b_{\textsc{ovb}} , \ \sigma_{\textsc{ovb}}^{2})$, with
%					\[
%						b_{\textsc{ovb}} = \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}} 
%						\qquad \text{and} \qquad
%						\sigma_{\textsc{ovb}}^{2} = \dfrac{\sigma_{u}^{2} + \delta^{2} \sigma_{w}^{2}(1 - \rho_{xw}^{2})}{n \ \sigma_{x}^{2}}
%						%\sqrt{n}\left( \hat{\beta}_{\textsc{ovb}} - \left(\beta_{1} + \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}}\right) \right)
%						 %\overset{d}{\to} \mathcal{N}\left( 0 , \ \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} (1 - \rho_{xw}^{2} )}{\sigma_{x}^{2}} \right) 
%					\]
%				\end{lemma}
%				
%			Note that since $\sigma_{x}^{2}$ and $\sigma_{w}^{2}$ are fixed, reasoning at $b_{\textsc{ovb}} = cst$ is equivalent to considering that $\gamma \delta = \kappa = cst$. Then, noting that $\forall i, u_{i} = y_i - \beta_{0} - \beta_{1}x_{i} - \delta w_{i}$ and computing its variance, we can rewrite the variance of the estimator as a function of fixed variances and one or less varying parameter:
			
				First, let us study the benchmark against which we are going to compare our causal approaches. Consider the ``naive'' regression of $y$ on $x$ (with $w$ omitted). 
				%Since, $w$ is unobserved, we consider the projection of $y$ on $X$ only, where $X = (\text{x}_{1}', ..., \text{x}_{n}')'$ with $\forall i, \text{x}_{i}' = (1, x_{i})$
%					~
%					\begin{equation}\label{maths_eq_ovb}
%						y = X\bm{\beta}_{\textsc{ovb}} + u_{\textsc{ovb}}
%					\end{equation}
%				
%				with, by definition of the projection, $\mathbb{E}[X'u_{\textsc{ovb}}] = 0$.
						
				\begin{lemma}\label{lemma_ovb}
					Based on the data generating process described in section \ref{maths_dgp}, for $\hat{\beta}_{\textsc{ovb}}$ the OLS estimate of $\beta_{1}$ in the regression of $y$ on $x$, $\hat{\beta}_{\textsc{ovb}} \overset{d}{\to} \mathcal{N}(\beta_{1} + b_{\textsc{ovb}} , \ \sigma_{\textsc{ovb}}^{2})$, with
					\[
						b_{\textsc{ovb}} = \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}} 
						\qquad \text{and} \qquad
						\sigma_{\textsc{ovb}}^{2} = \dfrac{\sigma_{u}^{2} + \delta^{2} \sigma_{w}^{2}(1 - \rho_{xw}^{2})}{n \ \sigma_{x}^{2}}
						%\sqrt{n}\left( \hat{\beta}_{\textsc{ovb}} - \left(\beta_{1} + \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}}\right) \right)
						 %\overset{d}{\to} \mathcal{N}\left( 0 , \ \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} (1 - \rho_{xw}^{2} )}{\sigma_{x}^{2}} \right) 
					\]
				\end{lemma}
				
			The intuition for the formula of the asymptotic variance has been discussed in the introduction: $\sigma_{u}^{2} + \delta^{2} \sigma_{w}(1 - \rho_{xw}^{2})$ is the part of the variance in $y$ that is not explained by $x$ ($\sigma^{2}_{y^{\perp x}}$).\\
			
			Note that, varying the parameter of interest, $\rho_{xw}$, will change the bias and $\sigma_{u}^{2}$. Since $\sigma_{x}^{2}$ and $\sigma_{w}^{2}$ are fixed, reasoning at $b_{\textsc{ovb}} = cst$ is equivalent to considering that $\gamma \delta = \kappa = const$. Then, noting that $\forall i, u_{i} = y_i - \beta_{0} - \beta_{1}x_{i} - \delta w_{i}$ and computing its variance, we can rewrite the variance of the estimator as a function of fixed variances and one or less varying parameter:
			~
			\[
				\sigma_{\textsc{ovb}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\sigma_{x}^{2} - 2\beta_{1}\kappa\sigma_{w}^{2} - \kappa^{2}\frac{\sigma_{w}^{4}}{\sigma_{x}^{2}}}{n \ \sigma_{x}^{2}}
			\]
			
			 This expression underlines that, for a given bias,  $\sigma_{\textsc{ovb}}^{2}$ does not vary with $\gamma$, or equivalently $\delta$, the parameters of interest. Applying lemma \ref{lemma_evol_exagg} proves that $E_{\textsc{ovb}}$ does not either.

%%%%%%%%%%%%%%  CTRL  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\subsubsection{Controlled regression}
			
				Next, let us turn to the ``ideal'' case in which no variable is omitted, \textit{i.e.} we control for the omitted variable $w$ and thus partial out confounders. The model considered accurately represents the DGP.
%				~
%				\begin{equation}\label{maths_eq_ovb}
%					y = X_{w}\bm{\beta}_{\textsc{ctrl}} + u_{\textsc{ctrl}}
%				\end{equation}
%				
				%with $X_{w} = (\text{x}_{w, 1}', ..., \text{x}_{w, n}')'$ with $\forall i, \text{x}_{w, i}' = (1, x_{i}, w_{i})$ and $\mathbb{E}[X_{w}'u] = 0$. 
				This corresponds to the usual OLS setting with a constant and two regressors that are uncorrelated with the error: $y$ regressed on $x$ and $w$.
						
				\begin{lemma}\label{lemma_ctrl}
					Based on the data generating process mentioned previously, for $\hat{\beta}_{\textsc{ctrl}}$ the OLS estimator of $\beta_{1}$ in the regression of $y$ on $x$ and $w$, $\hat{\beta}_{\textsc{ctrl}} \overset{d}{\to} \mathcal{N}(\beta_{1}, \ \sigma_{\textsc{ctrl}}^{2})$, with
					\[
						\sigma_{\textsc{ctrl}}^{2} = \dfrac{\sigma_{u}^{2}}{n \ \sigma_{x}^{2} (1 - \rho_{xw}^{2})}
					\]
				\end{lemma}
				
				Note that $\sigma_{x}^{2} (1 - \rho_{xw}^{2})$ is the part of the variance of $x$ that is not explained by $w$ ($\sigma^{2}_{x^{\perp w}}$) and $\sigma_{u}^{2}$ the part of the variance of $y$ that is not explained by $x$ nor $w$ ($\sigma^{2}_{y^{\perp x, w}}$); here too we retrieved a result described in introduction. For a given bias, we then rewrite $\sigma_{\textsc{ctrl}}^{2}$ as a function of fixed variances and one varying parameter, $\gamma$:
				~
				\[
					\sigma_{\textsc{ctrl}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\sigma_{x}^{2} - \frac{\kappa^{2}}{\gamma^{2}}\sigma_{w}^{2} - 2\beta_{1}\kappa \sigma_{w}^{2}}{n \ (\sigma_{x}^{2}  - \gamma^{2} \sigma_{w}^{2})}
				\]
				
				Since the numerator and denominator respectively increase and decrease with $|\gamma|$,  $\sigma_{\textsc{ctrl}}^{2}$ increases with $|\gamma|$. For a given bias, the more $w$ is correlated with $x$ (and thus roughly the less it is with $y$ since $\delta \gamma = const$), the larger the variance of the estimator. In addition, we can note that, for a given bias, the variance of the estimator can be arbitrarily large since $\lim_{\gamma^{2} \to \frac{\sigma_{x}^{2}}{\sigma_{w}^{2}}} \sigma_{\textsc{ctrl}}^{2} = + \infty$.
				
			
%%%%%%%%%%%%%%  IV   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
		
		\subsubsection{Instrumental Variables}
		
			In the previous section, we considered a case in which we removed variation that included unwanted endogenous variation. We now turn to the IV, a converse situation where we select variation we want, exogenous variation. We estimate the IV model in which we regress $y$ on $\text{x}_{i} = (1, x_{i})'$ instrumented by $\text{z}_{i} = (1, z_{i})'$. We are thus in a just-identified case and $\hat{\bm{\beta}}_{\textsc{2sls}} = \hat{\bm{\beta}}_{\textsc{iv}}$.						
			\begin{lemma}\label{lemma_iv}
				Based on the data generating process mentioned above, for $\hat{\beta}_{\textsc{iv}}$ the IV estimator of $\beta_{1}$ in the regression of $y$ on $x$ instrumented by $z$,  $\hat{\beta}_{\textsc{iv}} \overset{d}{\to} \mathcal{N}(\beta_{1}, \ \sigma_{\textsc{iv}}^{2})$, with
					\[
						\sigma_{\textsc{iv}}^{2} = \dfrac{\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}}{n \ \sigma_{x}^{2} \rho_{xz}^{2}}
					\]
			\end{lemma}
			
			Note that the numerator is $\sigma_{y \perp \hat{x}}^{2}$, the part of the variance in $y$ that is not explained by $\hat{x}$, the predicted value of $x$ in the first stage and the denominator is $\sigma_{\hat{x}}^{2}$. For a given bias, noting that $\rho_{xz} = \text{corr}(x, z) = \pi_1 \frac{\sigma_z}{\sigma_x}$ and replacing $\sigma_{u}^{2}$, we can rewrite $\sigma_{\textsc{iv}}^{2}$ as a function of fixed variances and one varying parameter, $\pi_{1}$:
			\[
				\sigma_{\textsc{iv}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\sigma_{x}^{2} - 2\beta_{1}\kappa \sigma_{w}^{2}}{n \ \pi_{1}^{2} \sigma_{z}^{2}}
			\]
			
			Clearly, the smaller $\pi_{1}$, the larger $\sigma_{\textsc{iv}}^{2}$. In addition, $\lim_{\pi_{1}\to 0} \sigma_{\textsc{iv}}^{2} = + \infty$.
			

			%We retrieve the usual result regarding the determinant of the 2SLS variance as discussed in \cite{hansen_econometrics_2022} for instance. %p354
			 %The variance of the estimator increases when $\sigma_u^{2}$, the variance of the error, increases. It decreases with  $\rho_{xz}$, the correlation between $x$ and $z$, and with $\sigma_{x}^{2}$ the variance of $x$, assuming the omitted variable fixed. 
				 
%%%%%%%%%%%%%%  Reduced form. %%%%%%%%%%%%%%%%%%	

		\subsubsection{Reduced form}
		
			Let us now assume that we want to directly estimate the effect of the instrument on the outcome of interest. Plugging equation \ref{maths_dgp_x_iv} into equation \ref{maths_dgp_y} yields:			
				\[
					y_{i} = (\beta_{0} + \beta_{1}\pi_0) + (\beta_{1}\pi_{1}) z_{i} + ((\delta + \beta_{1}\gamma) w_{i} + u_{i} + \beta_{1}e_{i})
				\]
				~
				Note that if we directly regress the outcome on the instrument, the resulting estimand will be different from that of the other models. To make them comparable, we could set $\pi_{1}$ to 1 so that an increase of 1 in the instrument causes an increase of $\beta_{1}$ in $y$. Regardless of whether we make this assumption or not, regressing $y$ on $z$ corresponds to the usual univariate, unbiased case and directly gives the following result:
				
				\begin{lemma}\label{lemma_red}
					Based on the data generating process mentioned previously, for $\hat{\beta}_{\textsc{red}}$, the OLS estimator of the reduced form regression of $y$ on $z$,  $\hat{\beta}_{\textsc{red}} \overset{d}{\to} \mathcal{N}(\beta_{1}, \ \sigma_{\textsc{red}}^{2})$, with
					\[
						\sigma_{\textsc{red}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\pi_{1}^{2}\sigma_{z}^{2}}{n \ \sigma_{z}^{2}}
					\]
				\end{lemma}
				
				Note that the numerator is the part of the variance of $y$ that is not explained by $z$ ($\sigma^{2}_{y^{\perp z}}$). In addition, it is clear that the smaller $\sigma_{z}^{2}$, the larger $\sigma_{\textsc{red}}^{2}$. In addition, $\lim_{\sigma_{z}\to 0} \sigma_{\textsc{red}}^{2} = + \infty$.\\
				
				In the binary case, $\sigma_{z}^{2} = p_{1}(1-p_{1})$ with $p_{1}$ the proportion of treated observations, \textit{i.e.}, the proportion of 1 in $z$. When most observations have the same treatment status, \textit{i.e.}, $p_{1}$ close to 0 or 1, $\sigma_{z}^{2}$ tends to zero and $\sigma_{\textsc{red}}^{2}$ shoots up. There is not enough variation in the treatment status to precisely identify the effect of interest. 
				
		\subsection{Exaggeration ratios}
		
			Combining the results from lemma \ref{lemma_ovb} through \ref{lemma_red}  regarding the asymptotic distribution of the various estimators with lemma \ref{lemma_evol_exagg} stating that exaggeration increases with the variance of a normally distributed estimator yields:
			~
			\begin{theorem}
				For the data generating process described in section \ref{maths_dgp}, the exaggeration ratio of the controlled, IV and reduced form estimators, respectively $E_{\textsc{ctrl}}, E_{\textsc{iv}} \text{ and } E_{\textsc{red}}$, are such that:
					\begin{itemize}
						\item $E_{\textsc{ctrl}}$ increases with the correlation between the omitted variable and the explanatory variable of interest (\textit{i.e.} $|\gamma|$ or $|\rho_{xw}|$), for a given bias,
						\item $E_{\textsc{iv}}$ decreases with the strength of the IV (\textit{i.e.} with $|\pi_{1}|$ or $|\rho_{xz}|$),
						\item $E_{\textsc{red}}$ increases when the number of exogenous shocks decreases in the binary case
					\end{itemize}
			\end{theorem}
			
			Also using the same lemma and the limit properties of the variances described in section \ref{maths_dgp}, and since, at fixed bias, $E_{\textsc{ovb}}$ does not vary with the parameters of interest, we get:
			~
			\begin{theorem}
				For the data generating process described in section \ref{maths_dgp}, $\forall \ b_{\textsc{ovb}}$,
					\begin{itemize}
						\item $\exists \ \gamma$ s.t. $E_{\textsc{ctrl}} > E_{\textsc{ovb}}$
						\item $\exists \ \pi_{1}$ s.t. $E_{\textsc{iv}} > E_{\textsc{ovb}}$
						\item $\exists \ \sigma_{z}$ s.t. $E_{\textsc{red}} > E_{\textsc{ovb}}$
					\end{itemize}
			\end{theorem}
			
			For some parameter values, statistically significant estimates can be larger on average when using a convincing causal identification strategy that eliminates the omitted variable bias than when embracing the bias and running a naive biased regression.	
	
%-----------------------------------------------------------------------------

% SIMULATIONS

%-----------------------------------------------------------------------------	
				
	\section{Simulations} \label{simulations}
		
		 As visible in equation \ref{exagg_general},  the exaggeration ratio depends on the true effect and can therefore only be computed when this true effect is known. Since it is never the case in real-world settings, I turn to Monte-Carlo simulations.\\
    
    For clarity, I split the simulations by identification strategy %. While the general idea that causal inference methods discard variation to identify effects is shared across strategies, the confounding-exaggeration trade-off is mediated through a distinctive channel for each of them. 
    and build simulations that reproduce real-world examples from economics of education for RDD, labor economics for matching, political economy for IV, health economics for exogenous shocks and environmental economics for control and fixed effects approaches. Real-world settings enable to clearly grasp the relationships between the different variables and to set realistic parameter values. Since all these simulations have an illustrative purpose only, I intentionally focus on settings in which statistical power can be low. All the models are correctly specified and accurately represent the data generating process, except for the omitted variable bias (OVB).
    
     		For each identification strategy, I start by laying out how the method enables to retrieve a causal effect. It naturally points to the key parameter through which the confounding/exaggeration trade-off is mediated. I then briefly describe the example setting considered and the simulation assumptions. I finally display the simulation outputs and discuss the implications of the trade-off that are specific to the identification strategy considered. Detailed codes for simulation procedures are available on the \href{https://vincentbagilet.github.io/causal_inflation/}{project's website}.
		
		
%%%%%%%%%%      SIM    Controls        %%%%%%%%%%%%%%%
		
		 \subsection{Controlling for confounders}
		
     			\paragraph{Intuition.} To identify a causal effect and avoid the risk of confounders, an ``ideal'' approach would be to partial them out by directly controlling for them. % by including additional variables in the regression model to eliminate endogenous variation. In the ideal case, we would control for the omitted variable and for this only. 
			However, as discussed in the introduction and section \ref{maths}, controlling for an additional variable may increase the variance of the estimator if it absorbs more variation in the explanatory variable of interest than in the outcome variable.
			% ($y$). Controlling for a variable ($w$) is equivalent to partialling out this variable from both $x$ and $y$. Since the variance of the OLS estimator is given by the ratio of the variance of the residuals $\sigma_{y^{\perp x, w}}^2$ over $n$ times the variance of the explanatory variable after partialling out $w$ ($\sigma_{x^{\perp w}}^2$), if controlling absorbs more of the variance in $x$ than in $y$, it will increase the variance of the resulting estimator and create exaggeration. 
			The same reasoning applies to Fixed Effects (FEs): if including FE partials out more of the variation in $x$ than in $y$, it will increase the variance of the estimator.
			
			\paragraph{Case-study and simulation procedure.} To highlight this trade-off, I consider the extreme case in which either we perfectly control for confounders or do not control for them. For illustration purposes, I use \textit{only arbitrary numbers for now.\footnote{I will use a concrete example, as in other simulations, in the near future.}} I consider a simple setting, with one outcome variable $y$, one explanatory variable $x$ and an omitted variable $w$. The data generating process is the same as described in equations \ref{maths_dgp_y} and \ref{maths_dgp_x}. As in this section, I reason at bias fixed and variances of the defined variables fixed, \textit{i.e.} varying the share of the variance of $x$ and $y$ that is explained by $w$.
			
			\paragraph{Results.}  Figure \ref{graph_controls} displays the results of these simulations. The more the unobserved variable is linked to the explanatory variable of interest as compared to the outcome variable, \textit{i.e.}, the larger the $\gamma/\delta$ ratio, the larger the exaggeration. When this ratio is large, controlling can cause exaggeration to become larger than the OVB plus exaggeration when the variable is omitted.
        
       			 \begin{figure}[!h] 
				\begin{center}
					\caption{Evolution of the Bias with the Correlation of the omitted variable with $x$ and $y$, conditional on significativity.}
					\label{graph_controls}
					\includegraphics[width=0.8\linewidth]{images/main_graph_controls_paper.pdf}
		      			\caption*{\footnotesize \textit{Notes}: The green line indicates the average bias for estimates from the control model that are statistically significant at the 5\%. The beige line represents the bias of statistically significant estimates from the model with the omitted variable. In this simulation, N = 2,000. Gamma is the parameter for $w$ in the generating equation of $x$. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_inflation/controls.html}{link}.}
				\end{center}
				\vspace{-1cm}
			\end{figure} 
		
%%%%%%%%%%      SIM    Matching        %%%%%%%%%%%%%%%		
	
 \subsection{Matching}
		
			\paragraph{Intuition.} Another approach to retrieve a causal effect in a situation of selection on observables is to use matching. This method defines ``counterfactuals'' for treated units by picking comparable units in the untreated pool. 
			%It will retrieve the causal effect specific to matched treated units %In the ideal case where all confounders are assumed to be observed, matching can be used to estimate the causal effect specific to matched treated units. %Contrary to multivariate regression models, this method makes the common support of the data explicit, avoids model extrapolation and non-parametrically adjusts for observed confounders \citep{ho_matching_2007}. 
			In the case of propensity score matching, treated units are matched to units that would have a similar predicted probability of taking the treatment, \textit{i.e.} couple of units with a difference in propensity score lower than a critical value called the caliper. The smaller the caliper, the more comparable units have to be to be matched and therefore the lower the risk of confounding. Yet, with a stringent caliper, some units may not find a match and be pruned, decreasing the effective sample size. This can lead to a loss in statistical power and produce statistically significant estimates that are inflated. In the case of matching, the confounding-exaggeration trade-off is therefore mediated by the value of the caliper.
                
       	 	\paragraph{Case-study and simulation procedure.} \textit{I need to modify this simulation to make it realistic. To do so, I plan to simulate true variables distributions and run an actual matching algorithm.}
		
		 I illustrate this issue by simulating a labor training program where the treatment is not randomly allocated \citep{dehejia_causal_1999}. Individuals self-select into the training program and may therefore have different characteristics from individuals who do not choose to enroll. To emulate this, I assume that the distribution of the propensity scores differ for treated and control groups: they are drawn from $\mathcal{N}(\mu_T,\sigma_T)$ and $\mathcal{N}(\mu_C, \sigma_C)$ respectively. This can be analogous to considering that matching is done based on the value of a unique covariate. Based on how these propensity scores are created, I define the potential monthly income of each individual $i$, under the treatment or not.%: $Y_i(0)=\text{Wage}\times\text{PS}_i+\mathcal{N}(\mu_{\text{u}},\sigma_{\text{u}})$. Wage is the baseline wage, PS$_{i}$ the propensity score of individual $i$ and some noise is drawn from $\mathcal{N}(\mu_{\text{N}},\sigma_{\text{N}})$. This equation makes the potential outcomes Y(0) partly different for treated and control units, creating the required common support issue. We then simulate the potential outcomes Y$_i$(1) by adding a constant treatment effect of the training program. 
        
        		Based on this simulation framework, I generate 1000 datasets for each propensity score matching procedure with caliper values ranging from 0 to 1. Parameters values of the simulation are set to make them realistic and can be found \href{https://vincentbagilet.github.io/causal_inflation/Matching.html}{here}. Once units are matched, I simply regress the observed revenue on the treatment indicator.
        
         \begin{figure}[!h] 
			    \caption{Evolution of Bias with the Caliper in Propensity Score Matching, Conditional on Statistical Significance.}
				\label{graph_matching}
			 \centering\includegraphics[width=0.9\linewidth]{images/main_graph_matching_paper.pdf}
			 \caption*{\footnotesize \textit{Notes}: The green line indicates the average bias for all estimates, regardless of their statistical significance. The beige line represents the inflation of statistically significant estimates at the 5\% level. The caliper is expressed in standard deviation of the propensity score distribution. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_exaggeration/Matching.html}{link}.}
		\end{figure} 
        
        \paragraph{Results.} Figure \ref{graph_matching} indicates that the average bias of estimates, regardless of their statistical significance, decreases with the value of the caliper as units become more comparable. For large caliper values, units are not comparable enough and confoundings bias the effect. For small caliper values, they become comparable but the sample size becomes too small to allow for a precise estimation of the treatment effect and exaggeration arises. Statistically significant estimates never get close of the true effect. This imprecision, and thus exaggeration, results from the fact that the matching procedure does not use information on outcomes that would reduce the residual variance of the model but rather focuses on reducing bias arising from covariates imbalance \citep{rubin_using_2001}. 
        
%%%%%%%%%%      SIM    RDD        %%%%%%%%%%%%%%%
		
		 \subsection{Regression Discontinuity Design}
		
     			\paragraph{Intuition.} To identify a causal effect, a regression discontinuity approach also prunes units that cannot be deemed comparable enough to any units with the opposite treatment status. This method relies on the assumption that for values close to the threshold, treatment assignment is quasi-random. Under this assumption, individuals just below and just above the threshold would be comparable in terms of observed and unobserved characteristics, and only differ in their treatment status. To avoid confounding, the RDD focuses on observations within a certain bandwidth around the threshold and discards observations further away. The effective sample size where the identification of causal effect of the treatment is the most credible is thus smaller than the total sample size, leading to a lower precision. For this method, the confounding-exaggeration trade-off is therefore mediated by the size of the bandwidth.
			
			\paragraph{Case-study and simulation procedure.} To illustrate this trade-off, I consider a standard application of the sharp RD design in economics of education in which students are offered additional lessons based on the score they obtained on a standardized test. \cite{thistlethwaite_regression-discontinuity_1960} introduced the concept of RDD using a similar type of quasi-experiment. Students with test scores below a given threshold receive the treatment while those above do not. Since students far above and far below the threshold may differ along unobserved characteristics such as ability, a RDD estimates the effect of the treatment by comparing outcomes of students whose initial test scores are just below and just above this threshold. 
        
        			 The simulation framework for the RDD is as follows. If a student $i$ has an initial scores $Qual_{i}$ below a cutoff $C$, they must take additional lessons, making the allocation of the treatment $T$ sharp: $T_i = \mathbb{I}[Qual_{i} < C]$. Final scores are correlated with qualification score. Further assume that both qualification and final test scores are affected by students' unobserved ability $w$ in a non-linear (cubic) way. A high or low ability has a strong positive impact on test scores while an average one does not strongly impact test scores. The final test score of student $i$ is thus: $Final_{i} = \beta_{0} + \beta_{1} T_i + \eta Qual_{i} +  \delta f(w_i) + u_{i}$, where $f$ a non linear function (here cubic) and $u_{i} \sim \mathcal{N}(0, \sigma_{u}^{2})$ random noise. $\beta_{1}$ is the causal parameter of interest. 
        
        			To make the simulations realistic, I derive parameters values from statistics from the Department of Education and treatment effect sizes from a meta-analysis of RCTs in economics of education by \cite{kraft_interpreting_2020}.  Given these parameters values, I then generate 1000 datasets with 10,000 observations. For each dataset, I estimate the treatment effect by regressing the final score on the treatment status and the qualifying score for different bandwidth sizes. 
				
			\paragraph{Results.}  Figure \ref{graph_RDD} displays the results of these simulations. While the average of all estimates gets close to the true effect as bandwidth size and thus OVB decrease, the average of statistically significant estimates never gets close to the true effect in this setting. For large bandwidths, the omitted variable biases the effect while for small bandwidths, the small sample size creates exaggeration issues. The optimal bandwidth literature describes a similar trade-off but with different consequences \citep{imbens_optimal_2012}. They consider a bias-precision trade-off, I consider an omitted variable bias-exaggeration bias trade-off. As for matching, the parameter mediating the trade-off can directly be adjusted in a continuous way by the researchers and the more we reduce one of these two biases, the more we increase the other. For other methods such as IV or exogenous shocks, the issue is more dichotomized and the use of the causal identification strategy comes with a drawback.
        
       			 \begin{figure}[!h] 
				\begin{center}
					\caption{Evolution of the Bias with Bandwidth Size in Regression Discontinuity Design, conditional on significativity.}
					\label{graph_RDD}
					\includegraphics[width=0.8\linewidth]{images/main_graph_RDD_paper.pdf}
		      			\caption*{\footnotesize \textit{Notes}: The green line indicates the average bias for all estimates, regardless of their statistical significance. The beige line represents the inflation of statistically significant estimates at the 5\% level. In this simulation, N = 10,000. The bandwidth size is expressed as the proportion of the total number of observations of the entire sample. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_inflation/RDD.html}{link}.}
				\end{center}
				\vspace{-1cm}
			\end{figure} 
		

        
%%%%%%%%%%      SIM    IV        %%%%%%%%%%%%%%%
		
        		\subsection{Instrumental Variables Strategy}\label{sim_IV}
            
                		\paragraph{Intuition.} 
        			Instrumental variables strategies overcome the issue of unobserved confounding by only considering exogenous variation in the treatment, \textit{i.e.} the variation that is explained by the instrument. Even when this exogenous fraction of the variation is limited, the instrument can successfully eliminate confounding on average. However, the IV estimator will be imprecise and statistical power low. In the case of the IV, the confounding-exaggeration trade-off is mediated by the strength of the instrument considered. The weaker the instrument, the more inflated statistically significant estimates will be.
        		
        			\paragraph{Case-study and simulation procedure.} 
        			To illustrate this trade-off, I focus on the impact of voter turnout on election results. To avoid the threat of confounding in this setting, studies often take advantage of exogenous factors such as rainfall that affect voter turnout. I reproduce such setting and assume that the true data generating process for the republican vote share is such that  in location $i$, $Share_{i} = \beta_{0} + \beta_{1} Turnout_{i} + \delta w_{i} + u_{i}$, where $w$ is an unobserved variable and $u \sim \mathcal{N}(0, \sigma_{u}^{2})$ some random noise. The causal parameter of interest is $\beta_{1}$. In addition, turnout is affected by the amount of rain: $Turnout_{i} = \pi_{0} + \pi_{1} Rain_{i} + \gamma w_{i} + e_{i}$, where $Rain_{i}$ is the amount of rain in location $i$ on the day of the election 
			%\footnote{In the DiD event study simulations that reproduce this setting, $Rain_{i}$ is a dummy for whether it rained or not on the day of the election in location $i$.} 
			and $e$ some random noise drawn form $\mathcal{N}(0, \sigma_{e}^{2})$. I refer to $\pi_{1}$ as the strength of the instrumental variable. 
        			
        			To make the simulations realistic, I derive parameters values from a set of existing studies using similar variables \citep{gomez_republicans_2007, fujiwara_habit_2016, cooperman_randomization_2017}. For each value of the IV strength considered, I create 1000 datasets. I run both a naive ordinary least squares model and a two-stage least squares model to estimate the impact of voter turnout on the vote share of a party.
        			
        			\paragraph{Results.} 
			Figure \ref{graph_IV} displays, for different IV strengths, the average of statistically significant estimates scaled by the true effect size for both the IV and the naive regression model. When the instrument is strong, the IV will recover the true effect, contrarily to the the naive regression model. Yet, when the IV strength decreases, the exaggeration of statistical significant estimates skyrockets. Even if the intensity of the omitted variable bias is large, for limited IV strengths, the exaggeration ratio can become larger than the omitted variable bias. When the only available instrument is weak, using the naive regression model would, on average, produce statistically significant estimates that are closer to the true effect size than the IV. Of interest for applied research, a large $F$-statistic does not necessarily attenuate this problem. For the parameter values considered here, this phenomenon arises even in cases for which the $F$-statistic is substantially larger than the usually recommended threshold of 10, as illustrated in the \href{https://vincentbagilet.github.io/causal_inflation/IV.html#f-statistic-analysis}{online supplementary materials}. 
        		
                    		 \begin{figure}[!h] 
                    			\begin{center}
                    				\caption{Evolution of the Bias of Statistically Significant Estimates Against Strength of the Instrument in the IV Case.}
                    				\label{graph_IV}
                    				\includegraphics[width=0.8\linewidth]{images/main_graph_IV_paper.pdf}
                                    \caption*{\footnotesize \textit{Notes}: The green line indicates the average bias for IV estimates that are statistically significant at the 5\%. The beige line represents the bias of statistically significant OLS estimates at the 5\% level. The strength of the instrumental variable is expressed as the value of the linear parameter linking rainfall to turnout. In this simulation, N = 10,000. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_inflation/IV.html}{link}.}
                                    \end{center}
				\vspace{-1cm}
                    		\end{figure} 
		
% I start with the RDD and matching cases because in these settings the researcher has a hand on the parameter driving power: the bandwidth size and the stringency of the matching mechanism.



%%%%%%%%%%      SIM    EVENT        %%%%%%%%%%%%%%%

		\subsection{Exogenous shocks}\label{sim_shocks}
    
        			\paragraph{Intuition.}  To avoid confounding, strategies such as DiD and event studies take advantage of exogenous variation in the treatment status caused by exogenous shocks or events. In many settings, while the number of observations may be large, the number of events, their duration or the proportion of individuals affected might be limited. As a consequence, the number of (un)treated observations can be small and the variation available to identify the treatment limited. Statistical power is maximized when the proportion of treated observations is equal to the proportion of untreated ones, as extensively discussed in the randomized controlled trial literature. In studies using discrete exogenous shocks, a confounding-exaggeration trade-off is thus mediated by the number of observations treated. This issue does not only concern DiD event studies but is particularly salient in this case. 
			
			\paragraph{Case-study and simulation procedure.} %To illustrate this trade-off, I reproduce the analysis described for instrumental variable strategies described in section \ref{sim_IV} but considering that $Rain$ is a dummy equals to one if it rained on the day of the election.
			To illustrate this trade-off, I simulate a study of the impact of air pollution reduction on newborn weight of babies. To avoid confounding, one can exploit exogenous shocks to air pollution such as plant closures, creation of a low emission zone or of an urban toll. I simulate the analysis at the zip code and monthly levels and focus on the example of toxic plant closures. I consider that the average birth weight in zip code $z$ at time period $t$, $bw_{zt}$, depends on a zip code fixed effect $\zeta_z$, a time fixed effect $\tau_t$, and the treatment status $T_{z,t}$, which is equal to one if a plant is closed in this period and 0 otherwise. The average birth weights $bw_{zt}$ is defined as follows: $bw_{z,t} = \alpha + \beta T_{z, t} + \zeta_z + \tau_t + u_{z,t}$. To further simplify the identification of the effect, I assume a non-staggered treatment allocation and constant and homogenous effects. I only vary the proportion of zip codes affected by toxic plant closings.

       The parameters values of the simulations are inspired from \cite{currie_environmental_2015} and \cite{lavaine_energy_2017}. For a fixed sample size of 120,000 observations, I generate 1000 datasets for an increasing number of treated observations and estimate the correct two-way fixed effects model.
        
			\paragraph{Results.} Figure \ref{graph_shocks} displays the results of these simulations. Even though the actual sample size is extremely large in the example, if the number of treated observations is small, exaggeration can be important. A very large number of observations does not necessarily prevent exaggeration to arise.  

			\begin{figure}[!h] 
                    			\begin{center}
                    				\caption{Evolution of Bias With the Number of Treated Observations, for Statistically Significant Estimates, in the Exogenous Shocks Case}
                    				\label{graph_shocks}
                    				\includegraphics[width=0.8\linewidth]{images/main_graph_DID_paper.pdf}
                                    \caption*{\footnotesize \textit{Notes}: The line indicates the average bias for estimates that are statistically significant at the 5\%. In this simulation, N = 120,000. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_inflation/DID.html}{link}.}
                                    \end{center}
				\vspace{-1cm}
                    		\end{figure} 
		
		
%-----------------------------------------------------------------------------

% PRACTICAL RECOMMENDATIONS

%-----------------------------------------------------------------------------

	\section{Navigating the trade-off} \label{discussion}

		% HOOK OF THE SECTION
 		In the previous sections, I argued that that using causal identification strategies induce a trade-off between avoiding confounding and exaggerating true effects. How can we, as applied researchers using observational data, arbitrate it? Since key pieces of information such as the true effect and the effect of omitted variables are inherently unknown, we cannot directly compute the biases caused by confounders and exaggeration. In this section, I examine how we can however get a sense of threats from both sides of the trade-off and visualize its main driver, the variation used for identification. I then discuss how changing attitudes towards statistical significance and replicating studies could limit the exaggeration issue.
	
	%Even though it does not produce uninflated estimates, reporting power calculations enables to evaluate the risk of exaggeration for a study. In this section, we present a workflow to evaluate and report the power of a study before and after its implementation. 
	
		\subsection{Gauging omitted variable bias}
	
			%"MEASURING" OVB
			On one side of the trade-off lies the widely discussed bias caused by confounders. Although it is in essence impossible to measure, tools such as sensitivity analyses are available to gauge its magnitude \citep{rosenbaum_observational_2002, middleton_bias_2016, oster_unobservable_2019, cinelli_making_2020}. For instance, the method developed in \cite{cinelli_making_2020} enables to assess how strong confounders would have to be to change the estimate of the treatment effect beyond a given level we are interested in. It offers bounds for the strength of the association between the treatment and potential omitted variables by weighting it against the measured association between the treatment and observed covariates. A typical conclusion from such an analysis would be: ``omitted variables would have to explain as much residual variance of the outcome and the treatment as the observed covariate $x$ (age for instance) to bring down the estimate to a value of $\beta_{l}$''. In addition, the authors implement graphical tools to facilitate this comparison. I suggest to use such quantitative bias analyses to evaluate the restrictiveness of the causal approach required to limit the threat of unobserved confounding to acceptable levels. 
			
		\subsection{Evaluating risks of exaggeration}

			On the other side of the trade-off lies the exaggeration emerging when statistical power is low. As OVB, exaggeration and statistical power are in essence impossible to measure as their computation depends on the true effect which is always unknown. Yet, power calculations can help assess them by making hypothesizes on the magnitude of the true effect. In randomized controlled trials, such computations are not only an established practice but a requirement \citep{duflo_using_2007, mcconnell_going_2015, athey_econometrics_2016}. They are, however, rarely reported in non-experimental studies. This can be understandable if power calculations are perceived only as a way to measure a design's ability to detect an effect when there is actually one. Yet, taking publication bias and the threat of exaggeration into account highlights the necessity of running power calculations in non-experimental studies as well. A low power or a relatively large variance not only makes it more difficult to detect an effect or to draw clear conclusions about its magnitude when detected but it can also create a bias. To avoid this bias, I advocate to make power more central to non-experimental analyses. Currently, in causal inference textbooks, very few pages are devoted to statistical power in non-experimental studies \citep{angrist_mostly_2009, angrist_mastering_2014, imbens_causal_2015, cunningham_causal_2021}. To the best of my knowledge, only two textbooks discuss the matter in depth \citep{shadish_experimental_2002, huntington-klein_effect_2021}. Results from power and exaggeration calculations would not only be highly informative but could also be reported very concisely in the robustness section of articles. 
			
			\subsubsection{Prospective power calculations}
						
				To evaluate the statistical power of a study, the risk of exaggeration and identifying the factors driving it, one can first simulate the design of the study \citep{hill_bayesian_2011, gelman_regression_2020, black_simulated_2021}. Simulating a data generating process from scratch requires thinking about the distribution of the variables, about their relationships and can also help underline the variation used for identification. %External information found in previous studies can help guide the simulation process to make it more realistic. 
			 I implemented such Monte Carlo simulations in \Cref{simulations}. In the replication material, I provide "R" code to be used as examples of how to run such simulations for most causal identification strategies. In situations where the relationships among covariates are too complex to emulate, one can also start from an existing dataset and add a known treatment effect. I implemented real-data simulations in a companion paper and describe their implementation in its \href{https://vincentbagilet.github.io/inference_pollution/}{replication material} \citep{bagilet_accurately_2023}. 
			 
			 %When simulations indicate that statistical power is low, additional data could be collected or the statistical model could be improved to increase precision. In any case, it should not stop the research project from carrying out. Simulation results rest on the way the data generation process was modeled, and it can be difficult to gauge the amount of noise present in data before actually analyzing them. The two main benefits of a prospective simulation procedure are that it allows us to consider factors that may affect the statistical power of our study and to avoid drawing misleading conclusions based solely on statistically significant estimates when statistical power is low.
				
			\subsubsection{Retrospective power calculations}
			
				Running post-analysis power calculations can also help getting a sense of the statistical power associated with a research design. Such \textit{retrospective} calculations allow to evaluate whether the design of the study would produce accurate and uninflated statistically significant estimates if the true effect was in fact smaller than the observed estimate \citep{gelman_beyond_2014, ioannidis_power_2017, stommes_reliability_2021}. 
				
				%can take this out if want to make the paper shorter
				I illustrate how a retrospective analysis works by taking the example of \cite{card_using_1993} on the relationship between human capital and income. He finds that an additional year of education, instrumented by the distance of growing up near a four-year college, causes a 13.2\% average increase in wage. The associated standard error is 5.5\%. Is there a risk of exaggeration with this design? Since, as noted by the author himself, the estimate is very imprecise we could expect so. Imagine that prior evidence suggests that the true effect is likely close to a 10\% increase in wage. We can then compute the statistical power of the study by drawing many estimates from a normal distribution centered around the hypothesized true effect of 10\% and with a standard deviation equal to the 5.5\% standard error obtained in \cite{card_using_1993}. Concretely, this proceeds as if we were able to replicate the study many times under the assumption that the true effect was equal to the hypothesized one.\footnote{\cite{timm_retrodesign_2019} and \cite{linden_retrodesign_2019} offer \texttt{R} and \texttt{Stata} packages that enable to easily run these calculations through an extremely short command: \texttt{retrodesign(10, 5.5)}.} Statistically significant estimates (at the 5\% level) would on average be roughly equal to 15\%, therefore overestimating the true effect by a factor of 1.5. Statistical power, the proportion of estimates that are significant, would only be 44\%. Conditional on a 10\% true effect size being a reasonable assumption, this study would be under-powered and exaggeration substantial.
			
				The usefulness of any retrospective power analysis lies on the assumption made regarding the true effect size. To identify a range of plausible effect sizes one can rely on results from meta-analyses or from existing studies that have a credible design (\textit{e.g.}, a large randomized controlled trial).\footnote{Note that when such meta-analyses are available, one can use a Bayesian procedure to shrink statistically significant estimates based on the corpus of estimates from prior studies \cite{zwet_proposal_2021, zwet_significance_2021, zwet_statistical_2021} .} When such information is not available, power calculations can be ran for a range of smaller but credible effect sizes.
				
		\subsection{Driver of the trade-off}
		
			%We have seen that the variation used for identification was the main driver of the confounding-exaggeration trade-off. This variation determines the variance of the estimator and thus the amount of exaggeration. In this section, I first discuss the impact of estimator variance and how exaggeration leads to reinterpret the well-known bias/variance trade-off. I then propose to use a tool that enables to visualize where the variation used for identification comes from.
		
			\subsubsection{Navigating the bias-variance trade-off}
			
				In non-experimental studies, estimator variance is often important to the extent that a large variance may lead to a failure to reject the null of zero effect when it is incorrect. It makes variance paramount as long as a statistically significant estimate is not obtained. Yet, exaggeration underlines that variance matters, even once a significant estimate has been obtained. 
			
				 Obtaining a statistically significant estimate from an imprecise estimator should not necessarily be interpreted as a sign of ``success'' in getting significance despite large confidence intervals. It could instead be a warning that this estimate may come from the tails of the distribution and would thus inaccurately represent the true effect. Conditional on having obtained a statistically significant estimate, a limited precision can hide a bias: exaggeration. This invites to revisit the well-known bias-variance trade-off: a larger variance can also lead to a larger (conditional) bias. When combined with the existing statistical significance filter, the bias-variance trade-off is in fact a bias-bias trade-off. This paper thus urges to pay more attention to the implications of our design choices on the variance of our estimators, even if a large variance did not prevent us from obtaining a statistically significant estimate.
			
			\subsubsection{Visualizing the driver of the trade-off}
		
				Causal inference strategies only leverage a subset of the variation to avoid confounders. However, when this subset is too small, exaggeration arises. Visualizing this variation (and the observations) actually used for identification can help navigate the confounding-exaggeration trade-off. The visualization tool I outline here both leverages the interpretation of causal inference methods as control approaches and builds on a procedure developed in \cite{aronow_does_2016} for a different purpose: evaluating external validity of standard regressions.
				
				\cite{aronow_does_2016} essentially interprets the estimate of the coefficient of the treatment of interest in a simple linear non-causal regression as a weighted average of individual treatment effects. The weight $w_{i}$ of individual $i$ is simply the squared difference between its treatment status $T_{i}$ and the value of this treatment status as predicted by the other covariates $X$: $w_{i} = (T_{i} - \mathbb{E}[T_{i} | X_{i}])^{2}$. If treatment effects are heterogenous, the weighting may lead some observations to be disproportionally (un)represented in the average effect of the treatment. %Some observations, whose treatment status is very well explained by covariates do not actually contribute to the estimation of the treatment effect. 
				In that case, the average of treatment is only representative of a subset of the individual treatments, leading to external validity issues. 
				
				The parallel with our setting directly follows from this interpretation regardless of whether the treatment is heterogenous or not: observations whose treatment status is well explained by covariates do not actually contribute to the estimation of the effect of the treatment. This may lead to a small \textit{effective} sample size and to exaggeration. In the control approach to causal inference strategies, the more variation in the treatment is absorbed when ``controlling'' for confoundings, the smaller the effective sample, potentially leading to exaggeration. \cite{aronow_does_2016} focused on the representativity of the effective sample for fear of external validity issues. I am interested on in its size for fear of exaggeration.
				
				It might then seem compelling to define this effective sample by proposing a weight value under which the associated observation does not actually contribute to  identification. Yet, considering the specificity of each analysis and that exaggeration depends on several factors, including the true effect size, I avoid proposing an \textit{ad hoc} cut-off. Instead, I suggest to visualize the individual weights. It allows to get a sense of where the variation comes from and which are the observations that actually contribute to the estimation. Since applied economics analyses often rely on panel data, I propose to use a heatmap as a base for visualization, with time on the \textit{x}-axis and individuals on the \textit{y}-axis. If the data is cross sectional, the same heatmap can be computed with individuals on the \textit{x}-axis and a length-one segment on the \textit{y}-axis.\\
				
				\textit{I still need to develop an example. The one I have, based on \cite{deschenes_economic_2007} is actually not a good example because their effect is a composite of various effects. I need to identify another paper. In addition, I need to think again about my metric because it needs to represent the ratio of the residual variance (after partialling out fixed effects) of $y$ over that of $x$.}
				
				 %I display additional graphs and share  \verb?R? code to build on the \href{https://vincentbagilet.github.io/causal_exaggeration/}{companion website}.
				
				%talk about leverage
				
	% GENERAL RECOMMENDATIONS
	\subsection{Attitude Towards Statistical Significance and Replication}
	
		Exaggeration only arises in the presence of publication bias. As shown in the simulations, if estimates were not filtered by their statistical significance, even under-powered studies would on average recover the true effect, as long as the estimator is unbiased. The exaggeration issue could therefore be addressed by tackling publication bias. 
		
		To identify broader pathways to eliminate this filtering of significant results, it is first helpful to discuss the processes that lead to statistically significant results when power and thus the probability of obtaining a significant estimate is low. In such situations, they can be obtained either by ``chance'' or as an outcome of the garden of forking paths \citep{simmons_false-positive_2011, gelman_garden_2013, kasy_forking_2021}. Forks appear at various stages along the path of research, for instance in data preparation, regarding the inclusion of a given cofntrol variable or later, regarding whether to carry on with a research that yields non-significant results. Due to the structural flaw that favors significance, the path followed may be more likely to lead to a statistically significant result. These choices are most often not the result of bad researcher practices but instead a product of a structure that portrays significant results as the end goal of research. 
		
		The issue being structural, system level changes in scientific practices could also alleviate exaggeration and the trade-off described in this paper. First, many researchers advocate abandoning statistical significance as a measure of a study's quality \citep{mcshane_abandon_2019}. To be effective, this change should be paired with an effort to replicate studies \citep{christensen_transparency_2018}. Replications, even of low powered studies, would eventually enable to build the actual distribution of the causal estimand of interest. Meta-analyzes would then reduce the uncertainty around the true value of the causal estimand by pooling estimates \citep{hernan_causal_2021}. Finally, the inflation of statistically significant estimates could be limited by interpreting confidence intervals and not point estimates and thus considering these intervals as compatibility intervals \citep{shadish_experimental_2002, amrhein_inferential_2019, romer_praise_2020}. The width of such intervals gives a range of effect sizes compatible with the data. Confidence intervals will be wide in under-powered studies signaling that point estimates should not be taken at face value, even if statistically significant.

%-----------------------------------------------------------------------------

% CONCLUSION

%-----------------------------------------------------------------------------

\section{Conclusion} \label{conclusion}

	The economic literature suffers from an extensive lack of statistical power \citep{ioannidis_power_2017} and strongly favors statistically significant findings \citep[for instance]{rosenthal_file_1979, andrews_identification_2019, abadie_statistical_2020, brodeur_methods_2020}. In such situations, estimates published from underpowered studies exaggerate true effect sizes, even when the estimators are ``unbiased'' in the usual sense of $\mathbb{E}[\hat{\beta}] = \beta$ \citep{ioannidis_why_2008, gelman_beyond_2014, lu_note_2019, zwet_significance_2021}. It is therefore not surprising that many estimates published in economics have been shown to be considerably exaggerated  \citep{camerer_evaluating_2016, ioannidis_power_2017}, despite the extensive use of convincing causal inference methods.  However, determinants for these exaggeration and power issues have remained understudied. I argue that exaggeration is exacerbated by the foundational component of causal inference: the fact that it only leverages subsets of the variation. Although causal methods enable to avoid confounding, they also reduce statistical power and thus increase the risk of exaggeration. The same aspect that makes these methods credible can create another type of bias. A systematic reporting of statistical power calculations and analysis of the variation actually used for identification could help avoid falling into this exaggeration trap.
	
	%Add something about the fact that the variance of an estimator matters even when a statistically significant result has been obtained because a "just significant" estimate might probably be exaggerated
	%
	
%-----------------------------------------------------------------------------

% NEXT STEPS

%-----------------------------------------------------------------------------

\section{Next steps} \label{next_steps}

	Here is an ordered list of the next tasks I plan to do. I would love to get feedback on both the content and the order of this list. Are there more urgent points than others? Are there other things I need to add to this list?\\

	\textit{For each task, I write down what section of the paper it pertains to (S = Simulations, M = Maths, R = Recommendations, W = Writing). Question marks mean that I am not certain that this task is necessary.}\\

	\begin{enumerate}
		\item (S) Verify the SNR in my simulations: are they similar to those in the published papers?
		\item (S) Add more visualizations
		\item (S) Calibrate simulations for FEs/controls
		\item (W) Add an actual example for controls: in a real data set, compare exaggeration when a given variable is used as a control or not + try to find existing papers for which adding more and more FEs increases variance
		\item (W) Expand the description of the simulations in the body of the paper (and move matching and exogenous shocks to the appendix) + add the simulation codes (my RMarkdown files) to the appendix
		\item (W) Add a literature review describing the extent of exaggeration in a literature (either acute health effects of air pollution, \textit{i.e.}, directly reinjecting my other paper or adding something different  based on Leo's work on interventions aimed at reducing household energy consumption or on behavioral interventions to promote household action on climate change)
		\item (R) Add discussion on shrinkage
		\item (S) Redo the matching simulations
		%\item (R) Code an actual example of how one could evaluate the trade-off for their study (\textit{e.g.}, present it as if I was Card and wanted to evaluate risks of exaggeration in my study)
		%\item (M) Code something to compare the various potential definitions of exaggeration ($\mathbb{E}[ |\hat{\beta} | | signif]$ vs $\mathbb{E}[\hat{\beta}  | signif]$, etc) and their respective limits
		%\item (S) For exogenous shocks, add other types of treatment allocation and thus other types of estimation methods: permanent treatment staggered (event study, TWFE), temporary (reduced form) (?)
		\item (R) Develop the visualization tool
	\end{enumerate}


%-----------------------------------------------------------------------------

% BIBLIO

%-----------------------------------------------------------------------------
		
	
\newpage
	
\bibliographystyle{agsm}
\bibliography{../causal_exaggeration}

\newpage

\appendix

	\section{Mathematical proofs}\label{maths_proofs}
	
%%%%%%%%%% Evol exaggeration ratio %%%%%%%%%%%%%%%%

		\subsection{Variation of the exaggeration ratio (lemma \ref{lemma_evol_exagg})}
			
			\begin{proof}
				\cite{lu_note_2019} and \cite{zwet_significance_2021} showed this in the case of $b = 0$. 
				To extend it to the biased case, consider 
				$E_{b} = \frac{\mathbb{E}\left[ |\hat{\beta}_{b}| \big| \beta_{1}, \sigma, |\hat{\beta}_{b}| > z_{\alpha} \sigma \right]}{|\beta_{1}|}$ the exaggeration ratio of interest. 
				Note that, since $\hat{\beta}_{b}$ is an unbiased estimator of $\beta_{1} + b$, $\tilde{E_{b}} = \frac{\mathbb{E}\left[ |\hat{\beta}_{b}| \big| \beta_{1}, \sigma, |\hat{\beta}_{b}| > z_{\alpha} \sigma \right]}{|\beta_{1 + b}|}$ has the properties described in the lemma. 
				Now, considering that $E_{b} = \left| \frac{\beta_{1} + b}{\beta_{1}} \right| \tilde{E_{b}}$ proves the properties when $\beta_{1}$ and $b$ have the same sign.
			\end{proof}
	
%%%%%%%%%% Distrib ovb  %%%%%%%%%%%%%%%%

\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{ovb}}$ (lemma \ref{lemma_ovb})}
			
			For readability, let us introduce the usual vector notation such that for instance $y = (y_1, ..., y_n)'$ and set $\bm{\beta} = (\beta_0, \beta_1)'$ and $\text{x}_i = (1, x_i)'$. I also use capital letters to denote matrices (for instance $X = (\text{x}_{1}', ..., \text{x}_{n}')'$).\\
			
			\begin{proof} 
				Since, we do not observe $w$, we consider the projection of $y$ on $X$ only:
						~
						\begin{equation}\label{maths_eq_ovb}
							y = X\bm{\beta}_{\textsc{ovb}} + u_{\textsc{ovb}}
						\end{equation}
				
						where by definition of the projection, $\mathbb{E}[X'u_{\textsc{ovb}}] = 0$.\\
				
				We first compute the bias of the estimator. From equation  \ref{maths_eq_ovb} we get: 
				~
				\begin{equation}\label{bias_ovb}
					\begin{aligned}
            					& X'y = X'X\bm{\beta}_{\textsc{ovb}} + X'u_{\textsc{ovb}}\\
            					\Rightarrow \quad & \mathbb{E}[X'y] = \underbrace{\mathbb{E}[X'X]}_{\text{pos. def.}}\bm{\beta}_{\textsc{ovb}} + \underbrace{\mathbb{E}[X'u_{\textsc{ovb}}]}_{0} \\
            					\Leftrightarrow \quad & \bm{\beta}_{\textsc{ovb}} = \mathbb{E}[X'X]^{-1} \mathbb{E}[X'(X\bm{\beta} + \delta w + u)] & \text{cf eq. \ref{maths_dgp_y}}\\
            					\Leftrightarrow \quad & \bm{\beta}_{\textsc{ovb}} = \bm{\beta} + \mathbb{E}[X'X]^{-1} \mathbb{E}[X'w] \delta
					\end{aligned}
				\end{equation}		
				
				We then compute the asymptotic distribution. We can write: 
				~
				\begin{align*}
					\sqrt{n}(\hat{\bm{\beta}}_{\textsc{ovb}} - \bm{\beta}_{\textsc{ovb}}) &  = \left(\dfrac{1}{n} \sum_{i = 1}^{n} \text{x}_{i} \text{x}_{i}' \right)^{-1} \sqrt{n} \left(\dfrac{1}{n} \sum_{i = 1}^{n} \text{x}_{i} u_{\textsc{ovb}, i} \right)
				\end{align*}	
				
				Applying the Weak Law of Large Numbers (WLLN), the Central Limit Theorem (CLT) and Slutsky's theorem yields:
				
				\begin{equation}\label{asympt_ovb}
					\sqrt{n}(\hat{\bm{\beta}}_{\textsc{ovb}} - \bm{\beta}_{\textsc{ovb}}) \overset{d}{\to} \mathcal{N}\left(0,  \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1}\right) 
				\end{equation}
				
				
				We are interested in the second component of $\hat{\bm{\beta}}_{\textsc{ovb}}$. To retrieve it we need to compute $\mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1}$, $ \mathbb{E}[\text{x}_{i} w_{i}] $ and $\mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}]$.  
				
				\[ 
				\mathbb{E}[\text{x}_{i}\text{x}_i'] 
				= \mathbb{E} \begin{bmatrix}
					1 & x_{i}\\ 
					x_{i} & x_{i}^{2}
				\end{bmatrix} 
				=
				\begin{bmatrix}
					1 & \mu_{x}\\ 
					\mu_{x} & \sigma_{x}^{2} + \mu_{x}^2
				\end{bmatrix} 
				\quad \Rightarrow \quad
				     \mathbb{E}[\text{x}_{i}\text{x}_i']^{-1} = \dfrac{1}{\sigma_{x}^{2}} 
				     									\begin{bmatrix}
														 \sigma_{x}^{2} + \mu_{x}^2 & -\mu_{x}\\ 
														-\mu_{x} & 1
													\end{bmatrix} 
				\]
				
				\begin{equation*} 
            				\mathbb{E}[\text{x}_{i}w_i] = 
            				\mathbb{E}
            					\begin{bmatrix}
            						w_i\\ 
            						x_iw_i
            					\end{bmatrix} 
            				=
            				\begin{bmatrix}
            				    	0\\ 
            					\mathbb{E}[x_{i}]\underbrace{\mathbb{E}[w_{i}]}_{0} + \text{cov}(x_{i}, w_{i}) 
            				\end{bmatrix} 
            				= 
            				\begin{bmatrix}
            				    	0\\ 
            					\gamma  \underbrace{\text{var}(w_{i})}_{\sigma_{w}^{2}} + \underbrace{\text{cov}(\epsilon_{i}, w_{i})}_{0} 
            				\end{bmatrix} 
            				=
            				\begin{bmatrix}
            				    	0\\ 
            					\gamma \sigma_{w}^{2}
            				\end{bmatrix} 
				\end{equation*}
				
				\begin{equation}\label{bias_ovb}
					\Rightarrow \quad
					\mathbb{E}[\text{x}_{i}\text{x}_i']^{-1}\mathbb{E}[\text{x}_{i}w_i] 
					=
					\dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}}
					\begin{bmatrix}
				    		- \mu_{x} \\
						1
					\end{bmatrix} 
				\end{equation}
				
				Note that $\mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] \overset{\textsc{lie}}{=} \mathbb{E}[\text{x}_{i}\text{x}_{i}'\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]]$. We thus first compute $\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]$, noting that:
				~
				\begin{align*}
            				u_{\textsc{ovb}, i} & =  y_i - \text{x}_i'\bm{\beta}_{\textsc{ovb}} \\
            					& = \delta w_{i} + u_i + \text{x}_i' (\bm{\beta} - \bm{\beta}_{\textsc{ovb}})\\
            					& = \delta w_{i} + u_i - \underbrace{\text{x}_{i}' \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{x}_{i}w_{i}]}_{\text{projection of $w_i$ on $x_i$}} \delta\\
            					& =  u_i + \delta \underbrace{\left(w_{i} - \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})\right)}_{\text{part of $w_i$ orthogonal to $x_i$}}\\
						& = u_i + \delta w_{i}^{\perp} & \text{where } w_{i}^{\perp} = w_{i} - \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})
				\end{align*}
				
				And thus,
				\begin{align*}
            				\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}] & = \mathbb{E}[(u_i + \delta w_{i}^{\perp})^{2}| x_{i}]\\
					& = \mathbb{E}[u_{i}^{2} | x_{i}] + 2\delta \mathbb{E}[u_{i}w_{i}^{\perp} | x_{i}] + \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]\\
					& = \sigma_{u}^{2} + 2\delta \left( \mathbb{E}[u_{i}w_{i} | x_{i}] -  \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})\underbrace{\mathbb{E}[u_{i}|x_{i}]}_{0} \right) + \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]\\
					& \overset{\textsc{lie}}{=} \sigma_{u}^{2} + 2\delta \mathbb{E}[w_{i} \underbrace{\mathbb{E}[u_{i} | x_{i}, w_{i}]}_{0} | x_{i}] + \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]\\
					& = \sigma_{u}^{2} +  \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]
				\end{align*}
				
				Notice that, by the law of total variance, $\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}] = \text{Var}(w_{i}^{\perp} | x_{i}) + \mathbb{E}[w_{i}^{\perp} | x_{i}]^{2}$. Now, since $w_{i}^{\perp}$ is the component of $w_{i}$ that is orthogonal to $x_{i}$ and by the projection interpretation of the conditional variance,  $\mathbb{E}[w_{i}^{\perp} | x_{i}] = 0$. And thus, since by assumption $\text{Var}(w_{i}^{\perp} | x_{i}) = \text{Var}(w_{i}^{\perp})$, 
				~
				\begin{align*}
					\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}] & = \text{Var}(w_{i}^{\perp} | x_{i})\\
					& = \text{Var}(w_{i}^{\perp}) \\
					& = \mathbb{E}[(w_{i}^{\perp})^{2}] - \mathbb{E}[w_{i}^{\perp}]^{2}\\
					& = \mathbb{E}\left[ \left(w_{i} - \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})\right)^{2} \right] - \left(\underbrace{\mathbb{E}[w_{i}]}_{0} + \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} \underbrace{\mathbb{E}[x_{i} - \mu_{x}]}_{0} \right)^{2}\\
					& = \underbrace{\mathbb{E}[w_{i}^{2}]}_{\sigma_{w}^{2}} - 2 \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} \left( \underbrace{\mathbb{E}[x_{i}w_{i}]}_{\gamma \sigma_{w}^{2}} - \mu_{x}\underbrace{\mathbb{E}[w_{i}]}_{0} \right) + \dfrac{\gamma^{2}\sigma_{w}^{4}}{\sigma_{x}^{4}} \underbrace{\mathbb{E}[(x_{i} - \mu_{x})^{2}]}_{\sigma_{x}^{2}}\\
					& = \sigma_{w}^{2} \left(1 - \dfrac{\gamma^{2}\sigma_{w}^{2}}{\sigma_{x}^{2}} \right)
				\end{align*}
				
				Note that this variance is well defined (positive) only if $\sigma_{x}^{2} \geq \gamma^{2}\sigma_{w}^{2}$. Under this condition, 
				~
				\begin{equation}
					\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}] =  \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} \left(1 - \dfrac{\gamma^{2}\sigma_{w}^{2}}{\sigma_{x}^{2}} \right)
				\end{equation}
				
				Thus, under our set of assumptions, $\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]$ does not depend on $x_{i}$ and $\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}] =  \mathbb{E}[u_{\textsc{ovb}, i}^{2}]$. We denote this quantity $\sigma_{u_{\textsc{ovb}}}^{2}$.\\
				
				We can now compute the variance of the estimator $\hat{\bm{\beta}}_{\textsc{ovb}}$, noting that $\mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] = \mathbb{E}[\text{x}_{i}\text{x}_{i}'\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]] =  \mathbb{E}[\text{x}_{i}\text{x}_{i}' \sigma_{u_{\textsc{ovb}}}^{2}] = \sigma_{u_{\textsc{ovb}}}^{2} \mathbb{E}[\text{x}_{i}\text{x}_{i}']$. And thus $\mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} = \sigma_{u_{\textsc{ovb}}}^{2} \mathbb{E}[\text{x}_{i}\text{x}_{i}']$.\\
				
				Plugin this and equation \ref{bias_ovb} into equation \ref{asympt_ovb}, we get, for $\hat{\beta}_{\textsc{ovb}}$, the second component of $\hat{\bm{\beta}}_{\textsc{ovb}}$:
				~
				\begin{equation*}
						\hat{\beta}_{\textsc{ovb}} \overset{d}{\to} \mathcal{N}\left( \beta_1 + \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}}, \ \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} \left(1 - \frac{\gamma^{2}\sigma_{w}^{2}}{\sigma_{x}^{2}} \right)}{n \ \sigma_{x}^{2}} \right) 
				\end{equation*}
				
				Then, noting that $\rho_{xw} =  \text{corr}(x, w) = \frac{ \text{cov}(\mu_{x} + \gamma w + \epsilon, w)}{\sigma_{x}\sigma_{w}} = \frac{\gamma\sigma_{w}}{\sigma_{x}}$, we have:
				\[
					\sigma_{\textsc{ovb}}^{2} = \text{avar}\left(\hat{\beta}_{\textsc{ovb}}\right) = \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} \left(1 - \rho_{xw}^{2} \right)}{n \ \sigma_{x}^{2}}
				\]
				~
			\end{proof}

	
%%%%%%%%%% Distrib CTRL  %%%%%%%%%%%%%%%%

\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{ctrl}}$ (lemma \ref{lemma_ctrl})}
	
	\begin{proof}
				The proof is the well know proof of the asymptotic distribution of the OLS. I simply compute $\mathbb{E}[x_{w, i}x_{w, i}']^{-1}$ to retrieve the variance of the parameter of interest $\bm{\beta}_{\textsc{ctrl}}$. We know that we have:
				~
				\[ \sqrt{n}(\hat{\bm{\beta}}_{\textsc{ctrl}} - \bm{\beta}_{\textsc{ctrl}}) \overset{d}{\to} 
					\mathcal{N}\left(0,  \mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1} \sigma_{u}^{2} \right) \]
			
			We are interested in the second component of $\hat{\bm{\beta}}_{\textsc{ctrl}}$. To retrieve it we need to compute $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1}$. 
			~
			\[ 
				\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}'] 
				= \mathbb{E} \begin{bmatrix}
					1 & x_{i} & w\\ 
					x_{i} & x_{i}^{2} & x_{i}w_{i}\\\
					w_{i} & x_{i}w_{i} & w_{i}^{2}
				\end{bmatrix} 
				=
%				\begin{bmatrix}
%					1 & \mu_{x} & 0\\ 
%					\mu_{x} & \sigma_{x}^{2} + \mu_{x}^{2} & \mathbb{E}[xw]\\
%					0 & \mathbb{E}[xw] & \sigma_{w}^{2}
%				\end{bmatrix} 
%				=
				\begin{bmatrix}
					1 & \mu_{x} & 0\\ 
					\mu_{x} & \sigma_{x}^{2} + \mu_{x}^{2} & \gamma\sigma_{w}^{2}\\
					0 & \gamma\sigma_{w}^{2} & \sigma_{w}^{2}
				\end{bmatrix} 
				\]
				
				Note that we have $\mathbb{E}[x_{i}w_{i}] = \mathbb{E}[x_{i}]\underbrace{\mathbb{E}[w_{i}]}_{0} + \text{cov}(x_{i}, w_{i}) = \gamma  \underbrace{\text{var}(w_{i})}_{\sigma_{w}^{2}} + \underbrace{\text{cov}(\epsilon_{i}, w_{i})}_{0} = \gamma\sigma_{w}^{2}$.\\
				
				 Now, $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1} = \dfrac{1}{\text{det}(\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}'])}~^{t}\text{C}$ with C the comatrix of $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']$. We have:
				~
				\[
					\text{det}(\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']) = (\sigma_{x}^{2} + \mu_{x}^{2})\sigma_{w}^{2} -\sigma_{w}^{2}\mu_{x}^{2} - \gamma^{2}\sigma_{w}^{4} = \sigma_{w}^{2}(\sigma_{x}^{2} - \gamma^{2}\sigma_{w}^{2})
				\]
				
				and the ``central'' component of C, $\sigma_{w}^{2}$. Thus the central component of interest of $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1}$ is $\frac{1}{\sigma_{x}^{2} - \gamma^{2}\sigma_{w}^{2}}$. Therefore, for $\hat{\beta}_{\textsc{ctrl}}$, the second component of $\hat{\bm{\beta}}_{\textsc{ctrl}}$, we have:
				\begin{equation}
						\hat{\beta}_{\textsc{ctrl}} \overset{d}{\to}
							 \mathcal{N}\left( \beta_1 , \ \dfrac{\sigma_{u}^{2}}{n \ (\sigma_{x}^{2} - \gamma^{2}\sigma_{w}^{2})} \right) 
				\end{equation}
				
				Then, noting that $\rho_{xw} =  \text{corr}(x, w) = \frac{ \text{cov}(\mu_{x} + \gamma w + \epsilon, w)}{\sigma_{x}\sigma_{w}} = \frac{\gamma\sigma_{w}}{\sigma_{x}}$, we have:
				\[
					\sigma_{\textsc{ctrl}}^{2} = \dfrac{\sigma_{u}^{2}}{n \ \sigma_{x}^{2}(1 - \rho_{xw}^{2})}
				\]
				~
				\end{proof}
	
%%%%%%%%%% Distrib IV  %%%%%%%%%%%%%%%%

\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{iv}}$ (lemma \ref{lemma_iv})}
	
	\begin{proof}
			Since $u_{\textsc{iv}} = u_{\textsc{ovb}} = \delta w + u$, we have $\sigma_{u_{\textsc{iv}}}^2 = \sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}$. Thus, the usual asymptotic distribution of the IV gives:
				~
				\[ \sqrt{n}(\hat{\bm{\beta}}_{\textsc{iv}} - \bm{\beta}) \overset{d}{\to} 
					\mathcal{N}\left(0,  (\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}) \mathbb{E}[\text{z}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{z}_{i}\text{z}_{i}'] \left( \mathbb{E}[\text{z}_{i}\text{x}_{i}']^{-1}\right)' \right) \]
					
			We are interested in the second component of $\hat{\bm{\beta}}_{\textsc{iv}}$. To retrieve it we need to compute $\mathbb{E}[\text{z}_{i} \text{z}_{i}]$, $\mathbb{E}[\text{x}_{i}\text{z}_{i}']^{-1}$ and its transpose.  
			~
			\[ 
				\mathbb{E}[\text{z}_{i}\text{z}_i'] 
				= \mathbb{E} \begin{bmatrix}
					1 & z_{i}\\ 
					z_{i} & z_{i}^{2}
				\end{bmatrix} 
				=
				\begin{bmatrix}
					1 & \mu_{z}\\ 
					\mu_{z} & \sigma_{z}^{2} + \mu_{z}^2
				\end{bmatrix} 
				\]
			
			\[ 
				\mathbb{E}[\text{z}_{i}\text{x}_i'] 
				=
				\begin{bmatrix}
					1 & \mathbb{E}[x_{i}]\\ 
					\mathbb{E}[z_{i}] & \mathbb{E}[z_{i}x_{i}]
				\end{bmatrix} 
				=
				\begin{bmatrix}
					1 & \pi_0 + \pi_1 \mathbb{E}[z_{i}] + \gamma \underbrace{\mathbb{E}[w_{i}]}_{0} + \underbrace{\mathbb{E}[e_{i}]}_{0}\\ 
					\mu_{z} & \pi_0 \mathbb{E}[z_{i}] + \pi_1 \mathbb{E}[z_{i}^{2}] + \gamma \underbrace{\mathbb{E}[z_{i}w_{i}]}_{0} + \underbrace{\mathbb{E}[z_{i}e_{i}]}_{0}
				\end{bmatrix} =
				\begin{bmatrix}
					1 & \pi_0 + \pi_1 \mu_{z}\\ 
					\mu_{z} & \pi_0\mu_z + \pi_1 (\sigma_{z}^{2} + \mu_{z}^{2})
				\end{bmatrix} 
			\]
			\[
				 \Rightarrow \quad
				     \mathbb{E}[\text{z}_{i}\text{x}_i']^{-1} = \dfrac{1}{\pi_1 \sigma_{z}^{2}} 
				     			\begin{bmatrix}
								 \pi_0 \mu_z + \pi_1 (\sigma_{z}^{2} + \mu_{z}^{2})& - \pi_0 - \pi_1 \mu_{z}\\ 
								 -\mu_{z} & 1
							\end{bmatrix} 
			\]
			
		Thus, 
			\[
				     \mathbb{E}[\text{z}_{i}\text{x}_i']^{-1}\mathbb{E}[\text{z}_{i}\text{z}_i']  \left( \mathbb{E}[\text{z}_{i}\text{x}_i']^{-1} \right)' 
				     		= \dfrac{1}{\pi_1 \sigma_{z}^{2}} 
				     			\begin{bmatrix}
								2 \pi_0 \mu_z + \pi_1(\sigma_{z}^{2} + \mu_{z}^{2}) + \frac{\pi_0^2}{\pi_1}&   -\mu_{z} -  \frac{\pi_0}{\pi_1} \\ 
								  -\mu_{z} -  \frac{\pi_0}{\pi_1} & \frac{1}{\pi_1}
							\end{bmatrix} 
			\]
			
			And so, for $\hat{\beta}_{\textsc{iv}}$, the second component of $\hat{\bm{\beta}}_{\textsc{iv}}$, we have:
				\begin{equation}
					\sqrt{n}\left( \hat{\beta}_{\textsc{iv}} - \beta_{1} \right)
					\overset{d}{\to}
					\mathcal{N}\left( 0 , \ \dfrac{\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}}{n \ \pi_{1}^{2}\sigma_{z}^{2}} \right) 
				\end{equation}
				~
				Now, since $ \rho_{xz} = \text{corr}(x_{i}, z_{i}) =  \frac{\text{cov}( \pi_0 + \pi_1 z_i + \gamma w_{i} + e_{i} , z_i)}{\sigma_x \sigma_z} = \pi_1 \frac{\sigma_z}{\sigma_x}$,
				~
				\[
						\sqrt{n}\left( \hat{\beta}_{\textsc{iv}} - \beta_{1} \right)
						\overset{d}{\to}
							 \mathcal{N}\left( 0 , \ \dfrac{\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}}{\sigma_{x}^{2} \rho_{xz}^{2}} \right) 
				\]
			\end{proof}
				
%%%%%%%%%% Distrib RED  %%%%%%%%%%%%%%%%


\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{red}}$ (lemma \ref{lemma_red})}
				
			\begin{proof}
				The proof is straightforward: this is the usual univariate, unbiased case, with and error term equal to $(\delta + \beta_{1}\gamma) w_{i} + u_{i} + \beta_{1}e_{i}$. Since $w$, $u$ and $\epsilon_{\textsc{red}}$ uncorrelated, its variance is $(\delta + \beta_{1}\gamma)^{2} \sigma_{w}^{2} + \sigma_{u}^{2} + \beta_{1}^{2}\sigma_{e}^{2}$.
			\end{proof}
		
\end{document}