@article{_what_2023,
  title = {What's Trending in Difference-in-Differences? {{A}} Synthesis of the Recent Econometrics Literature},
  shorttitle = {What's Trending in Difference-in-Differences?},
  year = {2023},
  month = aug,
  journal = {Journal of Econometrics},
  volume = {235},
  number = {2},
  pages = {2218--2244},
  publisher = {North-Holland},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2023.03.008},
  urldate = {2024-07-17},
  abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners.{\dots}},
  langid = {american},
  file = {/Users/vincentbagilet/Zotero/storage/I8HTA4S5/2023 - What’s trending in difference-in-differences A sy.pdf;/Users/vincentbagilet/Zotero/storage/RU4FX9Z2/S0304407623001318.html}
}

@article{abadie_statistical_2020,
  title = {Statistical {{Nonsignificance}} in {{Empirical Economics}}},
  author = {Abadie, Alberto},
  year = {2020},
  month = jun,
  journal = {American Economic Review: Insights},
  volume = {2},
  number = {2},
  pages = {193--208},
  issn = {2640-205X, 2640-2068},
  doi = {10.1257/aeri.20190252},
  urldate = {2021-06-29},
  abstract = {Statistical significance is often interpreted as providing greater information than nonsignificance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts that are common in economics, where data-sets are large and there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. Therefore, we advocate visible reporting and discussion of nonsignificant results. (JEL C12, C90)},
  langid = {english},
  keywords = {Example,NHST,Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/23UWSYFW/Abadie - 2020 - Statistical Nonsignificance in Empirical Economics.pdf}
}

@article{abeler_reference_2011,
  title = {Reference {{Points}} and {{Effort Provision}}},
  author = {Abeler, Johannes and Falk, Armin and Goette, Lorenz and Huffman, David},
  year = {2011},
  month = apr,
  journal = {American Economic Review},
  volume = {101},
  number = {2},
  pages = {470--492},
  issn = {0002-8282},
  doi = {10.1257/aer.101.2.470},
  urldate = {2024-06-28},
  abstract = {A key open question for theories of reference-dependent preferences is: what determines the reference point? One candidate is expectations: what people expect could affect how they feel about what actually occurs. In a real-effort experiment, we manipulate the rational expectations of subjects and check whether this manipulation influences their effort provision. We find that effort provision is significantly different between treatments in the way predicted by models of expectation-based, reference-dependent preferences: if expectations are high, subjects work longer and earn more money than if expectations are low. (JEL D12, D84, J22)},
  langid = {english},
  keywords = {Consumer Economics: Empirical Analysis Expectations,Speculations Time Allocation and Labor Supply},
  file = {/Users/vincentbagilet/Zotero/storage/5SIG9P64/Abeler et al. - 2011 - Reference Points and Effort Provision.pdf}
}

@article{adhvaryu_light_2020,
  title = {The {{Light}} and the {{Heat}}: {{Productivity Co-Benefits}} of {{Energy-Saving Technology}}},
  shorttitle = {The {{Light}} and the {{Heat}}},
  author = {Adhvaryu, Achyuta and Kala, Namrata and Nyshadham, Anant},
  year = {2020},
  month = oct,
  journal = {The Review of Economics and Statistics},
  volume = {102},
  number = {4},
  pages = {779--792},
  issn = {0034-6535},
  doi = {10.1162/rest_a_00886},
  urldate = {2024-05-11},
  abstract = {We study the adoption of energy-efficient LED lighting in garment factories around Bangalore, India. Combining daily production line--level data with weather data, we estimate a negative, nonlinear productivity-temperature gradient. We find that LED lighting raises productivity on hot days. Using the firm's costs data, we estimate that the payback period for LED adoption is less than one-third the length after accounting for productivity co-benefits. The average factory in our data gains about \$2,880 in power consumption savings and about \$7,500 in productivity gains.},
  keywords = {Labor,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/8V2YLQUN/Adhvaryu et al. - 2020 - The Light and the Heat Productivity Co-Benefits o.pdf;/Users/vincentbagilet/Zotero/storage/ZWBC38G4/The-Light-and-the-Heat-Productivity-Co-Benefits-of.html}
}

@article{amrhein_inferential_2019,
  title = {Inferential {{Statistics}} as {{Descriptive Statistics}}: {{There Is No Replication Crisis}} If {{We Don}}'t {{Expect Replication}}},
  shorttitle = {Inferential {{Statistics}} as {{Descriptive Statistics}}},
  author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {262--270},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1543137},
  urldate = {2022-02-01},
  abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a ``replication crisis'' may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
  langid = {english},
  keywords = {Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/5ISKITED/Amrhein et al. - 2019 - Inferential Statistics as Descriptive Statistics .pdf;/Users/vincentbagilet/Zotero/storage/5PREM7H5/00031305.2018.pdf}
}

@article{anderson-cook_experimental_2005,
  title = {Experimental and {{Quasi-Experimental Designs}} for {{Generalized Causal Inference}}},
  author = {{Anderson-Cook}, Christine M},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {708--708},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2005.s22},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/AM4RXJGN/Anderson-Cook - 2005 - Experimental and Quasi-Experimental Designs for Ge.pdf}
}

@article{andrews_identification_2019,
  title = {Identification of and {{Correction}} for {{Publication Bias}}},
  author = {Andrews, Isaiah and Kasy, Maximilian},
  year = {2019},
  month = aug,
  journal = {American Economic Review},
  volume = {109},
  number = {8},
  pages = {2766--2794},
  issn = {0002-8282},
  doi = {10.1257/aer.20180310},
  urldate = {2022-01-13},
  abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.},
  langid = {english},
  keywords = {Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/88YV27KP/Andrews and Kasy - 2019 - Identification of and Correction for Publication B.pdf}
}

@article{andrews_weak_2019,
  title = {Weak {{Instruments}} in {{Instrumental Variables Regression}}: {{Theory}} and {{Practice}}},
  shorttitle = {Weak {{Instruments}} in {{Instrumental Variables Regression}}},
  author = {Andrews, Isaiah and Stock, James H. and Sun, Liyang},
  year = {2019},
  journal = {Annual Review of Economics},
  volume = {11},
  number = {1},
  pages = {727--753},
  doi = {10.1146/annurev-economics-080218-025643},
  urldate = {2022-04-21},
  abstract = {When instruments are weakly correlated with endogenous regressors, conventional methods for instrumental variables (IV) estimation and inference become unreliable. A large literature in econometrics has developed procedures for detecting weak instruments and constructing robust confidence sets, but many of the results in this literature are limited to settings with independent and homoskedastic data, while data encountered in practice frequently violate these assumptions. We review the literature on weak instruments in linear IV regression with an emphasis on results for nonhomoskedastic (heteroskedastic, serially correlated, or clustered) data. To assess the practical importance of weak instruments, we also report tabulations and simulations based on a survey of papers published in the American Economic Review from 2014 to 2018 that use IV. These results suggest that weak instruments remain an important issue for empirical practice, and that there are simple steps that researchers can take to better handle weak instruments in applications.},
  keywords = {IV,Literature review,Maths,Weak IV},
  file = {/Users/vincentbagilet/Zotero/storage/7D7WJ5AP/Andrews et al. - 2019 - Weak Instruments in Instrumental Variables Regress.pdf}
}

@article{angrist_credibility_2010,
  title = {The {{Credibility Revolution}} in {{Empirical Economics}}: {{How Better Research Design Is Taking}} the {{Con}} out of {{Econometrics}}},
  shorttitle = {The {{Credibility Revolution}} in {{Empirical Economics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = {2010},
  month = jun,
  journal = {Journal of Economic Perspectives},
  volume = {24},
  number = {2},
  pages = {3--30},
  issn = {0895-3309},
  doi = {10.1257/jep.24.2.3},
  urldate = {2022-02-09},
  abstract = {Since Edward Leamer's memorable 1983 paper, "Let's Take the Con out of Econometrics," empirical microeconomics has experienced a credibility revolution. While Leamer's suggested remedy, sensitivity analysis, has played a role in this, we argue that the primary engine driving improvement has been a focus on the quality of empirical research designs. The advantages of a good research design are perhaps most easily apparent in research using random assignment. We begin with an overview of Leamer's 1983 critique and his proposed remedies. We then turn to the key factors we see contributing to improved empirical work, including the availability of more and better data, along with advances in theoretical econometric understanding, but especially the fact that research design has moved front and center in much of empirical micro. We offer a brief digression into macroeconomics and industrial organization, where progress -- by our lights -- is less dramatic, although there is work in both fields that we find encouraging. Finally, we discuss the view that the design pendulum has swung too far. Critics of design-driven studies argue that in pursuit of clean and credible research designs, researchers seek good answers instead of good questions. We briefly respond to this concern, which worries us little.},
  langid = {english},
  keywords = {Econometrics,Economic Methodology},
  file = {/Users/vincentbagilet/Zotero/storage/3SRYGLIT/Angrist and Pischke - 2010 - The Credibility Revolution in Empirical Economics.pdf}
}

@article{angrist_identification_1996,
  title = {Identification of {{Causal Effects Using Instrumental Variables}}},
  author = {Angrist, Joshua D. and Imbens, Guido W. and Rubin, Donald B.},
  year = {1996},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {91},
  number = {434},
  pages = {444--455},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.1996.10476902},
  urldate = {2022-12-09},
  abstract = {We outline a framework for causal inference in settings where assignment to a binary treatment is ignorable, but compliance with the assignment is not perfect so that the receipt of treatment is nonignorable. To address the problems associated with comparing subjects by the ignorable assignment---an ``intention-to-treat analysis''---we make use of instrumental variables, which have long been used by economists in the context of regression models with constant treatment effects. We show that the instrumental variables (IV) estimand can be embedded within the Rubin Causal Model (RCM) and that under some simple and easily interpretable assumptions, the IV estimand is the average causal effect for a subgroup of units, the compliers. Without these assumptions, the IV estimand is simply the ratio of intention-to-treat causal estimands with no interpretation as an average causal effect. The advantages of embedding the IV approach in the RCM are that it clarifies the nature of critical assumptions needed for a causal interpretation, and moreover allows us to consider sensitivity of the results to deviations from key assumptions in a straightforward manner. We apply our analysis to estimate the effect of veteran status in the Vietnam era on mortality, using the lottery number that assigned priority for the draft as an instrument, and we use our results to investigate the sensitivity of the conclusions to critical assumptions.},
  keywords = {Compliers,ITT,IV,Maths,STUVA,Thomas},
  file = {/Users/vincentbagilet/Zotero/storage/33PEE24W/Angrist et al. - 1996 - Identification of Causal Effects Using Instrumenta.pdf}
}

@book{angrist_mastering_2014,
  title = {Mastering '{{Metrics}}: {{The Path}} from {{Cause}} to {{Effect}}},
  shorttitle = {Mastering '{{Metrics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = {2014},
  month = dec,
  publisher = {Princeton University Press},
  abstract = {From Joshua Angrist, winner of the Nobel Prize in Economics, and J{\"o}rn-Steffen Pischke, an accessible and fun guide to the essential tools of econometric researchApplied econometrics, known to aficionados as 'metrics, is the original data science. 'Metrics encompasses the statistical methods economists use to untangle cause and effect in human affairs. Through accessible discussion and with a dose of kung fu--themed humor, Mastering 'Metrics presents the essential tools of econometric research and demonstrates why econometrics is exciting and useful.The five most valuable econometric methods, or what the authors call the Furious Five--random assignment, regression, instrumental variables, regression discontinuity designs, and differences in differences--are illustrated through well-crafted real-world examples (vetted for awesomeness by Kung Fu Panda's Jade Palace). Does health insurance make you healthier? Randomized experiments provide answers. Are expensive private colleges and selective public high schools better than more pedestrian institutions? Regression analysis and a regression discontinuity design reveal the surprising truth. When private banks teeter, and depositors take their money and run, should central banks step in to save them? Differences-in-differences analysis of a Depression-era banking crisis offers a response. Could arresting O. J. Simpson have saved his ex-wife's life? Instrumental variables methods instruct law enforcement authorities in how best to respond to domestic abuse.Wielding econometric tools with skill and confidence, Mastering 'Metrics uses data and statistics to illuminate the path from cause to effect.Shows why econometrics is importantExplains econometric research through humorous and accessible discussionOutlines empirical methods central to modern econometric practiceWorks through interesting and relevant real-world examples},
  isbn = {978-1-4008-5238-3},
  langid = {english},
  keywords = {Business & Economics / Econometrics},
  file = {/Users/vincentbagilet/Zotero/storage/YUQMFUBX/Angrist and Pischke - 2015 - Mastering 'metrics the path from cause to effect.pdf}
}

@book{angrist_mostly_2009,
  title = {Mostly {{Harmless Econometrics}}: {{An Empiricist}}'s {{Companion}}},
  shorttitle = {Mostly {{Harmless Econometrics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = {2009},
  month = jan,
  edition = {1 edition},
  publisher = {Princeton University Press},
  address = {Princeton},
  abstract = {The core methods in today's econometric toolkit are linear regression for statistical control, instrumental variables methods for the analysis of natural experiments, and differences-in-differences methods that exploit policy changes. In the modern experimentalist paradigm, these techniques address clear causal questions such as: Do smaller classes increase learning? Should wife batterers be arrested? How much does education raise wages? Mostly Harmless Econometrics shows how the basic tools of applied econometrics allow the data to speak.  In addition to econometric essentials, Mostly Harmless Econometrics covers important new extensions--regression-discontinuity designs and quantile regression--as well as how to get standard errors right. Joshua Angrist and J{\"o}rn-Steffen Pischke explain why fancier econometric techniques are typically unnecessary and even dangerous. The applied econometric methods emphasized in this book are easy to use and relevant for many areas of contemporary social science.  An irreverent review of econometric essentials  A focus on tools that applied researchers use most  Chapters on regression-discontinuity designs, quantile regression, and standard errors  Many empirical examples  A clear and concise resource with wide applications},
  isbn = {978-0-691-12035-5},
  langid = {english},
  keywords = {DiD,Econometrics,Handbook,IV,Methods,RDD,Theory},
  file = {/Users/vincentbagilet/Zotero/storage/62ZYMP6F/recrut_econometrics.pdf}
}

@techreport{arel-bundockQuantitativePoliticalScience2022,
  type = {Working {{Paper}}},
  title = {Quantitative {{Political Science Research}} Is {{Greatly Underpowered}}},
  author = {{Arel-Bundock}, Vincent and Briggs, Ryan C. and Doucouliagos, Hristos and Mendoza Avi{\~n}a, Marco and Stanley, Tom D.},
  year = {2022},
  number = {6},
  institution = {I4R Discussion Paper Series},
  urldate = {2024-04-11},
  abstract = {The social sciences face a replicability crisis. A key determinant of replication success is statistical power. We assess the power of political science research by collating over 16,000 hypothesis tests from about 2,000 articles. Using generous assumptions, we find that the median analysis has about 10\% power and that only about 1 in 10 tests have at least 80\% power to detect the consensus effects reported in the literature. We also find substantial heterogeneity in tests across research areas, with some being characterized by high power but most having very low power. To contextualize our findings, we survey political methodologists to assess their expectations about power levels. Most methodologists greatly overestimate the statistical power of political science research.},
  copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/MA9KJZTH/Arel-Bundock et al. - 2022 - Quantitative Political Science Research is Greatly.pdf}
}

@techreport{aronow_does_2015,
  type = {{{SSRN Scholarly Paper}}},
  title = {Does {{Regression Produce Representative Estimates}} of {{Causal Effects}}?},
  author = {Aronow, Peter M. and Samii, Cyrus},
  year = {2015},
  month = feb,
  number = {ID 2224964},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  urldate = {2021-12-14},
  abstract = {With an unrepresentative sample, the estimate of a causal effect may fail to characterize how effects operate in the population of interest.  What is less well understood is that conventional estimation practices for observational studies may produce the same problem even with a representative sample.  Causal effects estimated via multiple regression differentially weight each unit's contribution.  The "effective sample'' that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce.  There is no general external validity basis for preferring multiple regression on representative samples over quasi-experimental or experimental methods.  We show how to estimate the "multiple regression weights'' that allow one to study the effective sample. We discuss alternative approaches that, under certain conditions, recover representative average causal effects.  The requisite conditions cannot always be met.},
  langid = {english},
  keywords = {Solutions,Variations},
  file = {/Users/vincentbagilet/Zotero/storage/I95XZK2R/Aronow and Samii - 2015 - Does Regression Produce Representative Estimates o.pdf}
}

@article{aronow_does_2016,
  title = {Does {{Regression Produce Representative Estimates}} of {{Causal Effects}}?},
  author = {Aronow, Peter M. and Samii, Cyrus},
  year = {2016},
  journal = {American Journal of Political Science},
  volume = {60},
  number = {1},
  pages = {250--267},
  issn = {1540-5907},
  doi = {10.1111/ajps.12185},
  urldate = {2022-05-05},
  abstract = {With an unrepresentative sample, the estimate of a causal effect may fail to characterize how effects operate in the population of interest. What is less well understood is that conventional estimation practices for observational studies may produce the same problem even with a representative sample. Causal effects estimated via multiple regression differentially weight each unit's contribution. The ``effective sample'' that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce. There is no general external validity basis for preferring multiple regression on representative samples over quasi-experimental or experimental methods. We show how to estimate the ``multiple regression weights'' that allow one to study the effective sample. We discuss alternative approaches that, under certain conditions, recover representative average causal effects. The requisite conditions cannot always be met.},
  langid = {english},
  keywords = {External validity,Maths,Weights},
  file = {/Users/vincentbagilet/Zotero/storage/QHNQAZNJ/Aronow and Samii - 2016 - Does Regression Produce Representative Estimates o.pdf}
}

@article{athey_econometrics_2016,
  title = {The {{Econometrics}} of {{Randomized Experiments}}},
  author = {Athey, Susan and Imbens, Guido},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.00698 [econ, stat]},
  eprint = {1607.00698},
  primaryclass = {econ, stat},
  urldate = {2022-02-01},
  abstract = {In this chapter, we present econometric and statistical methods for analyzing randomized experiments. For basic experiments we stress randomization-based inference as opposed to sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. We show how this perspective relates to regression analyses for randomized experiments. We discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. We also discuss complications in randomized experiments such as noncompliance. In the presence of non-compliance we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider in detail estimation and inference for heterogenous treatment effects in settings with (possibly many) covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62K99,Economics - Econometrics,G.3,Statistics - Methodology},
  file = {/Users/vincentbagilet/Zotero/storage/BF53F535/Athey and Imbens - 2016 - The Econometrics of Randomized Experiments.pdf}
}

@article{auffhammer_using_2013,
  title = {Using {{Weather Data}} and {{Climate Model Output}} in {{Economic Analyses}} of {{Climate Change}}},
  author = {Auffhammer, Maximilian and Hsiang, Solomon M. and Schlenker, Wolfram and Sobel, Adam},
  year = {2013},
  month = jul,
  journal = {Review of Environmental Economics and Policy},
  volume = {7},
  number = {2},
  pages = {181--198},
  issn = {1750-6816, 1750-6824},
  doi = {10.1093/reep/ret016},
  urldate = {2023-05-03},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/7UVP98JU/Auffhammer et al. - 2013 - Using Weather Data and Climate Model Output in Eco.pdf}
}

@misc{awareeye_finite_2020,
  type = {Forum Post},
  title = {Finite Sample Variance of {{OLS}} Estimator for Random Regressor},
  author = {{awareeye}},
  year = {2020},
  month = jan,
  journal = {Mathematics Stack Exchange},
  urldate = {2022-04-28}
}

@article{bagilet_accurate_2023,
  title = {Accurate {{Estimation}} of {{Small Effects}}: {{Illustration Through Air Pollution}} and {{Health}}},
  author = {Bagilet, Vincent},
  year = {2023},
  month = mar,
  abstract = {This paper evaluates the ability of various research designs in measuring relatively small effects, that of air pollution on health in the short-term. Analyzing the epidemiology and economics literature, we first show that a meaningful portion of these estimates are likely inaccurate. Relatively small effects are not only difficult to detect, but in the presence of publication bias resulting estimates also exaggerate true effect sizes. We identify the design parameters causing this issue using real data simulations replicating most prevailing inference methods. Finally, we propose a principled workflow to evaluate and avoid exaggeration when conducting an observational study.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/DJSYUJ58/Bagilet - Accurately Estimating Relatively Small Eﬀects Air.pdf}
}

@article{baiocchi_instrumental_2014,
  title = {Instrumental Variable Methods for Causal Inference},
  author = {Baiocchi, Michael and Cheng, Jing and Small, Dylan S.},
  year = {2014},
  journal = {Statistics in Medicine},
  volume = {33},
  number = {13},
  pages = {2297--2340},
  issn = {1097-0258},
  doi = {10.1002/sim.6128},
  urldate = {2022-07-28},
  abstract = {AbstractA goal of many health studies is to determine the causal effect of a treatment or intervention on health outcomes. Often, it is not ethically or practically possible to conduct a perfectly randomized experiment, and instead, an observational study must be used. A major challenge to the validity of observational studies is the possibility of unmeasured confounding (i.e., unmeasured ways in which the treatment and control groups differ before treatment administration, which also affect the outcome). Instrumental variables analysis is a method for controlling for unmeasured confounding. This type of analysis requires the measurement of a valid instrumental variable, which is a variable that (i) is independent of the unmeasured confounding; (ii) affects the treatment; and (iii) affects the outcome only indirectly through its effect on the treatment. This tutorial discusses the types of causal effects that can be estimated by instrumental variables analysis; the assumptions needed for instrumental variables analysis to provide valid estimates of causal effects and sensitivity analysis for those assumptions; methods of estimation of causal effects using instrumental variables; and sources of instrumental variables in health studies. Copyright {\copyright} 2014 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Compliers,IV,Maths},
  file = {/Users/vincentbagilet/Zotero/storage/JFHYH5FA/Baiocchi et al. - 2014 - Instrumental variable methods for causal inference.pdf}
}

@misc{baker_difference--differences_2019,
  title = {Difference-in-{{Differences Methodology}}},
  author = {Baker, Andrew C.},
  year = {2019},
  month = sep,
  journal = {Andrew Baker},
  urldate = {2021-12-14},
  abstract = {Introduction In this methodological section I will explain the issues with difference-in-differences (DiD) designs when there are multiple units and more than two time periods, and also the particular issues that arise when the treatment is conducted at staggered periods in time. In the canonical DiD set-up (e.g.~the Card and Kreuger minimum wage study comparing New Jersey and Pennsylvania) there are two units and two time periods, with one of the units being treated in the second period.},
  howpublished = {https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/},
  langid = {american}
}

@article{bennett_how_2020,
  title = {How {{Far}} Is {{Too Far}}? {{Estimation}} of an {{Interval}} for {{Generalization}} of a {{Regression Discontinuity Design Away}} from the {{Cutoff}}},
  author = {Bennett, Magdalena},
  year = {2020},
  month = mar,
  pages = {50},
  abstract = {Regression discontinuity designs are a commonly used approach for causal inference in observational studies. Under mild continuity assumptions, the method provides a robust estimate of the average treatment effect for observations directly at the threshold of assignment. However, it has limited external validity for populations away from the cutoff. This article proposes a strategy to overcome this limitation by identifying a wider interval around the cutoff for estimation using a Generalization of a Regression Discontinuity Design (GRD). In this interval, predictive covariates are used to explain away the relationship between the assignment score and the outcome of interest for the pre-intervention period. Under the partially-testable assumption of conditional timeinvariance in absence of the treatment, the generalization bandwidth can be applied to the post-intervention period, allowing for the estimation of average treatment effects for populations away from the cutoff. To illustrate this method, GRD is applied in the context of free higher education in Chile to estimate effects for vulnerable students.},
  langid = {english},
  keywords = {RDD},
  file = {/Users/vincentbagilet/Zotero/storage/2LIVM2SG/Bennett - How Far is Too Far Estimation of an Interval for .pdf;/Users/vincentbagilet/Zotero/storage/3LGFRK6F/Bennett - How Far is Too Far Estimation of an Interval for .pdf;/Users/vincentbagilet/Zotero/storage/FAKVUUSD/Bennett - How Far is Too Far Estimation of an Interval for .pdf;/Users/vincentbagilet/Zotero/storage/N5Q7WBP9/Bennett - How Far is Too Far Estimation of an Interval for .pdf}
}

@article{bind_causal_2019,
  title = {Causal {{Modeling}} in {{Environmental Health}}},
  author = {Bind, Marie-Ab{\`e}le},
  year = {2019},
  month = apr,
  journal = {Annual Review of Public Health},
  volume = {40},
  number = {1},
  pages = {23--43},
  issn = {0163-7525, 1545-2093},
  doi = {10.1146/annurev-publhealth-040218-044048},
  urldate = {2021-06-10},
  abstract = {The field of environmental health has been dominated by modeling associations, especially by regressing an observed outcome on a linear or nonlinear function of observed covariates. Readers interested in advances in policies for improving environmental health are, however, expecting to be informed about health effects resulting from, or more explicitly caused by, environmental exposures. The quantification of health impacts resulting from the removal of environmental exposures involves causal statements. Therefore, when possible, causal inference frameworks should be considered for analyzing the effects of environmental exposures on health outcomes.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/H5XNNMLC/Bind - 2019 - Causal Modeling in Environmental Health.pdf}
}

@techreport{black_simulated_2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {Simulated {{Power Analyses}} for {{Observational Studies}}: {{An Application}} to the {{Affordable Care Act Medicaid Expansion}}},
  shorttitle = {Simulated {{Power Analyses}} for {{Observational Studies}}},
  author = {Black, Bernard S. and Hollingsworth, Alex and Nunes, Leticia and Simon, Kosali Ilayperuma},
  year = {2021},
  month = mar,
  number = {ID 3368187},
  address = {Rochester, NY},
  institution = {Social Science Research Network},
  doi = {10.2139/ssrn.3368187},
  urldate = {2022-01-25},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate.An analysis with low power is unlikely to produce convincing evidence of atreatment effect even when one exists. Of greater concern, a statistically significantestimate from a low-powered analysis is likely to overstate the magnitude of the trueeffect size, often finding estimates of the wrong sign or that are several times toolarge. Yet statistical power is rarely reported in published economics work. This is inpart because modern research designs are complex enough that power cannot alwaysbe easily ascertained using simple formulae. Power can also be difficult to estimatein observational settings where researchers may not know---and have no ability tomanipulate---the true treatment effect or other parameters of interest. Using an appliedexample---the link between gaining health insurance and mortality---we conducta simulated power analysis to outline the importance of power and ways to estimatepower in complex research settings. We find that standard difference-in-differencesand triple differences analyses of Medicaid expansions using county or state mortalitydata would need to induce reductions in population mortality of at least 2\% to be wellpowered. While there is no single, correct method for conducting a simulated poweranalysis, our manuscript outlines decisions relevant for applied researchers interestedin conducting simulations appropriate to other settings.},
  langid = {english},
  keywords = {Event study},
  file = {/Users/vincentbagilet/Zotero/storage/E4I7G8CN/Black et al. - 2021 - Simulated Power Analyses for Observational Studies.pdf}
}

@article{black_simulated_2022,
  title = {Simulated Power Analyses for Observational Studies: {{An}} Application to the {{Affordable Care Act Medicaid}} Expansion},
  shorttitle = {Simulated Power Analyses for Observational Studies},
  author = {Black, Bernard and Hollingsworth, Alex and Nunes, Let{\'i}cia and Simon, Kosali},
  year = {2022},
  month = sep,
  journal = {Journal of Public Economics},
  volume = {213},
  pages = {104713},
  issn = {0047-2727},
  doi = {10.1016/j.jpubeco.2022.104713},
  urldate = {2024-04-11},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate. An analysis with low power is unlikely to produce convincing evidence of a treatment effect even when one exists. Of greater concern, a statistically significant estimate from a low-powered analysis is likely to misstate the true effect size, including finding estimates of the wrong sign or that are several times too large. Yet statistical power is rarely reported in published economics work. This is in part because many modern research designs are complex enough that power cannot be easily ascertained using simple formulae. Power can also be difficult to estimate in observational settings. Using an applied example--the link between gaining health insurance and mortality--we conduct a simulated power analysis to outline the importance of power and ways to estimate power in complex research settings. We find that standard difference-in-differences and triple differences analyses of Medicaid expansions using county or state mortality data would need to induce reductions in population mortality of at least 2\% to be well powered. While there is no single, correct method for conducting a simulated power analysis, our manuscript outlines how applied researchers can conduct simulations appropriate to their settings.},
  keywords = {Health,Health insurance,Power,Real data simulations,Simulations,US},
  file = {/Users/vincentbagilet/Zotero/storage/6CDCJTC6/Black et al. - 2022 - Simulated power analyses for observational studies.pdf;/Users/vincentbagilet/Zotero/storage/4933IHNM/S0047272722001153.html}
}

@article{bloom_modern_2012,
  title = {Modern {{Regression Discontinuity Analysis}}},
  author = {Bloom, Howard S.},
  year = {2012},
  month = jan,
  journal = {Journal of Research on Educational Effectiveness},
  volume = {5},
  number = {1},
  pages = {43--82},
  issn = {1934-5747, 1934-5739},
  doi = {10.1080/19345747.2011.578707},
  urldate = {2023-02-10},
  abstract = {This article provides a detailed discussion of the theory and practice of modern regression discontinuity (RD) analysis for estimating the effects of interventions or treatments. Part 1 briefly chronicles the history of RD analysis and summarizes its past applications. Part 2 explains how in theory an RD analysis can identify an average effect of treatment for a population and how different types of RD analyses---``sharp'' versus ``fuzzy''---can identify average treatment effects for different conceptual subpopulations. Part 3 of the article introduces graphical methods, parametric statistical methods, and nonparametric statistical methods for estimating treatment effects in practice from regression discontinuity data plus validation tests and robustness tests for assessing these estimates. Section 4 considers generalizing RD findings and presents several different views on and approaches to the issue. Part 5 notes some important issues to pursue in future research about or applications of RD analysis.},
  langid = {english},
  keywords = {Maths,R2,RDD,Variance},
  file = {/Users/vincentbagilet/Zotero/storage/YI54JLJ2/Bloom - 2012 - Modern Regression Discontinuity Analysis.pdf}
}

@article{blumberg_causal_2016,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}: {{Book Reviews}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Blumberg, Carol Joyce},
  year = {2016},
  month = apr,
  journal = {International Statistical Review},
  volume = {84},
  number = {1},
  pages = {159--159},
  issn = {03067734},
  doi = {10.1111/insr.12170},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/G8CKK59D/Blumberg - 2016 - Causal Inference for Statistics, Social, and Biome.pdf}
}

@article{brewer_advances_2023,
  title = {Advances in {{Causal Inference}} at the {{Intersection}} of {{Air Pollution}} and {{Health Outcomes}}},
  author = {Brewer, Dylan and Dench, Daniel and Taylor, Laura O.},
  year = {2023},
  month = oct,
  journal = {Annual Review of Resource Economics},
  volume = {15},
  number = {Volume 15, 2023},
  pages = {455--469},
  publisher = {Annual Reviews},
  issn = {1941-1340, 1941-1359},
  doi = {10.1146/annurev-resource-101722-081026},
  urldate = {2024-05-17},
  abstract = {This article provides an overview of the recent economics literature analyzing the effect of air pollution on health outcomes. We review the common approaches to measuring and modeling air pollution exposures and the epidemiological and biological literature on health outcomes that undergird federal air regulations in the United States. The article contrasts the methods used in the epidemiology literature with the causal inference framework used in economics. In particular, we review the common sources of estimation bias in epidemiological approaches that the economics literature has sought to overcome with research designs that take advantage of natural experiments. We review new promising research designs for estimating concentration-response functions and identify areas for further research.},
  langid = {english},
  keywords = {Air pollution,Review},
  file = {/Users/vincentbagilet/Zotero/storage/37YJDHR7/Brewer et al. - 2023 - Advances in Causal Inference at the Intersection o.pdf;/Users/vincentbagilet/Zotero/storage/IK6IP28W/annurev-resource-101722-081026.html}
}

@article{brewer_inference_2018,
  title = {Inference with {{Difference-in-Differences Revisited}}},
  author = {Brewer, Mike and Crossley, Thomas F. and Joyce, Robert},
  year = {2018},
  month = jan,
  journal = {Journal of Econometric Methods},
  volume = {7},
  number = {1},
  publisher = {De Gruyter},
  issn = {2156-6674},
  doi = {10.1515/jem-2017-0005},
  urldate = {2021-12-13},
  abstract = {A growing literature on inference in difference-in-differences (DiD) designs has been pessimistic about obtaining hypothesis tests of the correct size, particularly with few groups. We provide Monte Carlo evidence for four points: (i) it is possible to obtain tests of the correct size even with few groups, and in many settings very straightforward methods will achieve this; (ii) the main problem in DiD designs with grouped errors is instead low power to detect real effects; (iii) feasible GLS estimation combined with robust inference can increase power considerably whilst maintaining correct test size -- again, even with few groups, and (iv) using OLS with robust inference can lead to a perverse relationship between power and panel length.},
  langid = {english},
  keywords = {DID},
  file = {/Users/vincentbagilet/Zotero/storage/BEJDCC8L/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf;/Users/vincentbagilet/Zotero/storage/RTTNS8XE/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf;/Users/vincentbagilet/Zotero/storage/TUGNRABU/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf;/Users/vincentbagilet/Zotero/storage/X7WWSE7N/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf}
}

@article{broderick_automatic_2021,
  title = {An {{Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  shorttitle = {An {{Automatic Finite-Sample Robustness Metric}}},
  author = {Broderick, Tamara and Giordano, Ryan and Meager, Rachael},
  year = {2021},
  month = nov,
  journal = {arXiv:2011.14999 [econ, stat]},
  eprint = {2011.14999},
  primaryclass = {econ, stat},
  urldate = {2022-01-25},
  abstract = {We propose a method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data. Manually checking the influence of all possible small subsets is computationally infeasible, so we provide an approximation to find the most influential subset. Our metric, the "Approximate Maximum Influence Perturbation," is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several economics papers can be overturned by removing less than 1\% of the sample.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/Users/vincentbagilet/Zotero/storage/KRAKUDI9/Broderick et al. - 2021 - An Automatic Finite-Sample Robustness Metric When.pdf}
}

@article{brodeur_methods_2020,
  title = {Methods {{Matter}}: P-{{Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2020},
  month = nov,
  journal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  urldate = {2022-01-13},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal "revise and resubmit" process mitigates the problem; (iii) things are improving through time.},
  langid = {english},
  keywords = {and Selection,Hypothesis Testing: General,Model Evaluation,Publication bias,Sociology of Economics,Validation},
  file = {/Users/vincentbagilet/Zotero/storage/D6VNN7BM/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf;/Users/vincentbagilet/Zotero/storage/JA32AEG7/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf;/Users/vincentbagilet/Zotero/storage/JDQPIVBV/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf;/Users/vincentbagilet/Zotero/storage/YHKENYEL/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf}
}

@article{brodeur_star_2016,
  title = {Star {{Wars}}: {{The Empirics Strike Back}}},
  shorttitle = {Star {{Wars}}},
  author = {Brodeur, Abel and L{\'e}, Mathias and Sangnier, Marc and Zylberberg, Yanos},
  year = {2016},
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {1945-7782},
  doi = {10.1257/app.20150044},
  urldate = {2023-11-14},
  abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
  langid = {english},
  keywords = {Market for Economists Estimation: General,Role of Economics,Role of Economists},
  file = {/Users/vincentbagilet/Zotero/storage/R2AZFIBT/Brodeur et al. - 2016 - Star Wars The Empirics Strike Back.pdf}
}

@techreport{brodeur_unpacking_2023,
  title = {Unpacking {{P-Hacking}} and {{Publication Bias}}},
  author = {Brodeur, Abel and Carrell, Scott E. and Figlio, David N. and Lusher, Lester R.},
  year = {2023},
  month = aug,
  number = {w31548},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w31548},
  urldate = {2023-08-28},
  abstract = {Founded in 1920, the NBER is a private, non-profit, non-partisan organization dedicated to conducting economic research and to disseminating research findings among academics, public policy makers, and business professionals.},
  langid = {english},
  keywords = {Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/ID8GUP8P/Brodeur et al. - 2023 - Unpacking P-Hacking and Publication Bias.pdf}
}

@article{bunnenberg_size_nodate,
  title = {Size and Power of Difference-in-Differences Studies in Financial Economics: {{An}} Approximate Permutation Test},
  author = {Bunnenberg, Sebastian and Meyer, Steffen},
  pages = {26},
  abstract = {Researchers use difference-in-differences models to evaluate the causal effects of policy changes. As the empirical correlation across firms and time is usually unknown, estimating consistent standard errors is difficult and statistical inferences may be biased. We suggest an approximate permutation test using simulated interventions to reveal the empirical error distribution of estimated policy effects. In contrast to existing econometric corrections, such as single- or double-clustering, our approach does not impose any parametric form on the data. In comparison to alternative parametric tests, our procedure maintains correct size with simulated and real-world interventions. Simultaneously, it improves power.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/YI66AXPC/Bunnenberg and Meyer - Size and power of diﬀerence-in-diﬀerences studies .pdf}
}

@misc{burke_game_2023,
  type = {Working {{Paper}}},
  title = {Game, {{Sweat}}, {{Match}}: {{Temperature}} and {{Elite Worker Productivity}}},
  shorttitle = {Game, {{Sweat}}, {{Match}}},
  author = {Burke, Marshall and Tanutama, Vincent and {Heft-Neal}, Sam and Hino, Miyuki and Lobell, David},
  year = {2023},
  month = aug,
  series = {Working {{Paper Series}}},
  number = {31650},
  eprint = {31650},
  publisher = {National Bureau of Economic Research},
  doi = {10.3386/w31650},
  urldate = {2024-05-02},
  abstract = {The effect of hot temperatures on labor productivity is thought to be a key channel through which a warming climate will impact the economy, and these impacts could help explain broader observed relationships between temperature and economic output. Yet for many workers and jobs, especially the high-wage service-economy work that constitutes a large share of total economic output in wealthy nations, productivity is hard to measure and thus climate impacts hard to quantify. We study a high-wage job where individual productivity is readily observable: professional tennis. Using 15 years of data on 177 thousand tennis matches merged to hourly temperature data, we study the effects of temperature on tennis performance in contemporaneous and future matches. Variation in player birthplace and residence allows us to study whether players adapt to heat, and data from betting markets allows us to evaluate whether markets price climate risk. We find that hot temperatures increase contemporaneous errors and retirements, and reduce win probability in the subsequent match. In percentage terms, estimated effects on earnings are smaller than lower-wage settings studied in existing literature. By most measures, top players are less affected by hot temperatures. Most tennis betting markets appear to accurately price climate risk, and temperature impacts do not appear to offer profitable arbitrage opportunities.},
  archiveprefix = {National Bureau of Economic Research},
  keywords = {Labor,Literature,Literature review,NBER,Productivity,Review,Temperature,Tennis,Worker level},
  file = {/Users/vincentbagilet/Zotero/storage/8FBETSAL/Burke et al. - 2023 - Game, Sweat, Match Temperature and Elite Worker P.pdf}
}

@article{burkeAdaptation2016,
  title = {Adaptation to {{Climate Change}}: {{Evidence}} from {{US Agriculture}}},
  shorttitle = {Adaptation to {{Climate Change}}},
  author = {Burke, Marshall and Emerick, Kyle},
  year = {2016},
  month = aug,
  journal = {American Economic Journal: Economic Policy},
  volume = {8},
  number = {3},
  pages = {106--140},
  issn = {1945-7731},
  doi = {10.1257/pol.20130025},
  urldate = {2023-09-29},
  abstract = {Understanding the potential impacts of climate change on economic outcomes requires knowing how agents might adapt to a changing climate. We exploit large variation in recent temperature and precipitation trends to identify adaptation to climate change in US agriculture, and use this information to generate new estimates of the potential impact of future climate change on agricultural outcomes. Longer run adaptations appear to have mitigated less than half--and more likely none--of the large negative short-run impacts of extreme heat on productivity. Limited recent adaptation implies substantial losses under future climate change in the absence of countervailing investments.},
  langid = {english},
  keywords = {Agriculture and Environment Valuation of Environmental Effects Climate,Agriculture: Aggregate Supply and Demand Analysis,Global Warming,Irrigation,Land Reform,Land Use,Natural Disasters and Their Management,Prices Land Ownership and Tenure},
  file = {/Users/vincentbagilet/Zotero/storage/IMLCVERJ/Burke and Emerick - 2016 - Adaptation to Climate Change Evidence from US Agr.pdf}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  urldate = {2022-01-26},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Molecular neuroscience},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: Nature Research Journals\\
Primary\_atype: Research\\
Subject\_term: Molecular neuroscience\\
Subject\_term\_id: molecular-neuroscience},
  file = {/Users/vincentbagilet/Zotero/storage/HTNY2I7W/Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@misc{cachon_severe_2012,
  type = {{{SSRN Scholarly Paper}}},
  title = {Severe {{Weather}} and {{Automobile Assembly Productivity}}},
  author = {Cachon, Gerard P. and Gallino, Santiago and Olivares, Marcelo},
  year = {2012},
  month = dec,
  number = {2099798},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.2099798},
  urldate = {2024-06-21},
  abstract = {It is apparent that severe weather should hamper the productivity of work that occurs outside. But what is the effect of extreme rain, snow, heat and wind on work that occurs indoors, such as the production of automobiles? Using weekly production data from 64 automobile plants in the United States over a ten-year period, we find that adverse weather conditions lead to a significant reduction in production. For example, a week with six or more days of heat exceeding 90F reduces production in that week by 8\% on average. The location impacted the least by weather (Princeton, IN) loses on average 0.5\% of its production due to severe weather and the location with the most adverse weather (Montgomery, AL) suffers a production loss of 3.0\%. Across our sample of plants, severe weather reduces production on average by 1.5\%. While it is possible that plants are able to recover these losses at some later date, we do not find evidence that recovery occurs in the week after the event. Furthermore, even if recovery does occur at some point, at the very least, these shocks are costly as they increase the volatility of production. Our findings are useful both for assessing the potential productivity shock associated with inclement weather as well as guiding managers on where to locate a new production facility - in addition to the traditional factors considered in plant location (e.g., labor costs, local regulations, proximity to customers, access to suppliers), we add the prevalence of bad weather. These results can be expected to become more relevant as climate change may increase the severity and frequency of severe weather.},
  langid = {english},
  keywords = {automobile industry,climate change,econometrics,operations management,productivity,weather},
  file = {/Users/vincentbagilet/Zotero/storage/KPXJREP8/Cachon et al. - 2012 - Severe Weather and Automobile Assembly Productivit.pdf}
}

@article{cai_impact_2018,
  title = {The Impact of Temperature on Manufacturing Worker Productivity: {{Evidence}} from Personnel Data},
  shorttitle = {The Impact of Temperature on Manufacturing Worker Productivity},
  author = {Cai, Xiqian and Lu, Yi and Wang, Jin},
  year = {2018},
  month = dec,
  journal = {Journal of Comparative Economics},
  volume = {46},
  number = {4},
  pages = {889--905},
  issn = {0147-5967},
  doi = {10.1016/j.jce.2018.06.003},
  urldate = {2024-06-11},
  abstract = {This paper presents novel evidence on the impact of temperature on daily indoor worker productivity in a non-climate-controlled manufacturing environment in China. Combining individual worker productivity data from personnel records with weather data, it documents an inverted-U shaped relationship between temperature and labor productivity. Workers do not increase avoidance behavior. The findings suggest that the economic loss from reduced manufacturing labor productivity due to ambient temperature is quantitatively important, providing new insights into the biological effects of climate factor on human labor. Further, back-of-the envelope calculations indicate that the estimated welfare gains from preventing extreme temperatures are substantial.},
  keywords = {China,Labor,Manufacturing,Productivity,Temperature,Worker level},
  file = {/Users/vincentbagilet/Zotero/storage/FY72LU7G/Cai et al. - 2018 - The impact of temperature on manufacturing worker .pdf;/Users/vincentbagilet/Zotero/storage/HHLETRFI/S014759671830204X.html}
}

@article{camerer_evaluating_2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaf0918},
  urldate = {2021-11-11},
  keywords = {Experiments,Replications},
  file = {/Users/vincentbagilet/Zotero/storage/8UPTEKMG/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/vincentbagilet/Zotero/storage/QIQBYX9F/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/vincentbagilet/Zotero/storage/V3VUUYJ6/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/vincentbagilet/Zotero/storage/XB9QNCGT/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf}
}

@article{card_estimating_2001,
  title = {Estimating the {{Return}} to {{Schooling}}: {{Progress}} on {{Some Persistent Econometric Problems}}},
  shorttitle = {Estimating the {{Return}} to {{Schooling}}},
  author = {Card, David},
  year = {2001},
  journal = {Econometrica},
  volume = {69},
  number = {5},
  pages = {1127--1160},
  issn = {1468-0262},
  doi = {10.1111/1468-0262.00237},
  urldate = {2024-06-10},
  abstract = {This paper reviews a set of recent studies that have attempted to measure the causal effect of education on labor market earnings by using institutional features of the supply side of the education system as exogenous determinants of schooling outcomes. A simple theoretical model that highlights the role of comparative advantage in the optimal schooling decision is presented and used to motivate an extended discussion of econometric issues, including the properties of ordinary least squares and instrumental variables estimators. A review of studies that have used compulsory schooling laws, differences in the accessibility of schools, and similar features as instrumental variables for completed education, reveals that the resulting estimates of the return to schooling are typically as big or bigger than the corresponding ordinary least squares estimates. One interpretation of this finding is that marginal returns to education among the low-education subgroups typically affected by supply-side innovations tend to be relatively high, reflecting their high marginal costs of schooling, rather than low ability that limits their return to education.},
  copyright = {The Econometric Society},
  langid = {english},
  keywords = {ability bias,random coefficients,Returns to education},
  file = {/Users/vincentbagilet/Zotero/storage/EYX3J6R8/Card - 2001 - Estimating the Return to Schooling Progress on So.pdf;/Users/vincentbagilet/Zotero/storage/9N4GQTWA/1468-0262.html}
}

@techreport{card_using_1993,
  type = {Working {{Paper}}},
  title = {Using {{Geographic Variation}} in {{College Proximity}} to {{Estimate}} the {{Return}} to {{Schooling}}},
  author = {Card, David},
  year = {1993},
  month = oct,
  series = {Working {{Paper Series}}},
  number = {4483},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w4483},
  urldate = {2022-02-22},
  abstract = {A convincing analysis of the causal link between schooling and earnings requires an exogenous source of variation in education outcomes. This paper explores the use of college proximity as an exogenous determinant of schooling. Analysis of the NLS Young Men Cohort reveals that men who grew up in local labor markets with a nearby college have significantly higher education and earnings than other men. The education and earnings gains are concentrated among men with poorly-educated parents -- men who would otherwise stop schooling at relatively low levels. When college proximity is taken as an exogenous determinant of schooling the implied instrumental variables estimates of the return to schooling are 25-60\% higher than conventional ordinary least squares estimates. Since the effect of a nearby college on schooling attainment varies by family background it is possible to test whether college proximity is a legitimately exogenous determinant of schooling. The results affirm that marginal returns to education among children of less-educated parents are as high and perhaps much higher than the rates of return estimated by conventional methods.},
  file = {/Users/vincentbagilet/Zotero/storage/UPNNYWVP/Card - 1993 - Using Geographic Variation in College Proximity to.pdf}
}

@article{cattaneo_power_2019,
  title = {Power Calculations for Regression-Discontinuity Designs},
  author = {Cattaneo, Matias D. and Titiunik, Roc{\'i}o and {Vazquez-Bare}, Gonzalo},
  year = {2019},
  month = mar,
  journal = {The Stata Journal: Promoting communications on statistics and Stata},
  volume = {19},
  number = {1},
  pages = {210--245},
  issn = {1536-867X, 1536-8734},
  doi = {10.1177/1536867X19830919},
  urldate = {2022-02-21},
  abstract = {In this article, we introduce two commands, rdpow and rdsampsi, that conduct power calculations and survey sample selection when using local polynomial estimation and inference methods in regression-discontinuity designs. rdpow conducts power calculations using modern robust bias-corrected local polynomial inference procedures and allows for new hypothetical sample sizes and bandwidth selections, among other features. rdsampsi uses power calculations to compute the minimum sample size required to achieve a desired level of power, given estimated or user-supplied bandwidths, biases, and variances. Together, these commands are useful when devising new experiments or surveys in regression-discontinuity designs, which will later be analyzed using modern local polynomial techniques for estimation, inference, and falsification. Because our commands use the communitycontributed (and R) package rdrobust for the underlying bandwidths, biases, and variances estimation, all the options currently available in rdrobust can also be used for power calculations and sample-size selection, including preintervention covariate adjustment, clustered sampling, and many bandwidth selectors. Finally, we also provide companion R functions with the same syntax and capabilities.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/NPAY4N4H/Cattaneo et al. - 2019 - Power calculations for regression-discontinuity de.pdf}
}

@article{chang_is_2022,
  title = {Is {{Economics Research Replicable}}? {{Sixty Published Papers From Thirteen Journals Say}} ``{{Often Not}}''},
  shorttitle = {Is {{Economics Research Replicable}}?},
  author = {Chang, Andrew C. and Li, Phillip},
  year = {2022},
  month = jul,
  journal = {Critical Finance Review},
  volume = {11},
  publisher = {Now Publishers, Inc.},
  issn = {2164-5744, 2164-5760},
  doi = {10.1561/104.00000053},
  urldate = {2022-01-26},
  abstract = {Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say ``Often Not''},
  langid = {english}
}

@article{chattopadhyay_implied_2023,
  title = {On the Implied Weights of Linear Regression for Causal Inference},
  author = {Chattopadhyay, Ambarish and Zubizarreta, Jos{\'e} R},
  year = {2023},
  month = sep,
  journal = {Biometrika},
  volume = {110},
  number = {3},
  pages = {615--629},
  issn = {1464-3510},
  doi = {10.1093/biomet/asac058},
  urldate = {2023-10-17},
  abstract = {A basic principle in the design of observational studies is to approximate the randomized experiment that would have been conducted under ideal circumstances. At present, linear regression models are commonly used to analyse observational data and estimate causal effects. How do linear regression adjustments in observational studies emulate key features of randomized experiments, such as covariate balance, self-weighted sampling and study representativeness? In this paper, we provide answers to this and related questions by analysing the implied individual-level data weights of various linear regression methods. We derive new closed-form expressions of these implied weights, and examine their properties in both finite and large samples. Among others, in finite samples we characterize the implied target population of linear regression, and in large samples demonstrate the multiply robust properties of regression estimators from the perspective of their implied weights. We show that the implied weights of general regression methods can be equivalently obtained by solving a convex optimization problem. This equivalence allows us to bridge ideas from the regression modelling and causal inference literatures. As a result, we propose novel regression diagnostics for causal inference that are part of the design stage of an observational study. We implement the weights and diagnostics in the new lmw package for R.},
  keywords = {To read,Variation,Visualization,Weights},
  file = {/Users/vincentbagilet/Zotero/storage/7J9MIM72/Chattopadhyay and Zubizarreta - 2023 - On the implied weights of linear regression for ca.pdf}
}

@misc{chattopadhyay_lmw_2024,
  title = {Lmw: {{Linear Model Weights}} for {{Causal Inference}}},
  shorttitle = {Lmw},
  author = {Chattopadhyay, Ambarish and Greifer, Noah and Zubizarreta, Jose R.},
  year = {2024},
  month = apr,
  number = {arXiv:2303.08790},
  eprint = {2303.08790},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08790},
  urldate = {2024-06-10},
  abstract = {The linear regression model is widely used in the biomedical and social sciences as well as in policy and business research to adjust for covariates and estimate the average effects of treatments. Behind every causal inference endeavor there is a hypothetical randomized experiment. However, in routine regression analyses in observational studies, it is unclear how well the adjustments made by regression approximate key features of randomized experiments, such as covariate balance, study representativeness, sample boundedness, and unweighted sampling. In this paper, we provide software to empirically address this question. We introduce the lmw package for R to compute the implied linear model weights and perform diagnostics for their evaluation. The weights are obtained as part of the design stage of the study; that is, without using outcome information. The implementation is general and applicable, for instance, in settings with instrumental variables and multi-valued treatments; in essence, in any situation where the linear model is the vehicle for adjustment and estimation of average treatment effects with discrete-valued interventions.},
  archiveprefix = {arXiv},
  keywords = {Leo,To read,Weights},
  file = {/Users/vincentbagilet/Zotero/storage/WA4QPYKI/Chattopadhyay et al. - 2024 - lmw Linear Model Weights for Causal Inference.pdf;/Users/vincentbagilet/Zotero/storage/HB95724E/2303.html}
}

@article{chen_temperature_2019,
  title = {Temperature and Industrial Output: {{Firm-level}} Evidence from {{China}}},
  shorttitle = {Temperature and Industrial Output},
  author = {Chen, Xiaoguang and Yang, Lu},
  year = {2019},
  month = may,
  journal = {Journal of Environmental Economics and Management},
  volume = {95},
  pages = {257--274},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2017.07.009},
  urldate = {2024-06-14},
  abstract = {We pair a firm-level panel of annual industrial output with a fine-scale daily weather data set, to estimate the responses of industrial output to temperature changes in China. We have four primary findings. First, industrial output is nonlinear in temperature changes. With seasonal average temperatures as temperature variables, output responds positively to higher spring temperatures and negatively to elevated summer temperatures. With temperature bins as temperature variables, output increases linearly with temperature up to 21--24{$^\circ$}C, and then declines sharply at higher temperatures. Second, lagged temperature changes exert large and significant impacts on current year's output. Third, higher summer temperatures have larger detrimental effects on output in low-temperature regions than in high-temperature regions, which suggests that adaptation to warming may have been actively undertaken in high-temperature regions in China. Lastly, industrial output in China is projected to decrease by 3--36\% by 2080 under the slowest warming scenario (B1) and by 12--46\% under the most rapid warming scenario (A2) under the global climate models UKMO-HadCM3 and PCM.},
  keywords = {China,Firm level,Labor,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/G3M2694U/S009506961730520X.html}
}

@article{chopra_null_2024,
  title = {The {{Null Result Penalty}}},
  author = {Chopra, Felix and Haaland, Ingar and Roth, Christopher and Stegmann, Andreas},
  year = {2024},
  month = jan,
  journal = {The Economic Journal},
  volume = {134},
  number = {657},
  pages = {193--219},
  issn = {0013-0133},
  doi = {10.1093/ej/uead060},
  urldate = {2024-05-11},
  abstract = {We examine how the evaluation of research studies in economics depends on whether a study yielded a null result. Studies with null results are perceived to be less publishable, of lower quality, less important and less precisely estimated than studies with large and statistically significant results, even when holding constant all other study features, including the sample size and the precision of the estimates. The null result penalty is of similar magnitude among PhD students and journal editors. The penalty is larger when experts predict a large effect and when statistical uncertainty is communicated with p-values rather than standard errors. Our findings highlight the value of a pre-result review.},
  keywords = {Lab experiment},
  file = {/Users/vincentbagilet/Zotero/storage/S78CBFNN/Chopra et al. - 2024 - The Null Result Penalty.pdf;/Users/vincentbagilet/Zotero/storage/3V3A98I8/7238466.html}
}

@article{christensen_transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = {2018},
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  urldate = {2022-01-26},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  langid = {english},
  keywords = {Economics,Literature,Publication bias,Replication},
  file = {/Users/vincentbagilet/Zotero/storage/L82UKTMB/Christensen and Miguel - 2018 - Transparency, Reproducibility, and the Credibility.pdf}
}

@article{cinelli_crash_2022,
  title = {A {{Crash Course}} in {{Good}} and {{Bad Controls}}},
  author = {Cinelli, Carlos and Forney, Andrew and Pearl, Judea},
  year = {2022},
  month = may,
  journal = {Sociological Methods \& Research},
  pages = {00491241221099552},
  publisher = {SAGE Publications Inc},
  issn = {0049-1241},
  doi = {10.1177/00491241221099552},
  urldate = {2023-01-12},
  abstract = {Many students of statistics and econometrics express frustration with the way a problem known as ?bad control? is treated in the traditional literature. The issue arises when the addition of a variable to a regression equation produces an unintended discrepancy between the regression coefficient and the effect that the coefficient is intended to represent. Avoiding such discrepancies presents a challenge to all analysts in the data intensive sciences. This note describes graphical tools for understanding, visualizing, and resolving the problem through a series of illustrative examples. By making this ?crash course? accessible to instructors and practitioners, we hope to avail these tools to a broader community of scientists concerned with the causal interpretation of regression models.},
  langid = {english},
  keywords = {Bad controls},
  file = {/Users/vincentbagilet/Zotero/storage/LAVC2DCA/Cinelli et al. - 2022 - A Crash Course in Good and Bad Controls.pdf}
}

@article{cinelli_making_2020,
  title = {Making Sense of Sensitivity: Extending Omitted Variable Bias},
  shorttitle = {Making Sense of Sensitivity},
  author = {Cinelli, Carlos and Hazlett, Chad},
  year = {2020},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {82},
  number = {1},
  pages = {39--67},
  issn = {1467-9868},
  doi = {10.1111/rssb.12348},
  urldate = {2021-06-29},
  abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as `extreme scenarios'. Finally, we describe problems with a common `benchmarking' practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
  copyright = {{\copyright} 2019 Royal Statistical Society},
  langid = {english},
  keywords = {Omitted variable bias,Statistics},
  file = {/Users/vincentbagilet/Zotero/storage/6MJG6DDF/Cinelli and Hazlett - 2020 - Making sense of sensitivity extending omitted var.pdf}
}

@article{clarke_phantom_2005,
  title = {The {{Phantom Menace}}: {{Omitted Variable Bias}} in {{Econometric Research}}},
  shorttitle = {The {{Phantom Menace}}},
  author = {Clarke, Kevin A.},
  year = {2005},
  month = sep,
  journal = {Conflict Management and Peace Science},
  volume = {22},
  number = {4},
  pages = {341--352},
  publisher = {SAGE Publications Ltd},
  issn = {0738-8942},
  doi = {10.1080/07388940500339183},
  urldate = {2022-04-28},
  abstract = {Quantitative political science is awash in control variables. The justification for these bloated specifications is usually the fear of omitted variable bias. A key underlying assumption is that the danger posed by omitted variable bias can be ameliorated by the inclusion of relevant control variables. Unfortunately, as this article demonstrates, there is nothing in the mathematics of regression analysis that supports this conclusion. The inclusion of additional control variables may increase or decrease the bias, and we cannot know for sure which is the case in any particular situation. A brief discussion of alternative strategies for achieving experimental control follows the main result.},
  langid = {english},
  keywords = {Omitted variable bias},
  file = {/Users/vincentbagilet/Zotero/storage/CIYQ2FG3/Clarke - 2005 - The Phantom Menace Omitted Variable Bias in Econo.pdf}
}

@article{cochran_planning_2022,
  title = {The {{Planning}} of {{Observational Studies}} of {{Human Populations}}},
  author = {Cochran, W G},
  year = {2022},
  pages = {34},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/IQ56NYZX/Cochran - 2022 - The Planning of Observational Studies of Human Pop.pdf}
}

@article{cook_brain_2020,
  title = {Brain Freeze: Outdoor Cold and Indoor Cognitive Performance},
  shorttitle = {Brain Freeze},
  author = {Cook, Nikolai and Heyes, Anthony},
  year = {2020},
  month = may,
  journal = {Journal of Environmental Economics and Management},
  volume = {101},
  pages = {102318},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2020.102318},
  urldate = {2024-06-11},
  abstract = {We present first evidence that outdoor cold temperatures negatively impact indoor cognitive performance. We use a within-subject design and a large-scale dataset of adults in an incentivized setting. The performance decrement is large despite the subjects working in a fully climate-controlled environment. Using secondary data, we find evidence of partial adaptation at the organizational, individual and biological levels. The results are interpreted in the context of climate models that observe and predict an increase in the frequency of very cold days in some locations (e.g. Chicago) and a decrease in others (e.g. Beijing).},
  keywords = {Adaptation,Climate change,Climate resilience,Cognitive productivity,Cold temperature},
  file = {/Users/vincentbagilet/Zotero/storage/Y7ZJV2ES/Cook and Heyes - 2020 - Brain freeze outdoor cold and indoor cognitive pe.pdf;/Users/vincentbagilet/Zotero/storage/KMIR8SXU/S0095069620300413.html}
}

@article{cook_waiting_2008,
  title = {``{{Waiting}} for {{Life}} to {{Arrive}}'': {{A}} History of the Regression-Discontinuity Design in {{Psychology}}, {{Statistics}} and {{Economics}}},
  shorttitle = {``{{Waiting}} for {{Life}} to {{Arrive}}''},
  author = {Cook, Thomas D.},
  year = {2008},
  month = feb,
  journal = {Journal of Econometrics},
  series = {The Regression Discontinuity Design: {{Theory}} and Applications},
  volume = {142},
  number = {2},
  pages = {636--654},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2007.05.002},
  urldate = {2021-10-19},
  abstract = {This paper reviews the history of the regression discontinuity design in three academic disciplines. It describes the design's birth and subsequent demise in Psychology even though most problems with it had been solved there. It further describes the scant interest shown in the design by scholars formally trained in Statistics, and the design's poor reception in Economics from 1972 until about 1995, when its profile and acceptance changed. Reasons are given for this checkered history that is characterized as waiting for life to arrive.},
  langid = {english},
  keywords = {History,RDD}
}

@article{cooperman_randomization_2017,
  title = {Randomization {{Inference}} with {{Rainfall Data}}: {{Using Historical Weather Patterns}} for {{Variance Estimation}}},
  shorttitle = {Randomization {{Inference}} with {{Rainfall Data}}},
  author = {Cooperman, Alicia Dailey},
  year = {2017},
  month = jul,
  journal = {Political Analysis},
  volume = {25},
  number = {3},
  pages = {277--288},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.17},
  urldate = {2021-12-10},
  abstract = {Many recent papers in political science and economics use rainfall as a strategy to facilitate causal inference. Rainfall shocks are as-if randomly assigned, but the assignment of rainfall by county is highly correlated across space. Since clustered assignment does not occur within well-defined boundaries, it is challenging to estimate the variance of the effect of rainfall on political outcomes. I propose using randomization inference with historical weather patterns from 73 years as potential randomizations. I replicate the influential work on rainfall and voter turnout in presidential elections in the United States by Gomez, Hansford, and Krause (2007) and compare the estimated average treatment effect (ATE) to a sampling distribution of estimates under the sharp null hypothesis of no effect. The alternate randomizations are random draws from national rainfall patterns on election and would-be election days, which preserve the clustering in treatment assignment and eliminate the need to simulate weather patterns or make assumptions about unit boundaries for clustering. I find that the effect of rainfall on turnout is subject to greater sampling variability than previously estimated using conventional standard errors.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/AZQQ3762/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf;/Users/vincentbagilet/Zotero/storage/GUD7T7L7/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf;/Users/vincentbagilet/Zotero/storage/HRXBHHJP/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf;/Users/vincentbagilet/Zotero/storage/VZKAHT2K/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf}
}

@misc{courthoud_understanding_2022,
  title = {Understanding {{Omitted Variable Bias}}},
  author = {Courthoud, Matteo},
  year = {2022},
  month = aug,
  journal = {Medium},
  urldate = {2022-10-28},
  abstract = {A step-by-step guide for the most pervasive type of bias},
  langid = {english},
  keywords = {Blog post,Identify variation,Omitted variable bias}
}

@book{cunningham_causal_2021,
  title = {Causal {{Inference}}: {{The Mixtape}}},
  shorttitle = {Causal {{Inference}}},
  author = {Cunningham, Scott},
  year = {2021},
  month = jan,
  publisher = {Yale University Press},
  doi = {10.2307/j.ctv1c29t27},
  urldate = {2021-06-03},
  isbn = {978-0-300-25588-1 978-0-300-25168-5},
  langid = {english},
  keywords = {Causal inference,Handbook,Simulations,Statistics},
  file = {/Users/vincentbagilet/Zotero/storage/G8I9US2N/Cunningham - 2021 - Causal Inference The Mixtape.pdf;/Users/vincentbagilet/Zotero/storage/HMURX9VR/Cunningham - 2021 - Causal Inference The Mixtape.pdf;/Users/vincentbagilet/Zotero/storage/URKCJFMN/Cunningham - 2021 - Causal Inference The Mixtape.pdf;/Users/vincentbagilet/Zotero/storage/YYGUW32E/Cunningham - 2021 - Causal Inference The Mixtape.pdf}
}

@article{currie_environmental_2015,
  title = {Environmental {{Health Risks}} and {{Housing Values}}: {{Evidence}} from 1,600 {{Toxic Plant Openings}} and {{Closings}}},
  shorttitle = {Environmental {{Health Risks}} and {{Housing Values}}},
  author = {Currie, Janet and Davis, Lucas and Greenstone, Michael and Walker, Reed},
  year = {2015},
  month = feb,
  journal = {American Economic Review},
  volume = {105},
  number = {2},
  pages = {678--709},
  issn = {0002-8282},
  doi = {10.1257/aer.20121656},
  urldate = {2022-02-14},
  abstract = {Regulatory oversight of toxic emissions from industrial plants and understanding about these emissions' impacts are in their infancy. Applying a research design based on the openings and closings of 1,600 industrial plants to rich data on housing markets and infant health, we find that: toxic air emissions affect air quality only within 1 mile of the plant; plant openings lead to 11 percent declines in housing values within 0.5 mile or a loss of about \$4.25 million for these households; and a plant's operation is associated with a roughly 3 percent increase in the probability of low birthweight within 1 mile. (JEL I12, L60, Q52, Q53, Q58, R23, R31)},
  langid = {english},
  keywords = {Air pollution,Health,Housing,US},
  file = {/Users/vincentbagilet/Zotero/storage/A4RAKRRD/Currie et al. - 2015 - Environmental Health Risks and Housing Values Evi.pdf}
}

@techreport{de_chaisemartin_two-way_2022,
  type = {Working {{Paper}}},
  title = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}: {{A Survey}}},
  shorttitle = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = jan,
  series = {Working {{Paper Series}}},
  number = {29691},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w29691},
  urldate = {2022-02-18},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate policies' effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been show that those regressions may produce misleading estimates, if the policy's effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue, and that proposes alternative estimators robust to heterogeneous effects.},
  file = {/Users/vincentbagilet/Zotero/storage/Z7NS79JN/de Chaisemartin and D'Haultfoeuille - 2022 - Two-Way Fixed Effects and Differences-in-Differenc.pdf}
}

@misc{de_luca_ambiguous_2015,
  type = {{{SSRN Scholarly Paper}}},
  title = {On the {{Ambiguous Consequences}} of {{Omitting Variables}}},
  author = {De Luca, Giuseppe and Magnus, J. R. and Peracchi, Franco},
  year = {2015},
  month = may,
  number = {2609261},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.2609261},
  urldate = {2022-09-08},
  abstract = {This paper studies what happens when we move from a short regression to a long regression (or vice versa), when the long regression is shorter than the data-generation process. In the special case where the long regression equals the data-generation process, the least-squares estimators have smaller bias (in fact zero bias) but larger variances in the long regression than in the short regression. But if the long regression is also misspecified, the bias may not be smaller. We provide bias and mean squared error comparisons and study the dependence of the differences on the misspecification parameter.},
  langid = {english},
  keywords = {Bias,Least-squares estimators,Mean squared error,Misspecification,Omitted variables},
  file = {/Users/vincentbagilet/Zotero/storage/2CHXKULM/De Luca et al. - 2015 - On the Ambiguous Consequences of Omitting Variable.pdf}
}

@article{deaton_understanding_2018,
  title = {Understanding and Misunderstanding Randomized Controlled Trials},
  author = {Deaton, Angus and Cartwright, Nancy},
  year = {2018},
  month = aug,
  journal = {Social Science \& Medicine},
  series = {Randomized {{Controlled Trials}} and {{Evidence-based Policy}}: {{A Multidisciplinary Dialogue}}},
  volume = {210},
  pages = {2--21},
  issn = {0277-9536},
  doi = {10.1016/j.socscimed.2017.12.005},
  urldate = {2022-01-25},
  abstract = {Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment effect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more difficult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justification is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding `external validity' is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientific progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientific knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not `what works', but `why things work'.},
  langid = {english},
  keywords = {Precision,RCT,Variance/bias trade off},
  file = {/Users/vincentbagilet/Zotero/storage/JUVKRPBY/Deaton and Cartwright - 2018 - Understanding and misunderstanding randomized cont.pdf}
}

@article{dechaisemartin_twoway_2020,
  title = {Two-{{Way Fixed Effects Estimators}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultf{\oe}uille, Xavier},
  year = {2020},
  month = sep,
  journal = {American Economic Review},
  volume = {110},
  number = {9},
  pages = {2964--2996},
  issn = {0002-8282},
  doi = {10.1257/aer.20181169},
  urldate = {2025-01-28},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator.},
  langid = {english},
  keywords = {Media,Quantile Regressions Single Equation Models,Single Equation Models,Single Variables: Cross-Sectional Models,Single Variables: Panel Data Models,Spatial Models,Spatio-temporal Models Political Processes: Rent-seeking Lobbying Elections Legislatures and Voting Behavior Wage Level and Structure,Treatment Effect Models,Wage Differentials Trade Unions: Objectives Structure and Effects Entertainment},
  file = {/Users/vincentbagilet/Zotero/storage/W76C5D67/de Chaisemartin and D'Haultfœuille - 2020 - Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.pdf}
}

@article{dechezlepretre_impacts_2017,
  title = {The {{Impacts}} of {{Environmental Regulations}} on                         {{Competitiveness}}},
  author = {Dechezlepr{\^e}tre, Antoine and Sato, Misato},
  year = {2017},
  month = jul,
  journal = {Review of Environmental Economics and Policy},
  volume = {11},
  number = {2},
  pages = {183--206},
  publisher = {The University of Chicago Press},
  issn = {1750-6816},
  doi = {10.1093/reep/rex013},
  urldate = {2023-10-21},
  abstract = {This article reviews the empirical literature on the impacts of environmental regulations on firms' competitiveness as measured by trade, industry location, employment, productivity, and innovation. The evidence shows that environmental regulations can lead to statistically significant adverse effects on trade, employment, plant location, and productivity in the short run, in particular in a well-identified subset of pollution- and energy-intensive sectors, but that these impacts are small relative to general trends in production. At the same time, there is evidence that environmental regulations induce innovation in clean technologies, but the resulting benefits do not appear to be large enough to outweigh the costs of regulations for the regulated entities. As measures to address competitiveness impacts are increasingly incorporated into the design of environmental regulations, future research will be needed to assess the validity and effectiveness of such measures and to ensure they are compatible with the environmental objectives of the policies.},
  keywords = {Competitiveness,Environmental Regulation,Literature review},
  file = {/Users/vincentbagilet/Zotero/storage/C2XTIPJ3/Dechezleprêtre and Sato - 2017 - The Impacts of Environmental Regulations on       .pdf}
}

@article{dehejia_causal_1999,
  title = {Causal {{Effects}} in {{Nonexperimental Studies}}: {{Reevaluating}} the {{Evaluation}} of {{Training Programs}}},
  shorttitle = {Causal {{Effects}} in {{Nonexperimental Studies}}},
  author = {Dehejia, Rajeev H. and Wahba, Sadek},
  year = {1999},
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {448},
  eprint = {2669919},
  eprinttype = {jstor},
  pages = {1053--1062},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2669919},
  urldate = {2022-02-21},
  abstract = {This article uses propensity score methods to estimate the treatment impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention earnings. We use data from Lalonde's evaluation of nonexperimental methods that combine the treated units from a randomized evaluation of the NSW with nonexperimental comparison units drawn from survey datasets. We apply propensity score methods to this composite dataset and demonstrate that, relative to the estimators that Lalonde evaluates, propensity score estimates of the treatment impact are much closer to the experimental benchmark estimate. Propensity score methods assume that the variables associated with assignment to treatment are observed (referred to as ignorable treatment assignment, or selection on observables). Even under this assumption, it is difficult to control for differences between the treatment and comparison groups when they are dissimilar and when there are many preintervention variables. The estimated propensity score (the probability of assignment to treatment, conditional on preintervention variables) summarizes the preintervention variables. This offers a diagnostic on the comparability of the treatment and comparison groups, because one has only to compare the estimated propensity score across the two groups. We discuss several methods (such as stratification and matching) that use the propensity score to estimate the treatment impact. When the range of estimated propensity scores of the treatment and comparison groups overlap, these methods can estimate the treatment impact for the treatment group. A sensitivity analysis shows that our estimates are not sensitive to the specification of the estimated propensity score, but are sensitive to the assumption of selection on observables. We conclude that when the treatment and comparison groups overlap, and when the variables determining assignment to treatment are observed, these methods provide a means to estimate the treatment impact. Even though propensity score methods are not always applicable, they offer a diagnostic on the quality of nonexperimental comparison groups in terms of observable preintervention variables.},
  file = {/Users/vincentbagilet/Zotero/storage/XQ2JHVXI/Dehejia and Wahba - 1999 - Causal Effects in Nonexperimental Studies Reevalu.pdf}
}

@article{dellavigna_rcts_2022,
  title = {{{RCTs}} to {{Scale}}: {{Comprehensive Evidence From Two Nudge Units}}},
  shorttitle = {{{RCTs}} to {{Scale}}},
  author = {DellaVigna, Stefano and Linos, Elizabeth},
  year = {2022},
  journal = {Econometrica},
  volume = {90},
  number = {1},
  pages = {81--116},
  issn = {1468-0262},
  doi = {10.3982/ECTA18709},
  urldate = {2023-10-11},
  abstract = {Nudge interventions have quickly expanded from academic studies to larger implementation in so-called Nudge Units in governments. This provides an opportunity to compare interventions in research studies, versus at scale. We assemble a unique data set of 126 RCTs covering 23 million individuals, including all trials run by two of the largest Nudge Units in the United States. We compare these trials to a sample of nudge trials in academic journals from two recent meta-analyses. In the Academic Journals papers, the average impact of a nudge is very large---an 8.7 percentage point take-up effect, which is a 33.4\% increase over the average control. In the Nudge Units sample, the average impact is still sizable and highly statistically significant, but smaller at 1.4 percentage points, an 8.0\% increase. We document three dimensions which can account for the difference between these two estimates: (i) statistical power of the trials; (ii) characteristics of the interventions, such as topic area and behavioral channel; and (iii) selective publication. A meta-analysis model incorporating these dimensions indicates that selective publication in the Academic Journals sample, exacerbated by low statistical power, explains about 70 percent of the difference in effect sizes between the two samples. Different nudge characteristics account for most of the residual difference.},
  copyright = {{\copyright} 2022 The Econometric Society},
  langid = {english},
  keywords = {Field experiment,Nudges,Power,Publication bias,RCT},
  file = {/Users/vincentbagilet/Zotero/storage/GHTSS6TD/DellaVigna and Linos - 2022 - RCTs to Scale Comprehensive Evidence From Two Nud.pdf}
}

@misc{deryugina_does_2014,
  type = {Working {{Paper}}},
  title = {Does the {{Environment Still Matter}}? {{Daily Temperature}} and {{Income}} in the {{United States}}},
  shorttitle = {Does the {{Environment Still Matter}}?},
  author = {Deryugina, Tatyana and Hsiang, Solomon M.},
  year = {2014},
  month = dec,
  series = {Working {{Paper Series}}},
  number = {20750},
  eprint = {20750},
  publisher = {National Bureau of Economic Research},
  doi = {10.3386/w20750},
  urldate = {2024-06-11},
  abstract = {It is widely hypothesized that incomes in wealthy countries are insulated from environmental conditions because individuals have the resources needed to adapt to their environment. We test this idea in the wealthiest economy in human history. Using within-county variation in weather, we estimate the effect of daily temperature on annual income in United States counties over a 40-year period. We find that this single environmental parameter continues to play a large role in overall economic performance: productivity of individual days declines roughly 1.7\% for each 1{$^\circ$}C (1.8{$^\circ$}F) increase in daily average temperature above 15{$^\circ$}C (59{$^\circ$}F). A weekday above 30{$^\circ$}C (86{$^\circ$}F) costs an average county \$20 per person. Hot weekends have little effect. These estimates are net of many forms of adaptation, such as factor reallocation, defensive investments, transfers, and price changes. Because the effect of temperature has not changed since 1969, we infer that recent uptake or innovation in adaptation measures have been limited. The non-linearity of the effect on different components of income suggest that temperature matters because it reduces the productivity of the economy's basic elements, such as workers and crops. If counties could choose daily temperatures to maximize output, rather than accepting their geographically- determined endowment, we estimate that annual income growth would rise by 1.7 percentage points. Applying our estimates to a distribution of "business as usual" climate change projections indicates that warmer daily temperatures will lower annual growth by 0.06-0.16 percentage points in the United States unless populations engage in new forms of adaptation.},
  archiveprefix = {National Bureau of Economic Research},
  file = {/Users/vincentbagilet/Zotero/storage/AND8KGUT/Deryugina and Hsiang - 2014 - Does the Environment Still Matter Daily Temperatu.pdf}
}

@article{deryugina_mortality_2019,
  title = {The {{Mortality}} and {{Medical Costs}} of {{Air Pollution}}: {{Evidence}} from {{Changes}} in {{Wind Direction}}},
  shorttitle = {The {{Mortality}} and {{Medical Costs}} of {{Air Pollution}}},
  author = {Deryugina, Tatyana and Heutel, Garth and Miller, Nolan H. and Molitor, David and Reif, Julian},
  year = {2019},
  month = dec,
  journal = {American Economic Review},
  volume = {109},
  number = {12},
  pages = {4178--4219},
  issn = {0002-8282},
  doi = {10.1257/aer.20180279},
  urldate = {2020-06-05},
  abstract = {We estimate the causal effects of acute fine particulate matter exposure on mortality, health care use, and medical costs among the US elderly using Medicare data. We instrument for air pollution using changes in local wind direction and develop a new approach that uses machine learning to estimate the life-years lost due to pollution exposure. Finally, we characterize treatment effect heterogeneity using both life expectancy and generic machine learning inference. Both approaches find that mortality effects are concentrated in about 25 percent of the elderly population.},
  langid = {english},
  keywords = {Air pollution,Medicare,Mortality,Mortality displacement,US,Wind},
  file = {/Users/vincentbagilet/Zotero/storage/L99DEJJ4/Deryugina et al. - 2019 - The Mortality and Medical Costs of Air Pollution .pdf}
}

@article{deschenes_environmental_2014,
  title = {Environmental Regulations and Labor Markets},
  author = {Deschenes, Olivier},
  year = {2014},
  month = jul,
  journal = {IZA World of Labor},
  doi = {10.15185/izawol.22},
  urldate = {2023-10-04},
  abstract = {Balancing the benefits of environmental regulations for everyone and the costs to workers and firms},
  langid = {american},
  keywords = {CBA,Employment,Environment,Environmental Regulation},
  file = {/Users/vincentbagilet/Zotero/storage/XICYGDIL/Deschenes - 2014 - Environmental regulations and labor markets.pdf}
}

@article{deschenesEconomic2007,
  title = {The {{Economic Impacts}} of {{Climate Change}}: {{Evidence}} from {{Agricultural Output}} and {{Random Fluctuations}} in {{Weather}}},
  shorttitle = {The {{Economic Impacts}} of {{Climate Change}}},
  author = {Desch{\^e}nes, Olivier and Greenstone, Michael},
  year = {2007},
  month = mar,
  journal = {American Economic Review},
  volume = {97},
  number = {1},
  pages = {354--385},
  issn = {0002-8282},
  doi = {10.1257/aer.97.1.354},
  urldate = {2023-05-04},
  abstract = {This paper measures the economic impact of climate change on US agricultural land by estimating the effect of random year-to-year variation in temperature and precipitation on agricultural profits. The preferred estimates indicate that climate change will increase annual profits by \$1.3 billion in 2002 dollars (2002\$) or 4 percent. This estimate is robust to numerous specification checks and relatively precise, so large negative or positive effects are unlikely. We also find the hedonic approach---which is the standard in the previous literature---to be unreliable because it produces estimates that are extremely sensitive to seemingly minor choices about control variables, sample, and weighting. (JEL L25, Q12, Q51, Q54)},
  langid = {english},
  keywords = {Agriculture and Environment Valuation of Environmental Effects Climate,Firm Performance: Size Diversification and Scope Micro Analysis of Farm Firms Farm Households and Farm Input Markets Land Ownership and Tenure,Global Warming,Irrigation,Land Reform,Land Use,Natural Disasters},
  file = {/Users/vincentbagilet/Zotero/storage/4JGKGIWH/Deschênes and Greenstone - 2007 - The Economic Impacts of Climate Change Evidence f.pdf;/Users/vincentbagilet/Zotero/storage/UZHM3QGQ/Deschenes - 2002 - The Economic Impacts of Climate Change Evidence f.pdf}
}

@article{dominici_best_2017,
  title = {Best {{Practices}} for {{Gauging Evidence}} of {{Causality}} in {{Air Pollution Epidemiology}}},
  author = {Dominici, Francesca and Zigler, Corwin},
  year = {2017},
  month = dec,
  journal = {American Journal of Epidemiology},
  volume = {186},
  number = {12},
  pages = {1303--1309},
  issn = {0002-9262, 1476-6256},
  doi = {10.1093/aje/kwx307},
  urldate = {2021-06-10},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/P36TWDLR/Dominici et Zigler - 2017 - Best Practices for Gauging Evidence of Causality i.pdf}
}

@article{doucouliagos_are_2013,
  title = {Are {{All Economic Facts Greatly Exaggerated}}? {{Theory Competition}} and {{Selectivity}}},
  shorttitle = {Are {{All Economic Facts Greatly Exaggerated}}?},
  author = {Doucouliagos, Chris and Stanley, T.d.},
  year = {2013},
  journal = {Journal of Economic Surveys},
  volume = {27},
  number = {2},
  pages = {316--339},
  issn = {1467-6419},
  doi = {10.1111/j.1467-6419.2011.00706.x},
  urldate = {2024-05-16},
  abstract = {Abstract..There is growing concern and mounting evidence of selectivity in empirical economics. Most empirical economic literatures have a truncated distribution of results. The aim of this paper is to explore the link between publication selectivity and theory contests. This link is confirmed through the analysis of 87 distinct empirical economics literatures, involving more than three and a half thousand separate empirical studies, using objective measures of both selectivity and contests. Our meta--meta-analysis shows that publication selection is widespread, but not universal. It distorts scientific inference with potentially adverse effects on policy making, but competition and debate between rival theories reduces this selectivity and thereby improves economic inference.},
  copyright = {{\copyright} 2011 Blackwell Publishing Ltd},
  langid = {english},
  keywords = {Economics,Exaggeration,Meta-analysis,Publication bias,Theory},
  file = {/Users/vincentbagilet/Zotero/storage/ANYJEL3C/Doucouliagos and Stanley - 2013 - Are All Economic Facts Greatly Exaggerated Theory.pdf;/Users/vincentbagilet/Zotero/storage/QF78HLUH/j.1467-6419.2011.00706.html}
}

@misc{drysdale_winners_nodate,
  title = {A Winner's Curse Adjustment for a Single Test Statistic},
  author = {Drysdale, Erik},
  urldate = {2020-10-27},
  abstract = {Background},
  howpublished = {http://www.erikdrysdale.com/winners\_curse/},
  keywords = {Maths,Power,Type M/S error},
  file = {/Users/vincentbagilet/Zotero/storage/Y8ALBH4W/Drysdale - A winner's curse adjustment for a single test stat.pdf}
}

@incollection{duflo_using_2007,
  title = {Using {{Randomization}} in {{Development Economics Research}}: {{A Toolkit}}},
  shorttitle = {Chapter 61 {{Using Randomization}} in {{Development Economics Research}}},
  booktitle = {Handbook of {{Development Economics}}},
  author = {Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  editor = {Schultz, T. Paul and Strauss, John A.},
  year = {2007},
  month = jan,
  volume = {4},
  pages = {3895--3962},
  publisher = {Elsevier},
  doi = {10.1016/S1573-4471(07)04061-2},
  urldate = {2022-02-01},
  abstract = {This paper is a practical guide (a toolkit) for researchers, students and practitioners wishing to introduce randomization as part of a research design in the field. It first covers the rationale for the use of randomization, as a solution to selection bias and a partial solution to publication biases. Second, it discusses various ways in which randomization can be practically introduced in a field settings. Third, it discusses designs issues such as sample size requirements, stratification, level of randomization and data collection methods. Fourth, it discusses how to analyze data from randomized evaluations when there are departures from the basic framework. It reviews in particular how to handle imperfect compliance and externalities. Finally, it discusses some of the issues involved in drawing general conclusions from randomized evaluations, including the necessary use of theory as a guide when designing evaluations and interpreting results.},
  langid = {english},
  keywords = {development,experiments,program evaluation,randomized evaluations},
  file = {/Users/vincentbagilet/Zotero/storage/37M8Z5BV/Duflo et al. - 2007 - Using Randomization in Development Economics Resea.pdf}
}

@article{duflo_using_nodate,
  title = {{{USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH}}: {{A TOOLKIT}}},
  author = {Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  pages = {92},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/R5ZEKHVL/Duflo et al. - USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEA}
}

@article{duvendack_what_2017,
  title = {What {{Is Meant}} by "{{Replication}}" and {{Why Does It Encounter Resistance}} in {{Economics}}?},
  author = {Duvendack, Maren and {Palmer-Jones}, Richard and Reed, W. Robert},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {46--51},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171031},
  urldate = {2023-10-12},
  abstract = {This paper discusses recent trends in the use of replications in economics. We include the results of recent replication studies that have attempted to identify replication rates within the discipline. These studies generally find that replication rates are relatively low. We then consider obstacles to undertaking replication studies and highlight replication initiatives in psychology and political science, behind which economics appears to lag.},
  langid = {english},
  keywords = {Replication},
  file = {/Users/vincentbagilet/Zotero/storage/KHSI3IXG/Duvendack et al. - 2017 - What Is Meant by Replication and Why Does It Enc.pdf}
}

@article{ferman_inference_2019,
  title = {Inference in {{Differences-in-Differences}} with {{Few Treated Groups}} and {{Heteroskedasticity}}},
  author = {Ferman, Bruno and Pinto, Cristine},
  year = {2019},
  month = jul,
  journal = {The Review of Economics and Statistics},
  volume = {101},
  number = {3},
  pages = {452--467},
  issn = {0034-6535},
  doi = {10.1162/rest_a_00759},
  urldate = {2021-12-13},
  abstract = {We derive an inference method that works in differences-in-differences settings with few treated and many control groups in the presence of heteroskedasticity. As a leading example, we provide theoretical justification and empirical evidence that heteroskedasticity generated by variation in group sizes can invalidate existing inference methods, even in data sets with a large number of observations per group. In contrast, our inference method remains valid in this case. Our test can also be combined with feasible generalized least squares, providing a safeguard against misspecification of the serial correlation.},
  keywords = {DID},
  file = {/Users/vincentbagilet/Zotero/storage/C9R8BSER/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf;/Users/vincentbagilet/Zotero/storage/DQSVLHSF/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf;/Users/vincentbagilet/Zotero/storage/HZAZC6ZC/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf;/Users/vincentbagilet/Zotero/storage/INFNPQXB/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf}
}

@article{ferraro_featureis_2020,
  title = {Feature---{{Is}} a {{Replicability Crisis}} on the {{Horizon}} for {{Environmental}} and {{Resource Economics}}?},
  author = {Ferraro, Paul J. and Shukla, Pallavi},
  year = {2020},
  month = jun,
  journal = {Review of Environmental Economics and Policy},
  volume = {14},
  number = {2},
  pages = {339--351},
  publisher = {The University of Chicago Press},
  issn = {1750-6816},
  doi = {10.1093/reep/reaa011},
  urldate = {2021-03-23},
  abstract = {Environmental and resource economists pride themselves on the credibility of their           empirical research. In other disciplines, however, the credibility of empirical research           is increasingly being debated by scholars. At the core of these debates are critiques of           widespread practices, such as selectively reporting results or using designs with low           statistical power, and critiques of the professional incentives that encourage these           practices. These critiques have led to claims of a ``replicability crisis'' in science. We           show that questionable research practices are also prevalent in the environmental and           resource economics literature. We argue that the discipline needs to take the potential           harm from these practices more seriously. To mitigate this harm, we recommend changes in           the norms and practices of funders, editors, peer reviewers, and authors.             (JEL: Q0, C0)},
  keywords = {Environment,Exaggeration,Literature review,Meta-analysis,Power,Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/FXJV5P6J/Ferraro and Shukla - 2020 - Feature—Is a Replicability Crisis on the Horizon f.pdf;/Users/vincentbagilet/Zotero/storage/RUUER6TL/reaa011_supplementary_data.pdf}
}

@article{fisher_economic_2012,
  title = {The {{Economic Impacts}} of {{Climate Change}}: {{Evidence}} from {{Agricultural Output}} and {{Random Fluctuations}} in {{Weather}}: {{Comment}}},
  shorttitle = {The {{Economic Impacts}} of {{Climate Change}}},
  author = {Fisher, Anthony C. and Hanemann, W. Michael and Roberts, Michael J. and Schlenker, Wolfram},
  year = {2012},
  month = dec,
  journal = {American Economic Review},
  volume = {102},
  number = {7},
  pages = {3749--3760},
  issn = {0002-8282},
  doi = {10.1257/aer.102.7.3749},
  urldate = {2023-05-03},
  abstract = {In a series of studies employing a variety of approaches, we have found that the potential impact of climate change on US agriculture is likely negative. Desch{\~A}ªnes and Greenstone (2007) report dramatically different results based on regressions of agricultural profits and yields on weather variables. The divergence is explained by (1) missing and incorrect weather and climate data in their study; (2) their use of older climate change projections rather than the more recent and less optimistic projections from the Fourth Assessment Report; and (3) difficulties in their profit measure due to the confounding effects of storage.},
  langid = {english},
  keywords = {Agriculture and Environment Valuation of Environmental Effects Climate,Firm Performance: Size Diversification and Scope Micro Analysis of Farm Firms Farm Households and Farm Input Markets Land Ownership and Tenure,Global Warming,Irrigation,Land Reform,Land Use,Natural Disasters},
  file = {/Users/vincentbagilet/Zotero/storage/8L5MNX38/Fisher et al. - 2012 - The Economic Impacts of Climate Change Evidence f.pdf}
}

@article{fowler_electoral_2013,
  title = {Electoral and {{Policy Consequences}} of {{Voter Turnout}}: {{Evidence}} from {{Compulsory Voting}} in {{Australia}}},
  shorttitle = {Electoral and {{Policy Consequences}} of {{Voter Turnout}}},
  author = {Fowler, Anthony},
  year = {2013},
  journal = {Quarterly Journal of Political Science},
  volume = {8},
  number = {2},
  pages = {159--182},
  publisher = {now publishers},
  urldate = {2022-02-02},
  abstract = {Despite extensive research on voting, there is little evidence connecting turnout to tangible outcomes. Would election results and public policy be different if everyone voted? The adoption of compulsory voting in Australia provides a rare opportunity to address this question. First, I collect two novel data sources to assess the extent of turnout inequality in Australia before compulsory voting. Overwhelmingly, wealthy citizens voted more than their working-class counterparts. Next, exploiting the differential adoption of compulsory voting across states, I find that the policy increased voter turnout by 24 percentage points which in turn increased the vote shares and seat shares of the Labor Party by 7--10 percentage points. Finally, comparing across OECD countries, I find that Australia's adoption of compulsory voting significantly increased turnout and pension spending at the national level. Results suggest that democracies with voluntary voting do not represent the preferences of all citizens. Instead, increased voter turnout can dramatically alter election outcomes and resulting public policies.},
  file = {/Users/vincentbagilet/Zotero/storage/VC5CNH3Z/Fowler - 2013 - Electoral and Policy Consequences of Voter Turnout.pdf}
}

@article{fowler_regular_2015,
  title = {Regular {{Voters}}, {{Marginal Voters}} and the {{Electoral Effects}} of {{Turnout}}},
  author = {Fowler, Anthony},
  year = {2015},
  month = may,
  journal = {Political Science Research and Methods},
  volume = {3},
  number = {2},
  pages = {205--219},
  issn = {2049-8470, 2049-8489},
  doi = {10.1017/psrm.2015.18},
  urldate = {2024-06-28},
  abstract = {How do marginal voters differ from regular voters? This article develops a                         method for comparing the partisan preferences of regular voters to those                         marginal voters whose turnout decisions are influenced by exogenous factors                         and applies it to two sources of variation in turnout in the United                         States---weather and election timing. In both cases, marginal voters are over                         20 percentage points more supportive of the Democratic Party than regular                         voters---a significant divide. The findings suggest that the expansion or                         contraction of the electorate can have important consequences. Moreover, the                         findings suggest that election results do not always reflect the preferences                         of the citizenry, because the marginal citizens who may stay home have                         systematically different preferences than those who participate. Finally,                         the methods developed in the article may enable future researchers to                         compare regular and marginal voters on many different dimensions and in many                         different electoral settings.},
  langid = {english}
}

@article{freeman_power_2013,
  title = {Power and Sample Size Calculations for {{Mendelian}} Randomization Studies Using One Genetic Instrument},
  author = {Freeman, G. and Cowling, B. J. and Schooling, C. M.},
  year = {2013},
  month = aug,
  journal = {International Journal of Epidemiology},
  volume = {42},
  number = {4},
  pages = {1157--1163},
  issn = {0300-5771, 1464-3685},
  doi = {10.1093/ije/dyt110},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/BYCVRB6U/Freeman et al. - 2013 - Power and sample size calculations for Mendelian r.pdf}
}

@article{fujiwara_habit_2016,
  title = {Habit {{Formation}} in {{Voting}}: {{Evidence}} from {{Rainy Elections}}},
  shorttitle = {Habit {{Formation}} in {{Voting}}},
  author = {Fujiwara, Thomas and Meng, Kyle and Vogl, Tom},
  year = {2016},
  month = oct,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {4},
  pages = {160--188},
  issn = {1945-7782},
  doi = {10.1257/app.20140533},
  urldate = {2021-12-10},
  abstract = {We estimate habit formation in voting--the effect of past on current turnout--by exploiting transitory voting cost shocks. Using county-level data on US presidential elections from 1952-2012, we find that rainfall on current and past election days reduces voter turnout. Our estimates imply that a 1-point decrease in past turnout lowers current turnout by 0.6-1.0 points. Further analyses suggest that habit formation operates by reinforcing the direct consumption value of voting and that our estimates may be amplified by social spillovers.},
  langid = {english},
  keywords = {Elections,IV,Rainfall,Turnout},
  file = {/Users/vincentbagilet/Zotero/storage/DX8X7CVE/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf;/Users/vincentbagilet/Zotero/storage/DXI85JKA/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf;/Users/vincentbagilet/Zotero/storage/PAPLP8RE/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf;/Users/vincentbagilet/Zotero/storage/UJ7R46KS/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf}
}

@article{gelman_beyond_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  urldate = {2020-10-21},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {p-values,Power,Publication bias,Statistics,Type S/M errors},
  file = {/Users/vincentbagilet/Zotero/storage/QPUDVP3X/Gelman and Carlin - 2014 - Beyond Power Calculations Assessing Type S (Sign).pdf}
}

@misc{gelman_bias_2017,
  title = {``{{Bias}}'' and ``Variance'' Are Two Ways of Looking at the Same Thing. (``{{Bias}}'' Is Conditional, ``Variance'' Is Unconditional.)},
  author = {Gelman, Andrew},
  year = {March 18, 2017 1:02 PM},
  journal = {Statistical Modeling, Causal Inference, and Social Science},
  urldate = {2022-09-14}
}

@article{gelman_criticism_2022,
  title = {Criticism as Asynchronous Collaboration: {{An}} Example from Social Science Research},
  shorttitle = {Criticism as Asynchronous Collaboration},
  author = {Gelman, Andrew},
  year = {2022},
  journal = {Stat},
  volume = {11},
  number = {1},
  pages = {e464},
  issn = {2049-1573},
  doi = {10.1002/sta4.464},
  urldate = {2024-05-16},
  abstract = {I discuss a published paper in political science that made a claim that aroused skepticism. The reanalysis is an example of how we, as consumers as well as producers of science, can engage with published work. This can be viewed as a sort of collaboration performed implicitly between the authors of a published paper and later researchers who want to understand or use the published work.},
  copyright = {{\copyright} 2022 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {causal inference,regression,social sciences,subject areas},
  file = {/Users/vincentbagilet/Zotero/storage/HLW2NMCK/Gelman - 2022 - Criticism as asynchronous collaboration An exampl.pdf;/Users/vincentbagilet/Zotero/storage/BYDVAZGY/sta4.html}
}

@article{gelman_garden_2013,
  title = {The Garden of Forking Paths: {{Why}} Multiple Comparisons Can Be a Problem, Even When There Is No ``Fishing Expedition'' or ``p-Hacking'' and the Research Hypothesis Was Posited Ahead of Time},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2013},
  month = nov,
  abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data.},
  langid = {english},
  keywords = {Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/HCGEUVQQ/Gelman and Loken - The garden of forking paths Why multiple comparis.pdf}
}

@book{gelman_regression_2020,
  title = {{Regression and Other Stories}},
  author = {Gelman, Andrew},
  year = {2020},
  publisher = {Cambridge University Press},
  address = {Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore},
  isbn = {978-1-107-67651-0},
  langid = {Anglais}
}

@article{gelmanType2000,
  title = {Type {{S}} Error Rates for Classical and {{Bayesian}} Single and Multiple Comparison Procedures},
  author = {Gelman, Andrew and Tuerlinckx, Francis},
  year = {2000},
  month = sep,
  journal = {Computational Statistics},
  volume = {15},
  number = {3},
  pages = {373--390},
  issn = {1613-9658},
  doi = {10.1007/s001800000040},
  urldate = {2020-10-26},
  abstract = {In classical statistics, the significance of comparisons (e.g., {\texttheta}1- {\texttheta}2) is calibrated using the Type 1 error rate, relying on the assumption that the true difference is zero, which makes no sense in many applications. We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state ``{\texttheta}1 {$>$} {\texttheta}2 with confidence,'' ``{\texttheta}2 {$>$} {\texttheta}1 with confidence,'' or ``no claim with confidence.'' We focus on the Type S (for sign) error, which occurs when you claim ``{\texttheta}1 {$>$} {\texttheta}2 with confidence'' when {\texttheta}2{$>$} {\texttheta}1 (or vice-versa). We compute the Type S error rates for classical and Bayesian confidence statements and find that classical Type S error rates can be extremely high (up to 50\%). Bayesian confidence statements are conservative, in the sense that claims based on 95\% posterior intervals have Type S error rates between 0 and 2.5\%. For multiple comparison situations, the conclusions are similar.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/SHSBNPVS/Gelman and Tuerlinckx - 2000 - Type S error rates for classical and Bayesian sing.pdf}
}

@article{gibbons_broken_2019,
  title = {Broken or {{Fixed Effects}}?},
  author = {Gibbons, Charles E. and Serrato, Juan Carlos Su{\'a}rez and Urbancic, Michael B.},
  year = {2019},
  month = jan,
  journal = {Journal of Econometric Methods},
  volume = {8},
  number = {1},
  issn = {2156-6674},
  doi = {10.1515/jem-2017-0002},
  urldate = {2022-07-28},
  abstract = {We replicate eight influential papers to provide empirical evidence that, in the presence of heterogeneous treatment effects, OLS with fixed effects (FE) is generally not a consistent estimator of the average treatment effect (ATE). We propose two alternative estimators that recover the ATE in the presence of group-specific heterogeneity. We document that heterogeneous treatment effects are common and the ATE is often statistically and economically different from the FE estimate. In all but one of our replications, there is statistically significant treatment effect heterogeneity and, in six, the ATEs are either economically or statistically different from the FE estimates.},
  langid = {english},
  keywords = {Fixed effects,Heterogeneous effects,Identify variation,Maths,Weights},
  annotation = {Fixed effects lead to a weighted average of treatment effects (observations with more variance in the treatment conditional on covariates weighted more heavily)},
  file = {/Users/vincentbagilet/Zotero/storage/EK95NVH9/Gibbons et al. - 2019 - Broken or Fixed Effects.pdf}
}

@misc{gilbert_recovering_2023,
  title = {Recovering {{Effect Sizes}} from {{Dichotomous Variables Using Logistic Regression}}},
  author = {Gilbert, Josh and Miratrix, Luke},
  year = {2023},
  month = mar,
  urldate = {2023-10-18},
  abstract = {The Problem with Logistic Regression In OLS regression with continuous outcomes, an omitted variable will not affect the estimation of an included variable if {\dots}},
  langid = {english},
  keywords = {Controls,Leo,Logistic regression,To read,Variance}
}

@misc{goldfeld_generating_nodate,
  title = {Generating Data to Explore the Myriad Causal Effects That Can Be Estimated in Observational Data Analysis},
  author = {Goldfeld, Keith},
  journal = {ouR data generation},
  urldate = {2021-06-30},
  abstract = {I've been inspired by two recent talks describing the challenges of using instrumental variable (IV) methods. IV methods are used to estimate the causal effects of an exposure or intervention when there is unmeasured confounding. This estimated causal effect is very specific: the complier average causal effect (CACE). But, the CACE is just one of several possible causal estimands that we might be interested in. For example, there's the average causal effect (ACE) that represents a population average (not just based the subset of compliers).},
  langid = {english},
  keywords = {Simulations}
}

@article{goldsmith-pinkham_bartik_2020,
  title = {Bartik {{Instruments}}: {{What}}, {{When}}, {{Why}}, and {{How}}},
  shorttitle = {Bartik {{Instruments}}},
  author = {{Goldsmith-Pinkham}, Paul and Sorkin, Isaac and Swift, Henry},
  year = {2020},
  month = aug,
  journal = {American Economic Review},
  volume = {110},
  number = {8},
  pages = {2586--2624},
  issn = {0002-8282},
  doi = {10.1257/aer.20181047},
  urldate = {2021-10-11},
  abstract = {The Bartik instrument is formed by interacting local industry shares and national industry growth rates. We show that the typical use of a Bartik instrument assumes a pooled exposure research design, where the shares measure differential exposure to common shocks, and identification is based on exogeneity of the shares. Next, we show how the Bartik instrument weights each of the exposure designs. Finally, we discuss how to assess the plausibility of the research design. We illustrate our results through two applications: estimating the elasticity of labor supply, and estimating the elasticity of substitution between immigrants and natives.},
  langid = {english},
  keywords = {and Immigrants,and Transportation Economics: Regional Migration,Economics of Minorities,Empirical Studies of Trade,Indigenous Peoples,Industry Studies: Manufacturing: General,Model Construction and Estimation,Model Construction and Estimation Empirical Studies of Trade Economics of Minorities Races Indigenous Peoples and Immigrants,Neighborhood Characteristics,Neighborhood Characteristics Other Spatial Production and Pricing Analysis,Non-labor Discrimination,Non-labor Discrimination Time Allocation and Labor Supply Industry Studies: Manufacturing: General Urban Rural Regional Real Estate and Transportation Economics: Regional Migration,Other Spatial Production and Pricing Analysis,Population,Races,Real Estate,Regional,Regional Labor Markets,Rural,Time Allocation and Labor Supply,Urban},
  file = {/Users/vincentbagilet/Zotero/storage/223PSFJU/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf;/Users/vincentbagilet/Zotero/storage/7UCS4JRK/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf;/Users/vincentbagilet/Zotero/storage/JRE776A4/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf;/Users/vincentbagilet/Zotero/storage/JW8VUECR/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf}
}

@article{gomez_republicans_2007,
  title = {The {{Republicans Should Pray}} for {{Rain}}: {{Weather}}, {{Turnout}}, and {{Voting}} in {{U}}.{{S}}. {{Presidential Elections}}},
  shorttitle = {The {{Republicans Should Pray}} for {{Rain}}},
  author = {Gomez, Brad T. and Hansford, Thomas G. and Krause, George A.},
  year = {2007},
  journal = {The Journal of Politics},
  volume = {69},
  number = {3},
  eprint = {10.1111/j.1468-2508.2007.00565.x},
  eprinttype = {jstor},
  pages = {649--663},
  publisher = {[The University of Chicago Press, Southern Political Science Association]},
  issn = {0022-3816},
  doi = {10.1111/j.1468-2508.2007.00565.x},
  urldate = {2021-12-10},
  abstract = {The relationship between bad weather and lower levels of voter turnout is widely espoused by media, political practitioners, and, perhaps, even political scientists. Yet, there is virtually no solid empirical evidence linking weather to voter participation. This paper provides an extensive test of the claim. We examine the effect of weather on voter turnout in 14 U.S. presidential elections. Using GIS interpolations, we employ meteorological data drawn from over 22,000 U.S. weather stations to provide election day estimates of rain and snow for each U.S. county. We find that, when compared to normal conditions, rain significantly reduces voter participation by a rate of just less than 1\% per inch, while an inch of snowfall decreases turnout by almost .5\%. Poor weather is also shown to benefit the Republican party's vote share. Indeed, the weather may have contributed to two Electoral College outcomes, the 1960 and 2000 presidential elections.},
  file = {/Users/vincentbagilet/Zotero/storage/HWA85Y4P/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf;/Users/vincentbagilet/Zotero/storage/I54JJBNE/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf;/Users/vincentbagilet/Zotero/storage/SI5CHN2E/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf;/Users/vincentbagilet/Zotero/storage/ZSWKFZ5D/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf}
}

@article{goodman-bacon_difference--differences_2021,
  title = {Difference-in-Differences with Variation in Treatment Timing},
  author = {{Goodman-Bacon}, Andrew},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  series = {Themed {{Issue}}: {{Treatment Effect}} 1},
  volume = {225},
  number = {2},
  pages = {254--277},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2021.03.014},
  urldate = {2022-05-06},
  abstract = {The canonical difference-in-differences (DD) estimator contains two time periods, ''pre'' and ''post'', and two groups, ''treatment'' and ''control''. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
  langid = {english},
  keywords = {Econometrics,Fixed effects,Heterogeneous effects,Identify variation,TWFE,Weights},
  file = {/Users/vincentbagilet/Zotero/storage/6J3WS6NE/Goodman-Bacon - 2021 - Difference-in-differences with variation in treatm.pdf}
}

@article{graffzivin_temperature_2014,
  title = {Temperature and the {{Allocation}} of {{Time}}: {{Implications}} for {{Climate Change}}},
  shorttitle = {Temperature and the {{Allocation}} of {{Time}}},
  author = {Graff Zivin, Joshua and Neidell, Matthew},
  year = {2014},
  month = jan,
  journal = {Journal of Labor Economics},
  volume = {32},
  number = {1},
  pages = {1--26},
  publisher = {The University of Chicago Press},
  issn = {0734-306X},
  doi = {10.1086/671766},
  urldate = {2024-05-13},
  abstract = {We estimate the impacts of temperature on time allocation by exploiting plausibly exogenous variation in temperature over time within counties. Temperature increases at the higher end of the distribution reduce hours worked in industries with high exposure to climate and reduce time allocated to outdoor leisure for the nonemployed, with this time reallocated to indoor leisure. At the lower end of the distribution, time allocated to labor is nonresponsive to temperature increases, but outdoor leisure increases while indoor leisure decreases as temperature warms. We also find suggestive evidence of short-run adaptation to higher temperatures through temporal substitutions and acclimatization.},
  keywords = {Labor,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/W6CA373N/Graff Zivin and Neidell - 2014 - Temperature and the Allocation of Time Implicatio.pdf}
}

@article{graffzivin_temperature_2018,
  title = {Temperature and {{Human Capital}} in the {{Short}} and {{Long Run}}},
  author = {Graff Zivin, Joshua and Hsiang, Solomon M. and Neidell, Matthew},
  year = {2018},
  month = jan,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {5},
  number = {1},
  pages = {77--105},
  publisher = {The University of Chicago Press},
  issn = {2333-5955},
  doi = {10.1086/694177},
  urldate = {2024-06-11},
  abstract = {We provide the first estimates of the potential impact of climate change on cognitive performance and attainment, focusing on the impacts from both short-run weather and long-run climate. Exploiting the longitudinal structure of the NLSY79 and random fluctuations in weather across interviews, we identify the effect of temperature in models with child-specific fixed effects. We find that short-run changes in temperature lead to statistically significant decreases in cognitive performance on math (but not reading) beyond 26{$^\circ$}C (78.8{$^\circ$}F). In contrast, our long-run analysis, which relies upon long-difference and rich cross-sectional models, reveals an imprecisely estimated effect that is significantly smaller than the short-run relationship between climate and human capital.},
  keywords = {Adaptation,Climate change,Human capital,J24,Q54},
  file = {/Users/vincentbagilet/Zotero/storage/VKMVHTFT/Graff Zivin et al. - 2018 - Temperature and Human Capital in the Short and Lon.pdf}
}

@article{graffzivin_temperature_2020,
  title = {Temperature and High-Stakes Cognitive Performance: {{Evidence}} from the National College Entrance Examination in {{China}}},
  shorttitle = {Temperature and High-Stakes Cognitive Performance},
  author = {Graff Zivin, Joshua and Song, Yingquan and Tang, Qu and Zhang, Peng},
  year = {2020},
  month = nov,
  journal = {Journal of Environmental Economics and Management},
  volume = {104},
  pages = {102365},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2020.102365},
  urldate = {2024-06-11},
  abstract = {We provide the first nation-wide estimates of the effects of temperature on high-stakes cognitive performance in a developing country using data from the National College Entrance Examination (NCEE) in China. The NCEE is one of the most important institutions in China and affects millions of families. We find that a one-standard-deviation increase in temperature during the exam period within counties (2~{$^\circ$}C/3.6~{$^\circ$}F) decreases the total test score by 0.68\%, or 5.83\% of a standard deviation, with effects concentrated on the highest performing students. This suggests that temperature plays an important role in high-stakes cognitive performance and has potentially far-reaching impacts for the careers and lifetime earnings of students.},
  keywords = {Climate change,Cognitive performance,Standardized test,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/S7CXTT8P/Graff Zivin et al. - 2020 - Temperature and high-stakes cognitive performance.pdf;/Users/vincentbagilet/Zotero/storage/7X8WRUQ7/S0095069620300887.html}
}

@article{gray_environmental_2023,
  title = {Environmental {{Regulation}} and {{Labor Demand}}: {{What Does}} the {{Evidence Tell Us}}?},
  shorttitle = {Environmental {{Regulation}} and {{Labor Demand}}},
  author = {Gray, Wayne B. and Shadbegian, Ron and Wolverton, Ann},
  year = {2023},
  journal = {Annual Review of Resource Economics},
  volume = {15},
  number = {1},
  pages = {177--197},
  doi = {10.1146/annurev-resource-101422-115834},
  urldate = {2023-10-06},
  abstract = {Understanding the potential effect of environmental regulation on employment is of broad interest to key stakeholders. Concerns encompass both short- and longer-term effects on workers within the regulated sector, affected communities that already suffer from a lack of employment opportunities, and net employment in the overall economy. We begin our review by presenting a neoclassical microeconomic framework demonstrating how environmental regulations might affect labor demand. We then summarize the main empirical findings from the literature, including sector-specific partial equilibrium estimates and general equilibrium approaches to identifying the employment impacts of regulations. We also briefly describe the literature on how environmental regulations affect labor supply. Finally, we discuss remaining research gaps.},
  keywords = {CBA,Employment,Environment,Environmental Regulation,Jobs},
  file = {/Users/vincentbagilet/Zotero/storage/7CBNST2I/Gray et al. - 2023 - Environmental Regulation and Labor Demand What Do.pdf}
}

@article{greenstone_impacts_2002,
  title = {The {{Impacts}} of {{Environmental Regulations}} on {{Industrial Activity}}: {{Evidence}} from the 1970 and 1977 {{Clean Air Act Amendments}} and the {{Census}} of {{Manufactures}}},
  shorttitle = {The {{Impacts}} of {{Environmental Regulations}} on {{Industrial Activity}}},
  author = {Greenstone, Michael},
  year = {2002},
  month = dec,
  journal = {Journal of Political Economy},
  volume = {110},
  number = {6},
  pages = {1175--1219},
  publisher = {The University of Chicago Press},
  issn = {0022-3808},
  doi = {10.1086/342808},
  urldate = {2023-10-22},
  abstract = {This paper estimates the impacts of the Clean Air Act's division of counties into pollutant-specific nonattainment and attainment categories on measures of industrial activity obtained from 1.75 million plant observations from the Census of Manufactures. Emitters of the controlled pollutants in nonattainment counties were subject to greater regulatory oversight than emitters in attainment counties. The preferred statistical model for plant-level growth includes plant fixed effects, industry by period fixed effects, and county by period fixed effects. The estimates from this model suggest that in the first 15 years in which the Clean Air Act was in force (1972--87), nonattainment counties (relative to attainment ones) lost approximately 590,000 jobs, \$37 billion in capital stock, and \$75 billion (1987 dollars) of output in pollution-intensive industries. These findings are robust across many specifications, and the effects are apparent in many polluting industries.},
  keywords = {Clean Air Act,Employment,Environment,Environmental Regulation,Jobs,US},
  file = {/Users/vincentbagilet/Zotero/storage/6XFID3IG/Greenstone - 2002 - The Impacts of Environmental Regulations on Indust.pdf}
}

@article{griffin_moving_2021,
  title = {Moving beyond the Classic Difference-in-Differences Model: {{A}} Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies},
  shorttitle = {Moving beyond the Classic Difference-in-Differences Model},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Stuart, Elizabeth A. and Patrick, Stephen and McNeer, Elizabeth and Smart, Rosanna and Powell, David and Stein, Bradley D. and Schell, Terry and Pacula, Rosalie L.},
  year = {2021},
  month = jun,
  journal = {arXiv:2003.12008 [stat]},
  eprint = {2003.12008},
  primaryclass = {stat},
  urldate = {2021-12-13},
  abstract = {State-level policy evaluations commonly employ a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. Motivated by applied state-level opioid policy evaluations, this simulation study compares statistical performance of multiple variations of two-way fixed effect models traditionally used for DID under a range of simulation conditions. While most linear models resulted in minimal bias, non-linear models and population-weighted versions of classic linear two-way fixed effect and linear GEE models yielded considerable bias (60 to 160\%). Further, root mean square error is minimized by linear AR models when examining crude mortality rates and by negative binomial models when examining raw death counts. In the context of frequentist hypothesis testing, many models yielded high Type I error rates and very low rates of correctly rejecting the null hypothesis ({$<$} 10\%), raising concerns of spurious conclusions about policy effectiveness. When considering performance across models, the linear autoregressive models were optimal in terms of directional bias, root mean squared error, Type I error, and correct rejection rates. These findings highlight notable limitations of traditional statistical models commonly used for DID designs, designs widely used in opioid policy studies and in state policy evaluations more broadly.},
  archiveprefix = {arXiv},
  keywords = {DID},
  file = {/Users/vincentbagilet/Zotero/storage/2Z9C83TR/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf;/Users/vincentbagilet/Zotero/storage/6E65YRZL/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf;/Users/vincentbagilet/Zotero/storage/8W959THM/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf;/Users/vincentbagilet/Zotero/storage/WMTE6KXJ/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf}
}

@article{hafstead_jobs_2020,
  title = {Jobs and {{Environmental Regulation}}},
  author = {Hafstead, Marc A. C. and Williams, Roberton C.},
  year = {2020},
  month = jan,
  journal = {Environmental and Energy Policy and the Economy},
  volume = {1},
  pages = {192--240},
  publisher = {The University of Chicago Press},
  issn = {2689-7857},
  doi = {10.1086/706799},
  urldate = {2023-10-04},
  abstract = {Political debates about environmental regulation often center around the effect of policy on jobs. Opponents decry the ``job-killing'' Environmental Protection Agency (EPA) and proponents point to ``green jobs'' as a positive policy outcome. Beyond the political debates, Congress requires the EPA to evaluate ``potential losses or shifts of employment'' that regulations under the Clean Air Act may cause. Yet there is a sharp disconnect between the political importance of the jobs question and the limited research on the job effects of policy and general skepticism in the academic literature about the importance of those job effects for the costs and benefits of environmental regulation. In this paper, we discuss how the existing research on jobs and environmental regulations often falls short in evaluating these questions and consider recent new work that has attempted to address these problems. We provide an intuitive discussion of key questions for how job effects should enter into economic analysis of regulations. And, using an economic model that incorporates labor market frictions, we evaluate a range of environmental regulations in both the short and long run to develop a set of key stylized facts related to jobs and environmental regulations and to identify the key questions that current models cannot yet answer well.},
  keywords = {CBA,Employment,Environment,Environmental Regulation},
  file = {/Users/vincentbagilet/Zotero/storage/P9QWW5Q4/Hafstead and Williams - 2020 - Jobs and Environmental Regulation.pdf}
}

@article{hannaEffectPollutionLabor2015,
  title = {The Effect of Pollution on Labor Supply: {{Evidence}} from a Natural Experiment in {{Mexico City}}},
  shorttitle = {The Effect of Pollution on Labor Supply},
  author = {Hanna, Rema and Oliva, Paulina},
  year = {2015},
  month = feb,
  journal = {Journal of Public Economics},
  volume = {122},
  pages = {68--79},
  issn = {0047-2727},
  doi = {10.1016/j.jpubeco.2014.10.004},
  urldate = {2024-03-31},
  abstract = {Moderate effects of pollution on health may exert important influences on work. We exploit exogenous variation in pollution due to the closure of a large refinery in Mexico City to understand how pollution impacts labor supply. The closure led to a 19.7\% decline in pollution, as measured by SO2, in the surrounding neighborhoods. The closure led to a 1.3h (or 3.5\%) increase in work hours per week. The effects do not appear to be driven by differential labor demand shocks nor selective migration.},
  keywords = {Air pollution,DiD,Labor,Labor supply,Mexico,SO2,Triple diff},
  file = {/Users/vincentbagilet/Zotero/storage/QQ4FXDWT/Hanna and Oliva - 2015 - The effect of pollution on labor supply Evidence .pdf;/Users/vincentbagilet/Zotero/storage/XARTWMYN/S0047272714002096.html}
}

@article{hannon_investigator_1993,
  title = {Investigator {{Disturbance}} and {{Clutch Predation}} in {{Willow Ptarmigan}}: {{Methods}} for {{Evaluating Impact}} ({{M{\'e}todos}} Para Evaluar El Impacto Del Disturbio Causado Por El Investigador En La Depredaci{\'o}n de Camadas de Individuos de {{Lagopus}} Lagopus)},
  shorttitle = {Investigator {{Disturbance}} and {{Clutch Predation}} in {{Willow Ptarmigan}}},
  author = {Hannon, Susan J. and Martin, Kathy and Thomas, Len and Schieck, Jim},
  year = {1993},
  journal = {Journal of Field Ornithology},
  volume = {64},
  number = {4},
  eprint = {4513871},
  eprinttype = {jstor},
  pages = {575--586},
  publisher = {[Association of Field Ornithologists, Wiley]},
  issn = {0273-8570},
  urldate = {2024-05-16},
  abstract = {The possible effect of investigator disturbance on clutch predation in two populations of Willow Ptarmigan (Lagopus lagopus) was investigated using three different techniques. No significant difference was found in (1) the proportion of hens that succeeded in producing broods for hens whose nests were found vs. those that were not located, (2) proportion of broodless hens on an intensively studied area vs. an area that was not studied, (3) the proximity of nest flags for successful vs. depredated nests. A new technique was developed to compare rates of visits and types of nest checks between successful and depredated nests. This method controlled for period under observation and stage of breeding. Methods for minimizing investigator disturbance are discussed. /// Utilizando tres t{\'e}cnicas diferentes se estudi{\'o}, en dos poblaciones de Lagopus lagopus, el posible efecto del disturbio causado por el investigador en la depredaci{\'o}n de camadas. No se encontr{\'o} diferencia significativa en (1) la proporci{\'o}n de hembras exitosas en producir polluelos (al compararse aquellas cuyos nidos fueron localizados vs. aqu{\'e}llas cuyos nidos no fueron localizados), (2) proporci{\'o}n de hembras sin camadas en un {\'a}rea de estudio intensiva vs. {\'a}reas que no fueron estudiadas, y (3) la proximidad de banderolas a de nidos (entre nidos exitosos vs. nidos depredados). Una nueva t{\'e}cnica fue desarrollada para comparar el efecto de la tasa de visitas y tipos de examen a nidos entre nidos exitosos y depredados. Este m{\'e}todo puede ser regulado para per{\'i}odos de observaci{\'o}n y etapas reproductivas. Se discuten m{\'e}todos para minimizar el disturbio causado por los investigadores.},
  keywords = {Birds,Power,Power simulation,Simulations},
  file = {/Users/vincentbagilet/Zotero/storage/8BHLN5SE/Hannon et al. - 1993 - Investigator Disturbance and Clutch Predation in W.pdf}
}

@book{hansen_econometrics_2022,
  title = {Econometrics},
  author = {Hansen, Bruce},
  year = {2022},
  month = aug,
  publisher = {Princeton University Press},
  address = {Princeton},
  isbn = {978-0-691-23589-9},
  langid = {english},
  keywords = {Econometrics,Handbook},
  file = {/Users/vincentbagilet/Zotero/storage/3U6F55TK/Hansen - 2022 - Econometrics.pdf}
}

@article{hansford_estimating_2010,
  title = {Estimating the {{Electoral Effects}} of {{Voter Turnout}}},
  author = {Hansford, Thomas G. and Gomez, Brad T.},
  year = {2010},
  month = may,
  journal = {American Political Science Review},
  volume = {104},
  number = {2},
  pages = {268--288},
  publisher = {Cambridge University Press},
  issn = {1537-5943, 0003-0554},
  doi = {10.1017/S0003055410000109},
  urldate = {2022-02-23},
  abstract = {This article examines the electoral consequences of variation in voter turnout in the United States. Existing scholarship focuses on the claim that high turnout benefits Democrats, but evidence supporting this conjecture is variable and controversial. Previous work, however, does not account for endogeneity between turnout and electoral choice, and thus, causal claims are questionable. Using election day rainfall as an instrumental variable for voter turnout, we are able to estimate the effect of variation in turnout due to across-the-board changes in the utility of voting. We re-examine the Partisan Effects and Two-Effects Hypotheses, provide an empirical test of an Anti-Incumbent Hypothesis, and propose a Volatility Hypothesis, which posits that high turnout produces less predictable electoral outcomes. Using county-level data from the 1948--2000 presidential elections, we find support for each hypothesis. Failing to address the endogeneity problem would lead researchers to incorrectly reject all but the Anti-Incumbent Hypothesis. The effect of variation in turnout on electoral outcomes appears quite meaningful. Although election-specific factors other than turnout have the greatest influence on who wins an election, variation in turnout significantly affects vote shares at the county, national, and Electoral College levels.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/T9HWXXWP/Hansford and Gomez - 2010 - Estimating the Electoral Effects of Voter Turnout.pdf}
}

@book{harrerDoingMetaAnalysis,
  title = {Doing {{Meta-Analysis}} in {{R}}},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi A. and Ebert, David D.},
  urldate = {2024-03-29},
  abstract = {This is a guide on how to conduct Meta-Analyses in R.},
  langid = {english},
  keywords = {Handbook,Meta-analysis,R}
}

@article{he_straw_2020,
  title = {Straw Burning, {{PM2}}.5, and Death: {{Evidence}} from {{China}}},
  shorttitle = {Straw Burning, {{PM2}}.5, and Death},
  author = {He, Guojun and Liu, Tong and Zhou, Maigeng},
  year = {2020},
  month = jun,
  journal = {Journal of Development Economics},
  volume = {145},
  pages = {102468},
  issn = {0304-3878},
  doi = {10.1016/j.jdeveco.2020.102468},
  urldate = {2023-11-08},
  abstract = {This study uses satellite data to detect agricultural straw burning and estimates its impact on air pollution and health in China. We find that straw burning increases particulate matter pollution and causes people to die from cardiorespiratory diseases. We estimate that a 10~\hspace{0pt}{$\mu$}g/m3 increase in PM2.5 increases mortality by 3.25\%. Middle-aged and old people in rural areas are particularly sensitive to straw burning pollution. Exploratory analysis of China's programs to subsidize straw recycling suggests that extending these programs to all the straw burning regions would bring about a health benefit that is an order of magnitude larger than the cost.},
  keywords = {Air pollution,Mortality,Straw burning,Straw recycling,Stubble burning},
  file = {/Users/vincentbagilet/Zotero/storage/TRJQPKEV/He et al. - 2020 - Straw burning, PM2.5, and death Evidence from Chi.pdf}
}

@misc{heiss_exploring_2021,
  title = {Exploring {{R}}{$^2$} and Regression Variance with {{Euler}}/{{Venn}} Diagrams},
  author = {Heiss, Andrew},
  year = {2021},
  month = aug,
  urldate = {2022-10-20},
  abstract = {Use R to correctly close backdoor confounding in panel data with marginal structural models and inverse probability weights with both GEE and multilevel models},
  chapter = {blog},
  howpublished = {https://www.andrewheiss.com/blog/2021/08/21/r2-euler/},
  langid = {english}
}

@article{herbst_peer_2015,
  title = {Peer Effects on Worker Output in the Laboratory Generalize to the Field},
  author = {Herbst, Daniel and Mas, Alexandre},
  year = {2015},
  month = oct,
  journal = {Science},
  volume = {350},
  number = {6260},
  pages = {545--549},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac9555},
  urldate = {2023-02-15},
  abstract = {Comparing lab and field estimates             What do Swiss high-school students and Eastern European seasonal laborers have in common? When the former are tasked with stuffing questionnaires into envelopes in a classroom setting and the latter are employed to pick fruit in the United Kingdom, both work harder in the presence of their peers. Herbst and Mas reanalyzed the results of 35 such studies, either experiments carried out under controlled conditions or empirical studies based on data collected in the field (see the Perspective by Charness and Fehr). Encouragingly, they found that the magnitude of the spillover effect---how much harder a worker works when other workers are alongside---was the same.                            Science               , this issue p.               545               ; see also p.               512                        ,                             Lab experiments and real-world observations are in agreement that people work harder when they work together.                                [Also see Perspective by                 Charness and Fehr                 ]                                       ,                             We compare estimates of peer effects on worker output in laboratory experiments and field studies from naturally occurring environments. The mean study-level estimate of a change in a worker's productivity in response to an increase in a co-worker's productivity ({$\gamma$}) is                                                                        {$\gamma$}                     {\textasciicircum}                                                                  = 0.12 (SE = 0.03,               n               studies               = 34), with a between-study standard deviation {$\tau$} = 0.16. The mean estimated                                                                        {$\gamma$}                     {\textasciicircum}                                                                  -values are close between laboratory and field studies (                                                                                                                        {$\gamma$}                         {\textasciicircum}                                                                       lab                                                                 -                                                                     {$\gamma$}                         {\textasciicircum}                                                                       field                                                                                                              = 0.04,               P               = 0.55,               n               lab               = 11,               n               field               = 23), as are estimates of between-study variance {$\tau$}               2               (                                                                                                                        {$\tau$}                         {\textasciicircum}                                                                       lab                                              2                                          -                                                                     {$\tau$}                         {\textasciicircum}                                                                       field                                              2                                          =                     -                     0.003                                                                  ,               P               = 0.89). The small mean difference between laboratory and field estimates holds even after controlling for sample characteristics such as incentive schemes and work complexity (                                                                                                                        {$\gamma$}                         {\textasciicircum}                                                                       lab                                                                 -                                                                     {$\gamma$}                         {\textasciicircum}                                                                       field                                                                                                              = 0.03,               P               = 0.62,               n               samples               = 46). Laboratory experiments generalize quantitatively in that they provide an accurate description of the mean and variance of productivity spillovers.},
  langid = {english},
  keywords = {Literature review,Peer effects},
  file = {/Users/vincentbagilet/Zotero/storage/MBKDXDNS/Herbst and Mas - 2015 - Peer effects on worker output in the laboratory ge.pdf}
}

@book{hernan_causal_2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A and Robins, James M},
  year = {2020},
  edition = {Boca Raton: Chapman \& Hall/CRC},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/6PAWZAPU/Hernán and Robins - Causal Inference What If.pdf}
}

@article{hernan_causal_2022,
  title = {Causal Analyses of Existing Databases: No Power Calculations Required},
  shorttitle = {Causal Analyses of Existing Databases},
  author = {Hern{\'a}n, Miguel A.},
  year = {2022},
  month = apr,
  journal = {Journal of Clinical Epidemiology},
  volume = {144},
  pages = {203--205},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2021.08.028},
  urldate = {2024-05-07},
  abstract = {Observational databases are often used to study causal questions. Before being granted access to data or funding, researchers may need to prove that ``the statistical power of their analysis will be high.'' Analyses expected to have low power, and hence result in imprecise estimates, will not be approved. This restrictive attitude towards observational analyses is misguided. A key misunderstanding is the belief that the goal of a causal analysis is to ``detect'' an effect. Causal effects are not binary signals that are either detected or undetected; causal effects are numerical quantities that need to be estimated. Because the goal is to quantify the effect as unbiasedly and precisely as possible, the solution to observational analyses with imprecise effect estimates is not avoiding observational analyses with imprecise estimates, but rather encouraging the conduct of many observational analyses. It is preferable to have multiple studies with imprecise estimates than having no study at all. After several studies become available, we will meta-analyze them and provide a more precise pooled effect estimate. Therefore, the justification to withhold an observational analysis of preexisting data cannot be that our estimates will be imprecise. Ethical arguments for power calculations before conducting a randomized trial which place individuals at risk are not transferable to observational analyses of existing databases. If a causal question is important, analyze your data, publish your estimates, encourage others to do the same, and then meta-analyze. The alternative is an unanswered question.},
  keywords = {Causal analysis,Causal inference,Meta-analysis,Observational analysis,Observational studies,Sample size,Statistical power,Statistical significance},
  file = {/Users/vincentbagilet/Zotero/storage/TG9Z3EDY/Hernán - 2022 - Causal analyses of existing databases no power ca.pdf;/Users/vincentbagilet/Zotero/storage/V8NRUJS3/S0895435621002730.html}
}

@article{heyes_temperature_2019,
  title = {Temperature and {{Decisions}}: {{Evidence}} from 207,000 {{Court Cases}}},
  shorttitle = {Temperature and {{Decisions}}},
  author = {Heyes, Anthony and Saberian, Soodeh},
  year = {2019},
  month = apr,
  journal = {American Economic Journal: Applied Economics},
  volume = {11},
  number = {2},
  pages = {238--265},
  issn = {1945-7782},
  doi = {10.1257/app.20170223},
  urldate = {2024-05-11},
  abstract = {We analyze the impact of outdoor temperature on high-stakes decisions (immigration adjudications) made by professional decision-makers (US immigration judges). In our preferred specification, which includes spatial, temporal, and judge fixed effects, and controls for various potential confounders, a 10{$^\circ$}F degree increase in case-day temperature reduces decisions favorable to the applicant by 6.55 percent. This is despite judgements being made indoors, "protected" by climate control. Results are consistent with established links from temperature to mood and risk appetite and have important implications for evaluating the influence of climate on "cognitive output."},
  langid = {english},
  keywords = {California,Cognitive,Immigration,Judges,Labor,Law,Productivity,Temperature,US},
  file = {/Users/vincentbagilet/Zotero/storage/9CV25A3X/Heyes and Saberian - 2019 - Temperature and Decisions Evidence from 207,000 C.pdf}
}

@article{hill_bayesian_2011,
  title = {Bayesian {{Nonparametric Modeling}} for {{Causal Inference}}},
  author = {Hill, Jennifer L.},
  year = {2011},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {1},
  pages = {217--240},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2010.08162},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/TN4HL7CV/Hill - 2011 - Bayesian Nonparametric Modeling for Causal Inferen.pdf}
}

@article{hoxby_effects_2000,
  title = {The {{Effects}} of {{Class Size}} on {{Student Achievement}}: {{New Evidence}} from {{Population Variation}}*},
  shorttitle = {The {{Effects}} of {{Class Size}} on {{Student Achievement}}},
  author = {Hoxby, Caroline M.},
  year = {2000},
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {115},
  number = {4},
  pages = {1239--1285},
  issn = {0033-5533},
  doi = {10.1162/003355300555060},
  urldate = {2021-08-10},
  abstract = {I identify the effects of class size on student achievement using longitudinal variation in the population associated with each grade in 649 elementary schools. I use variation in class size driven by idiosyncratic variation in the population. I also use discrete jumps in class size that occur when a small change in enrollment triggers a maximum or minimum class size rule. The estimates indicate that class size does not have a statistically significant effect on student achievement. I rule out even modest effects (2 to 4 percent of a standard deviation in scores for a 10 percent reduction in class size).},
  keywords = {Class size,Education},
  file = {/Users/vincentbagilet/Zotero/storage/86LLSHR4/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf;/Users/vincentbagilet/Zotero/storage/B9R9XSTB/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf;/Users/vincentbagilet/Zotero/storage/JF6ZI3RW/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf;/Users/vincentbagilet/Zotero/storage/P2E2NJLX/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf}
}

@article{hsiang_analysis_2015,
  title = {Analysis of Statistical Power Reconciles Drought-Conflict Results in {{Africa}}},
  author = {Hsiang, Solomon M. and Burke, Marshall and Miguel, Edward and Meng, Kyle C. and Cane, Mark A.},
  year = {2015},
  month = dec,
  urldate = {2024-05-16},
  abstract = {Author(s): Hsiang, Solomon M.; Burke, Marshall; Miguel, Edward; Meng, Kyle C.; Cane, Mark A.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/XWQVYAW5/Hsiang et al. - 2015 - Analysis of statistical power reconciles drought-c.pdf}
}

@article{hsiang_temperatures_2010,
  title = {Temperatures and Cyclones Strongly Associated with Economic Production in the {{Caribbean}} and {{Central America}}},
  author = {Hsiang, Solomon M.},
  year = {2010},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {35},
  pages = {15367--15372},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1009510107},
  urldate = {2024-04-29},
  abstract = {Understanding the economic impact of surface temperatures is an important question for both economic development and climate change policy. This study shows that in 28 Caribbean-basin countries, the response of economic output to increased temperatures is structurally similar to the response of labor productivity to high temperatures, a mechanism omitted from economic models of future climate change. This similarity is demonstrated by isolating the direct influence of temperature from that of tropical cyclones, an important correlate. Notably, output losses occurring in nonagricultural production (--2.4\%/+1 {$^\circ$}C) substantially exceed losses occurring in agricultural production (--0.1\%/+1 {$^\circ$}C). Thus, these results suggest that current models of future climate change that focus on agricultural impacts but omit the response of workers to thermal stress may underestimate the global economic costs of climate change.},
  keywords = {Labor,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/HWCV69Z2/Hsiang - 2010 - Temperatures and cyclones strongly associated with.pdf}
}

@misc{hull_instrumental_2022,
  title = {Instrumental {{Variables}}},
  author = {Hull, Peter},
  year = {2022},
  keywords = {IV,Maths},
  file = {/Users/vincentbagilet/Zotero/storage/NDPBP5PU/IV_peter_hull.pdf}
}

@incollection{huntington-klein_chapter_2021,
  title = {Chapter 16 - {{Fixed Effects}} {\textbar} {{The Effect}}},
  booktitle = {The {{Effect}}: {{An Introduction}} to {{Research Design}} and {{Causality}}},
  author = {{Huntington-Klein}, Nick},
  year = {2021},
  urldate = {2025-01-27},
  abstract = {Chapter 16 - Fixed Effects {\textbar} The Effect is a textbook that covers the basics and concepts of research design, especially as applied to causal inference from observational data.},
  file = {/Users/vincentbagilet/Zotero/storage/G989FQAZ/Huntington-Klein - 2021 - Chapter 16 - Fixed Effects  The Effect.pdf;/Users/vincentbagilet/Zotero/storage/F7G3FKP5/ch-FixedEffects.html}
}

@book{huntington-klein_effect_2021,
  title = {The {{Effect}}: {{An Introduction}} to {{Research Design}} and {{Causality}}},
  shorttitle = {The {{Effect}}},
  author = {{Huntington-Klein}, Nick},
  year = {2021},
  month = nov,
  edition = {1},
  publisher = {{Chapman and Hall/CRC}},
  address = {Boca Raton},
  doi = {10.1201/9781003226055},
  urldate = {2022-02-21},
  isbn = {978-1-00-322605-5},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/WC6YEAMY/Huntington-Klein - 2021 - The Effect An Introduction to Research Design and.pdf}
}

@misc{huntington-klein_sources_2025,
  type = {{{SSRN Scholarly Paper}}},
  title = {The {{Sources}} of {{Researcher Variation}} in {{Economics}}},
  author = {{Huntington-Klein}, Nick and P{\"o}rtner, Claus C. and Acharya, Yubraj and Adamkovic, Matus and Adema, Joop and Agasa, Lameck Ondieki and Ahmad, Imtiaz and {Akbulut-Yuksel}, Mevlude and Andresen, Martin Eckhoff and Angenendt, David and Ant{\'o}n, Jos{\'e}-Ignacio and Arenas, Andreu and Aslim, Erkmen Giray and Avdeev, Stanislav and {Bacher-Hicks}, Andrew and Baker, Bradley and Bandara, Imesh Nuwan and Bansal, Avijit and Bartram, David and {Bech-Wysocka}, Katarzyna and Bennett, Christopher T. and Berha, Andu and Berniell, In{\'e}s and Bhai, Moiz and Bhattacharya, Shreya and Bjoerkheim, Markus and Bloem, Jeffrey R. and Brehm, Margaret and Brun, Mart{\'i}n and Buisson, Florent and Burli, Pralhad H. and Camp, Andrew M. and Cerutti, Nicola and Chen, Weiwei and Clement, Jeffrey and Collins, Matthew and Crawfurd, Lee and Cullinan, John and Deer, Lachlan and {Dorsey-Palmateer}, Reid and Duquette, Nicolas and Marino Fages, Diego and Falken, Grace and Farquharson, Christine and Feld, Jan and Feyman, Yevgeniy and Fiala, Nathan and Fitzpatrick, Anne and Fradkin, Andrey and French, Evaewero and Fu, Wei and Fumarco, Luca and Gallegos, Sebastian and Gal{\'a}rraga, Julio and Gamino, Aaron M. and Gauriot, Romain and Gay, Victor and Gayaker, Savas and Gazeaud, Jules and {de Gendre}, Alexandra and Gilpin, Gregory and Girardi, Daniele and Goldhaber, Dan and Harris, Mark N. and Heller, Blake H. and Henderson, Daniel J. and Henningsen, Arne and Henry, Junita and Herman, Cl{\'e}ment and Hern{\ae}s, {\O}ystein and Hill, Andrew and Holzmeister, Felix and Huysmans, Martijn and Imtiaz, M. Saad and Jain, Anil and Jakobsson, Niklas and Kaire, Jos{\'e} and Kameshwara, Kalyan Kumar and Karney, Daniel and Kim, Sie Won and Klotzb{\"u}cher, Valentin and Kronenberg, Christoph and LaFave, Dan and Lang, David and Lee, Ryan and Li{\'e}gey, Maxime and Long, Dede and Marcus, Jan and Mari, Gabriele and McCarthy, Ian M. and {Meinzen-Dick}, Laura and Merkus, Erik and Miller, Klaus M. and Mogge, Lukas and Murad, S. M. Woahid and Najam, Rafiuddin and Naumann, Elias and Nmadu, Job and Ozer, Gorkem Turgut and Paudel, Jayash and Petroulakis, Filippos and Peukert, Christian and Pitk{\"a}nen, Visa and Porcher, Simon and Prakash, Manab and Pua, Andrew Adrian and Pugatch, Todd and Putman, Daniel and Rayamajhee, Veeshan and Ur Rehman, Obeid and Reimao, Maira and Reuter, Anna and Ricks, Michael and Rios-Avila, Fernando and Rodriguez, Abel and Roeckert, Julian and Ropovik, Ivan and Roy, Jayjit and Salamanca, Nicolas and Samahita, Margaret and Samudra, Aparna and Sanogo, Vassiki and Sariyev, Orkhan and Schaak, Henning and Segel, Joel E. and Sievertsen, Hans Henrik and Smet, Mike and Smith, Brock and Sorensen, Lucy and Spantig, Lisa and Szczygielski, Krzysztof and Tagat, Anirudh and Ta{\c s}tan, H{\"u}seyin and Trombetta, Martin and Venkatesan, Madhavi and Vernet, Antoine and Volkov, Eden and Wagner, Gary A. and Wang, Yue and Ward, Zachary and Waters, Tom and Weber, Ellerie and Weinberg, Stephen E. and Wei{\ss}m{\"u}ller, Kristina S. and Westheide, Christian and Williams, Kevin and Ye, Xiaoyang and Yu, Jisang and Zahid, Muhammad Umer and Zanoli, Raffaele},
  year = {2025},
  month = feb,
  number = {5152665},
  eprint = {5152665},
  publisher = {Social Science Research Network},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.5152665},
  urldate = {2025-03-25},
  abstract = {We use a rigorous three-stage many-analysts design to assess how different researcher decisions---specifically data cleaning, research design, and the interpretation of a policy question---affect the variation in estimated treatment effects. A total of 146 research teams each completed the same causal inference task three times each: first with few constraints, then using a shared research design, and finally with pre-cleaned data in addition to a specified design. We find that even when analyzing the same data, teams reach different conclusions. In the first stage, the interquartile range (IQR) of the reported policy effect was 3.1 percentage points, with substantial outliers. Surprisingly, the second stage, which restricted research design choices, exhibited slightly higher IQR (4.0 percentage points), largely attributable to imperfect adherence to the prescribed protocol. By contrast, the final stage, featuring standardized data cleaning, narrowed variation in estimated effects, achieving an IQR of 2.4 percentage points. Reported sample sizes also displayed significant convergence under more restrictive conditions, with the IQR dropping from 295,187 in the first stage to 29,144 in the second, and effectively zero by the third. Our findings underscore the critical importance of data cleaning in shaping applied microeconomic results and highlight avenues for future replication efforts.},
  archiveprefix = {Social Science Research Network},
  langid = {english},
  keywords = {Applied econometrics,Causal inference,Metascience,Research methods},
  file = {/Users/vincentbagilet/Zotero/storage/I2H6ERUG/Huntington-Klein et al. - 2025 - The Sources of Researcher Variation in Economics.pdf}
}

@book{imbens_causal_2015,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  year = {2015},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139025751},
  urldate = {2022-02-22},
  abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
  isbn = {978-0-521-88588-1}
}

@article{imbens_optimal_2012,
  title = {Optimal {{Bandwidth Choice}} for the {{Regression Discontinuity Estimator}}},
  author = {Imbens, Guido and Kalyanaraman, Karthik},
  year = {2012},
  journal = {The Review of Economic Studies},
  volume = {79},
  number = {3},
  eprint = {23261375},
  eprinttype = {jstor},
  pages = {933--959},
  publisher = {[Oxford University Press, Review of Economic Studies, Ltd.]},
  issn = {0034-6527},
  urldate = {2022-01-25},
  abstract = {We investigate the choice of the bandwidth for the regression discontinuity estimator. We focus on estimation by local linear regression, which was shown to have attractive properties (Porter, J. 2003, "Estimation in the Regression Discontinuity Model" (unpublished, Department of Economics, University of Wisconsin, Madison)). We derive the asymptotically optimal bandwidth under squared error loss. This optimal bandwidth depends on unknown functionals of the distribution of the data and we propose simple and consistent estimators for these functionals to obtain a fully data-driven bandwidth algorithm. We show that this bandwidth estimator is optimal according to the criterion of Li (1987, "Asymptotic Optimality for C p , C L , Cross-validation and Generalized Cross-validation: Discrete Index Set", Annals of Statistics, 15, 958--975), although it is not unique in the sense that alternative consistent estimators for the unknown functionals would lead to bandwidth estimators with the same optimality properties. We illustrate the proposed bandwidth, and the sensitivity to the choices made in our algorithm, by applying the methods to a data set previously analysed by Lee (2008, "Randomized Experiments from Non-random Selection in U.S. House Elections", Journal of Econometrics, 142, 675--697) as well as by conducting a small simulation study.},
  keywords = {Optimal bandwith,Precision,RDD},
  file = {/Users/vincentbagilet/Zotero/storage/HMB6DNG8/Imbens and Kalyanaraman - 2012 - Optimal Bandwidth Choice for the Regression Discon.pdf}
}

@article{imbens_statistical_2021,
  title = {Statistical {{Significance}}, {{Values}}, and the {{Reporting}} of {{Uncertainty}}},
  author = {Imbens, Guido W.},
  year = {2021},
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {157--174},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.157},
  urldate = {2021-08-02},
  abstract = {The use of statistical significance and p-values has become a matter of substantial controversy in various fields using statistical methods. This has gone as far as some journals banning the use of indicators for statistical significance, or even any reports of p-values, and, in one case, any mention of confidence intervals. I discuss three of the issues that have led to these often-heated debates. First, I argue that in many cases, p-values and indicators of statistical significance do not answer the questions of primary interest. Such questions typically involve making (recommendations on) decisions under uncertainty. In that case, point estimates and measures of uncertainty in the form of confidence intervals or even better, Bayesian intervals, are often more informative summary statistics. In fact, in that case, the presence or absence of statistical significance is essentially irrelevant, and including them in the discussion may confuse the matter at hand. Second, I argue that there are also cases where testing null hypotheses is a natural goal and where p-values are reasonable and appropriate summary statistics. I conclude that banning them in general is counterproductive. Third, I discuss that the overemphasis in empirical work on statistical significance has led to abuse of p-values in the form of p-hacking and publication bias. The use of pre-analysis plans and replication studies, in combination with lowering the emphasis on statistical significance may help address these problems.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/FPVKDF76/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf;/Users/vincentbagilet/Zotero/storage/JXJMHQVG/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf;/Users/vincentbagilet/Zotero/storage/T8N7FZX4/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf;/Users/vincentbagilet/Zotero/storage/UHE67LM9/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf}
}

@techreport{institute_for_policy_integrity_does_2017,
  title = {Does {{Environmental Regulation Kill}} or {{Create Jobs}}?},
  author = {{Institute for Policy Integrity}},
  year = {2017},
  month = feb,
  keywords = {CBA,Employment,Environment,Environmental Regulation},
  file = {/Users/vincentbagilet/Zotero/storage/S7NPCDJJ/Jobs_and_Regulation_Factsheet.pdf}
}

@article{ioannidis_power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  month = oct,
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  publisher = {Oxford Academic},
  issn = {0013-0133},
  doi = {10.1111/ecoj.12461},
  urldate = {2020-10-09},
  abstract = {Abstract.  We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical econom},
  langid = {english},
  keywords = {Economics,Literature review,Meta-analysis,Power,Statistics},
  file = {/Users/vincentbagilet/Zotero/storage/CJP5KNRI/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf;/Users/vincentbagilet/Zotero/storage/GI6V8NR7/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf;/Users/vincentbagilet/Zotero/storage/GQD48CQ9/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf;/Users/vincentbagilet/Zotero/storage/ZD2VLDAZ/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf}
}

@article{ioannidis_why_2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  year = {2008},
  journal = {Epidemiology},
  volume = {19},
  number = {5},
  eprint = {25662607},
  eprinttype = {jstor},
  pages = {640--648},
  langid = {english},
  keywords = {Inflated effects,Publication bias,To read},
  file = {/Users/vincentbagilet/Zotero/storage/TSNPGPJS/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf;/Users/vincentbagilet/Zotero/storage/U6M3UN2G/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf;/Users/vincentbagilet/Zotero/storage/VJAU9HAA/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf;/Users/vincentbagilet/Zotero/storage/XEI7JJPE/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf}
}

@article{jacob_remedial_2004,
  title = {Remedial {{Education}} and {{Student Achievement}}: {{A Regression-Discontinuity Analysis}}},
  shorttitle = {Remedial {{Education}} and {{Student Achievement}}},
  author = {Jacob, Brian A. and Lefgren, Lars},
  year = {2004},
  month = feb,
  journal = {The Review of Economics and Statistics},
  volume = {86},
  number = {1},
  pages = {226--244},
  issn = {0034-6535},
  doi = {10.1162/003465304323023778},
  urldate = {2021-08-10},
  abstract = {As standards and accountability have become increasingly prominent features of the educational landscape, educators have relied more on remedial programs such as summer school and grade retention to help low-achieving students meet minimum academic standards. Yet the evidence on the effectiveness of such programs is mixed, and prior research suffers from selection bias. However, recent school reform efforts in Chicago provide an opportunity to examine the causal impact of these remedial education programs. In 1996, the Chicago Public Schools instituted an accountability policy that tied summer school and promotional decisions to performance on standardized tests, which resulted in a highly nonlinear relationship between current achievement and the probability of attending summer school or being retained. Using a regression discontinuity design, we find that the net effect of these programs was to substantially increase academic achievement among third-graders, but not sixth-graders. In addition, contrary to conventional wisdom and prior research, we find that retention increases achievement for third-grade students and has little effect on math achievement for sixth-grade students.},
  keywords = {Chicago,Education,Grade retention,RDD,Summer schools,US},
  file = {/Users/vincentbagilet/Zotero/storage/8XCDEVTB/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf;/Users/vincentbagilet/Zotero/storage/A38G3UBJ/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf;/Users/vincentbagilet/Zotero/storage/FKYNJNY2/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf;/Users/vincentbagilet/Zotero/storage/ZRVUJZ34/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf}
}

@article{jiangHave2017,
  title = {Have {{Instrumental Variables Brought Us Closer}} to the {{Truth}}},
  author = {Jiang, Wei},
  year = {2017},
  month = sep,
  journal = {The Review of Corporate Finance Studies},
  volume = {6},
  number = {2},
  pages = {127--140},
  publisher = {Oxford Academic},
  issn = {2046-9128},
  doi = {10.1093/rcfs/cfx015},
  urldate = {2024-03-26},
  abstract = {Abstract. A survey of 255 papers that rely on the instrumental variable (IV) approach for identifying causal effects published in the {\^a}Big Three{\^a} finance jo},
  langid = {english},
  keywords = {Finance,IV,Literature review},
  file = {/Users/vincentbagilet/Zotero/storage/7IZL7P5J/Jiang - 2017 - Have Instrumental Variables Brought Us Closer to t.pdf}
}

@article{kamenica_bayesian_2011,
  title = {Bayesian {{Persuasion}}},
  author = {Kamenica, Emir and Gentzkow, Matthew},
  year = {2011},
  month = oct,
  journal = {American Economic Review},
  volume = {101},
  number = {6},
  pages = {2590--2615},
  issn = {0002-8282},
  doi = {10.1257/aer.101.6.2590},
  urldate = {2021-10-26},
  abstract = {When is it possible for one person to persuade another to change her action? We consider a symmetric information model where a sender chooses a signal to reveal to a receiver, who then takes a noncontractible action that affects the welfare of both players. We derive necessary and sufficient conditions for the existence of a signal that strictly benefits the sender. We characterize sender-optimal signals. We examine comparative statics with respect to the alignment of the sender's and the receiver's preferences. Finally, we apply our results to persuasion by litigators, lobbyists, and salespeople. (JEL D72, D82, D83, K40, M31)},
  langid = {english},
  keywords = {Model},
  file = {/Users/vincentbagilet/Zotero/storage/4C2KHES5/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf;/Users/vincentbagilet/Zotero/storage/8XJ8XGAE/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf;/Users/vincentbagilet/Zotero/storage/W346WDLT/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf;/Users/vincentbagilet/Zotero/storage/WPDK5SNC/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf}
}

@article{kasy_forking_2021,
  title = {Of {{Forking Paths}} and {{Tied Hands}}: {{Selective Publication}} of {{Findings}}, and {{What Economists Should Do}} about {{It}}},
  shorttitle = {Of {{Forking Paths}} and {{Tied Hands}}},
  author = {Kasy, Maximilian},
  year = {2021},
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {175--192},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.175},
  urldate = {2021-08-02},
  abstract = {A key challenge for interpreting published empirical research is the fact that published findings might be selected by researchers or by journals. Selection might be based on criteria such as significance, consistency with theory, or the surprisingness of findings or their plausibility. Selection leads to biased estimates, reduced coverage of confidence intervals, and distorted posterior beliefs. I review methods for detecting and quantifying selection based on the distribution of p-values, systematic replication studies, and meta-studies. I then discuss the conflicting recommendations regarding selection result ing from alternative objectives, in particular, the validity of inference versus the relevance of findings for decision-makers. Based on this discussion, I consider various reform proposals, such as deemphasizing significance, pre-analysis plans, journals for null results and replication studies, and a functionally differentiated publication system. In conclusion, I argue that we need alternative foundations of statistics that go beyond the single-agent model of decision theory.},
  langid = {english},
  keywords = {Forking paths,Publication bias,Statistics},
  file = {/Users/vincentbagilet/Zotero/storage/3ZCKSJ87/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf;/Users/vincentbagilet/Zotero/storage/JXR4TXGH/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf;/Users/vincentbagilet/Zotero/storage/NLZ6XQIB/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf;/Users/vincentbagilet/Zotero/storage/YP4YI6TE/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf}
}

@misc{kim_drawing_2022,
  title = {Drawing a {{Line Between Sample Statistics}} and {{Population Inferences}}},
  author = {Kim, Eddie},
  year = {2022},
  month = sep,
  urldate = {2023-10-18},
  abstract = {A shady figure presents you with a game. They have a deck of cards numbered 1, 2, 3, or 4, the exact distribution of which you do not know. They randomly {\dots}},
  langid = {english}
}

@article{klaauw_estimating_2002,
  title = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}: {{A Regression}}--{{Discontinuity Approach}}*},
  shorttitle = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}},
  author = {Klaauw, Wilbert Van Der},
  year = {2002},
  journal = {International Economic Review},
  volume = {43},
  number = {4},
  pages = {1249--1287},
  issn = {1468-2354},
  doi = {10.1111/1468-2354.t01-1-00055},
  urldate = {2021-08-10},
  abstract = {An important problem faced by colleges and universities, that of evaluating the effect of their financial aid offers on student enrollment decisions, is complicated by the likely endogeneity of the aid offer variable in a student enrollment equation. This article shows how discontinuities in an East Coast college's aid assignment rule can be exploited to obtain credible estimates of the aid effect without having to rely on arbitrary exclusion restrictions and functional form assumptions. Semiparametric estimates based on a regression--discontinuity (RD) approach affirm the importance of financial aid as an effective instrument in competing with other colleges for students.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/DGA7N68X/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf;/Users/vincentbagilet/Zotero/storage/DXGSID8N/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf;/Users/vincentbagilet/Zotero/storage/SL9ZPSEX/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf;/Users/vincentbagilet/Zotero/storage/TAAQRRCI/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf}
}

@article{kraft_interpreting_2020,
  title = {Interpreting {{Effect Sizes}} of {{Education Interventions}}},
  author = {Kraft, Matthew A.},
  year = {2020},
  month = may,
  journal = {Educational Researcher},
  volume = {49},
  number = {4},
  pages = {241--253},
  publisher = {American Educational Research Association},
  issn = {0013-189X},
  doi = {10.3102/0013189X20912798},
  urldate = {2022-02-26},
  abstract = {Researchers commonly interpret effect sizes by applying benchmarks proposed by Jacob Cohen over a half century ago. However, effects that are small by Cohen's standards are large relative to the impacts of most field-based interventions. These benchmarks also fail to consider important differences in study features, program costs, and scalability. In this article, I present five broad guidelines for interpreting effect sizes that are applicable across the social sciences. I then propose a more structured schema with new empirical benchmarks for interpreting a specific class of studies: causal research on education interventions with standardized achievement outcomes. Together, these tools provide a practical approach for incorporating study features, costs, and scalability into the process of interpreting the policy importance of effect sizes.},
  langid = {english},
  keywords = {Education,Effect size,Literature review,Meta-analysis},
  file = {/Users/vincentbagilet/Zotero/storage/2K39JI5T/Kraft - 2020 - Interpreting Effect Sizes of Education Interventio.pdf}
}

@article{lai_effects_2023,
  title = {The {{Effects}} of {{Temperature}} on {{Labor Productivity}}},
  author = {Lai, Wangyang and Qiu, Yun and Tang, Qu and Xi, Chen and Zhang, Peng},
  year = {2023},
  month = oct,
  journal = {Annual Review of Resource Economics},
  volume = {15},
  number = {Volume 15, 2023},
  pages = {213--232},
  publisher = {Annual Reviews},
  issn = {1941-1340, 1941-1359},
  doi = {10.1146/annurev-resource-101222-125630},
  urldate = {2024-06-10},
  abstract = {This article reviews recent economic studies on the causal effects of temperature on labor productivity. The negative effects of extreme temperatures are widespread, and the magnitudes of the impact differ across social and economic factors. In addition to physical outputs, extreme temperatures also impair mental productivity, including cognition and learning. In utero exposure to extreme temperatures has profound effects on human development. Although the literature has detected various adaptation strategies, the conclusions are mixed. We discuss some limitations of existing studies and propose several directions for future research.},
  langid = {english},
  keywords = {Labor,Literature review,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/RZYWBCZ8/Lai et al. - 2023 - The Effects of Temperature on Labor Productivity.pdf;/Users/vincentbagilet/Zotero/storage/UMBHKLMY/annurev-resource-101222-125630.html}
}

@article{lalHow2024,
  title = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}? {{Practical Advice Based}} on 67 {{Replicated Studies}}},
  shorttitle = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}?},
  author = {Lal, Apoorva and Lockhart, Mackenzie and Xu, Yiqing and Zu, Ziwen},
  year = {2024},
  month = may,
  journal = {Political Analysis},
  pages = {1--20},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2024.2},
  urldate = {2024-05-06},
  abstract = {Instrumental variable (IV) strategies are widely used in political science to establish causal relationships, but the identifying assumptions required by an IV design are demanding, and assessing their validity remains challenging. In this paper, we replicate 67 articles published in three top political science journals from 2010 to 2022 and identify several concerning patterns. First, researchers often overestimate the strength of their instruments due to non-i.i.d. error structures such as clustering. Second, IV estimates are often highly uncertain, and the commonly used t-test for two-stage-least-squares (2SLS) estimates frequently underestimate the uncertainties. Third, in most replicated studies, 2SLS estimates are significantly larger in magnitude than ordinary-least-squares estimates, and their absolute ratio is inversely related to the strength of the instrument in observational studies---a pattern not observed in experimental ones---suggesting potential violations of unconfoundedness or the exclusion restriction in the former. We provide a checklist and software to help researchers avoid these pitfalls and improve their practice.},
  langid = {english},
  keywords = {IV,Political Science,Replication,Review},
  file = {/Users/vincentbagilet/Zotero/storage/FVXP54SR/Lal et al. - 2024 - How Much Should We Trust Instrumental Variable Est.pdf}
}

@article{lalonde_evaluating_1986,
  title = {Evaluating the {{Econometric Evaluations}} of {{Training Programs}} with {{Experimental Data}}},
  author = {LaLonde, Robert J.},
  year = {1986},
  journal = {The American Economic Review},
  volume = {76},
  number = {4},
  eprint = {1806062},
  eprinttype = {jstor},
  pages = {604--620},
  publisher = {American Economic Association},
  issn = {0002-8282},
  urldate = {2022-01-13},
  abstract = {This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric procedures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.}
}

@misc{langHow2023,
  type = {Working {{Paper}}},
  title = {How {{Credible}} Is the {{Credibility Revolution}}?},
  author = {Lang, Kevin},
  year = {2023},
  month = sep,
  series = {Working {{Paper Series}}},
  number = {31666},
  eprint = {31666},
  publisher = {National Bureau of Economic Research},
  doi = {10.3386/w31666},
  urldate = {2023-09-11},
  abstract = {When economists analyze a well-conducted RCT or natural experiment and find a statistically significant effect, they conclude the null of no effect is unlikely to be true. But how frequently is this conclusion warranted? The answer depends on the proportion of tested nulls that are true and the power of the tests. I model the distribution of t-statistics in leading economics journals. Using my preferred model, 65\% of narrowly rejected null hypotheses and 41\% of all rejected null hypotheses with {\textbar}t{\textbar}{$<$}10 are likely to be false rejections. For the null to have only a .05 probability of being true requires a t of 5.48.},
  archiveprefix = {National Bureau of Economic Research},
  keywords = {Publication bias,To read},
  file = {/Users/vincentbagilet/Zotero/storage/MJ82HCWM/Lang - 2023 - How Credible is the Credibility Revolution.pdf}
}

@article{lavaine_energy_2017,
  title = {Energy {{Production}} and {{Health Externalities}}: {{Evidence}} from {{Oil Refinery Strikes}} in {{France}}},
  shorttitle = {Energy {{Production}} and {{Health Externalities}}},
  author = {Lavaine, Emmanuelle and Neidell, Mathew},
  year = {2017},
  month = jun,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {4},
  number = {2},
  pages = {447--477},
  publisher = {The University of Chicago Press},
  issn = {2333-5955},
  doi = {10.1086/691554},
  urldate = {2022-02-14},
  abstract = {This paper examines the effect of energy production on health using a recent strike that affected oil refineries in France as a natural experiment. First, we show that the temporary reduction in refining led to a significant reduction in sulfur dioxide (SO2) concentrations. Second, this shock significantly increased birth weight and gestational age of newborns, particularly for those exposed to the strike during the first and third trimesters of pregnancy, and decreased asthma and bronchitis admissions. Back-of-the-envelope calculations suggest that a 1-unit (or 26\%) decline in monthly SO2 leads to an {\texteuro}89 million increase in lifetime earnings per birth-year cohort. This externality from oil refineries should be an important part of policy discussions surrounding the production of energy.},
  keywords = {AHEAP,Air pollution,Brithweight,DiD,France,Health,SO2},
  file = {/Users/vincentbagilet/Zotero/storage/M25UKNLA/Lavaine and Neidell - 2017 - Energy Production and Health Externalities Eviden.pdf}
}

@article{leamer_lets_2021,
  title = {Let's {{Take}} the {{Con Out}} of {{Econometrics}}},
  author = {Leamer, Edward E},
  year = {2021},
  pages = {14},
  langid = {english},
  keywords = {Inflated effects,To read},
  file = {/Users/vincentbagilet/Zotero/storage/56KTMW3G/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf;/Users/vincentbagilet/Zotero/storage/5KN2N38Z/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf;/Users/vincentbagilet/Zotero/storage/DSKNDMX2/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf;/Users/vincentbagilet/Zotero/storage/E9LN3TCU/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf}
}

@article{lee_regression_2010,
  title = {Regression {{Discontinuity Designs}} in {{Economics}}},
  author = {Lee, David S. and Lemieux, Thomas},
  year = {2010},
  journal = {Journal of Economic Literature},
  volume = {48},
  number = {2},
  eprint = {20778728},
  eprinttype = {jstor},
  pages = {281--355},
  publisher = {American Economic Association},
  issn = {0022-0515},
  urldate = {2021-08-27},
  abstract = {This paper provides an introduction and "user guide" to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a "quasi-experimental" design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD.},
  keywords = {Literature review,RDD},
  file = {/Users/vincentbagilet/Zotero/storage/MA5BFJJ6/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf;/Users/vincentbagilet/Zotero/storage/QFVT48EJ/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf;/Users/vincentbagilet/Zotero/storage/VXR9ZRWJ/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf;/Users/vincentbagilet/Zotero/storage/Z7VRRZE9/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf}
}

@techreport{lee_valid_2021,
  type = {Working {{Paper}}},
  title = {Valid T-Ratio {{Inference}} for {{IV}}},
  author = {Lee, David S. and McCrary, Justin and Moreira, Marcelo J. and Porter, Jack R.},
  year = {2021},
  month = aug,
  series = {Working {{Paper Series}}},
  number = {29124},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w29124},
  urldate = {2022-04-12},
  abstract = {In the single-IV model, researchers commonly rely on t-ratio-based inference, even though the literature has quantified its potentially severe large-sample distortions. Building on Stock and Yogo (2005), we introduce the tF critical value function, leading to a standard error adjustment that is a smooth function of the first-stage F-statistic. For one-quarter of specifications in 61 AER papers, corrected standard errors are at least 49 and 136 percent larger than conventional 2SLS standard errors at the 5-percent and 1-percent significance levels, respectively. tF confidence intervals have shorter expected length than those of Anderson and Rubin (1949), whenever both are bounded.},
  file = {/Users/vincentbagilet/Zotero/storage/DFJWGHFN/Lee et al. - 2021 - Valid t-ratio Inference for IV.pdf}
}

@misc{lee_what_2023,
  type = {Working {{Paper}}},
  title = {What to Do When You Can't Use '1.96' {{Confidence Intervals}} for {{IV}}},
  author = {Lee, David S. and McCrary, Justin and Moreira, Marcelo J. and Porter, Jack R. and Yap, Luther},
  year = {2023},
  month = nov,
  series = {Working {{Paper Series}}},
  number = {31893},
  eprint = {31893},
  publisher = {National Bureau of Economic Research},
  doi = {10.3386/w31893},
  urldate = {2023-11-28},
  abstract = {To address the well-established large-sample invalidity of the +/-1.96 critical values for the t-ratio in the single variable just-identified IV model, applied research typically qualifies the inference based on the first-stage-F (Staiger and Stock (1997) and Stock and Yogo (2005)). We fully extend this F-based approach to its logical conclusion by presenting new critical values for the t-ratio to additionally accommodate values of F that do not meet existing thresholds needed for validity. These new t-ratio critical values simultaneously fix the main problem of over-rejection (invalidity) and the under-appreciated possibility of under-rejection (conservativeness) that can occur when relying solely on the usual 1.96 critical value. We show that the corresponding new confidence intervals are generally expected to be substantially shorter than competing ``robust to weak instrument'' intervals, including those from the recommended benchmark of Anderson and Rubin (1949) (AR). In a sample of 89 specifications from 10 recent empirical studies drawn from five general interest journals, the new ``VtF'' intervals are shorter than AR intervals 100 percent of the time, and even more likely to produce statistically significant results than the usual +/-1.96 procedure.},
  archiveprefix = {National Bureau of Economic Research},
  file = {/Users/vincentbagilet/Zotero/storage/9K9GJMJT/Lee et al. - 2023 - What to do when you can't use '1.96' Confidence In.pdf}
}

@misc{linden_retrodesign_2019,
  title = {{{RETRODESIGN}}: {{Stata}} Module to Compute Type-{{S}} ({{Sign}}) and Type-{{M}} ({{Magnitude}}) Errors},
  shorttitle = {{{RETRODESIGN}}},
  author = {Linden, Ariel},
  year = {2019},
  month = oct,
  journal = {Statistical Software Components},
  urldate = {2022-02-22},
  abstract = {retrodesign computes power, type-S, and type-M errors for one or more specified effect sizes. A type-S (sign) error indicates the probability of an effect size estimate being in the wrong direction, and a type-M (magnitude) error indicates the factor by which the magnitude of an effect might be overestimated -- given that the test statistic is statistically significant (Gelman and Carlin 2014). Gelman and Carlin (2014) propose computing the type-M error using the Student's t distribution while Lu, Qiu, and Deng (2019) propose a closed form solution for computing the type-M error. Both methods are implemented in retrodesign. retrodesign produces results identical to those computed in the retrodesign package for R.},
  howpublished = {Boston College Department of Economics},
  langid = {english},
  keywords = {design calculation,power analysis,replication crisis,Stata,statistical significance,type M error,type S error}
}

@misc{lobell_good_nodate,
  title = {The Good and Bad of Fixed Effects},
  author = {Lobell, David},
  urldate = {2023-05-04},
  abstract = {If you ever want to scare an economist, the two words "omitted variable" will usually do the trick. I was not trained in an economics depa...},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/XJJAFVNC/Lobell - The good and bad of fixed effects.pdf}
}

@article{lopalo_temperature_2023,
  title = {Temperature, {{Worker Productivity}}, and {{Adaptation}}: {{Evidence}} from {{Survey Data Production}}},
  shorttitle = {Temperature, {{Worker Productivity}}, and {{Adaptation}}},
  author = {LoPalo, Melissa},
  year = {2023},
  month = jan,
  journal = {American Economic Journal: Applied Economics},
  volume = {15},
  number = {1},
  pages = {192--229},
  issn = {1945-7782},
  doi = {10.1257/app.20200547},
  urldate = {2024-04-10},
  abstract = {This paper estimates the impact of daily weather on worker productivity by using household survey data to study interviewers. Using data from over 9,000 Demographic and Health Surveys interviewers in 46 countries, I find that interviewers complete 13.6 percent fewer interviews per hour on the hottest and most humid days. Workers maintain the same total output by starting earlier in the day and spending more time on each interview at the expense of spending more hours in the field with the same total pay. In addition, interviewers become differentially less productive on tasks that are less easily monitored.},
  langid = {english},
  keywords = {DHS,Interviewer,Labor,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/6K9V3QBI/17861.pdf;/Users/vincentbagilet/Zotero/storage/ZVPMCP3B/LoPalo - 2023 - Temperature, Worker Productivity, and Adaptation .pdf}
}

@article{lu_note_2019,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  year = {2019},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {72},
  number = {1},
  pages = {1--17},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12132},
  urldate = {2021-06-03},
  abstract = {Motivated by the recent replication and reproducibility crisis, Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641) advocated focusing on controlling for Type S/M errors, instead of the classic Type I/II errors, when conducting hypothesis testing. In this paper, we aim to fill several theoretical gaps in the methodology proposed by Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641). In particular, we derive the closed-form expression for the expected Type M error, and study the mathematical properties of the probability of Type S error as well as the expected Type M error, such as monotonicity. We demonstrate the advantages of our results through numerical and empirical examples.},
  copyright = {{\copyright} 2018 The British Psychological Society},
  langid = {english},
  keywords = {Maths,Power,Type M/S error},
  file = {/Users/vincentbagilet/Zotero/storage/3FMY695Y/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Zotero/storage/8AJLYKCW/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Zotero/storage/NU6J9YSV/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Zotero/storage/R63NKECQ/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Zotero/storage/ULCHMHF4/Peña et al. - 2019 - A novel imputation method for missing values in ai.pdf}
}

@article{maniadis_one_2014,
  title = {One {{Swallow Doesn}}'t {{Make}} a {{Summer}}: {{New Evidence}} on {{Anchoring Effects}}},
  shorttitle = {One {{Swallow Doesn}}'t {{Make}} a {{Summer}}},
  author = {Maniadis, Zacharias and Tufano, Fabio and List, John A.},
  year = {2014},
  month = jan,
  journal = {American Economic Review},
  volume = {104},
  number = {1},
  pages = {277--290},
  issn = {0002-8282},
  doi = {10.1257/aer.104.1.277},
  urldate = {2021-11-11},
  abstract = {Some researchers have argued that anchoring in economic valuations casts doubt on the assumption of consistent and stable preferences. We present new evidence that explores the strength of certain anchoring results. We then present a theoretical framework that provides insights into why we should be cautious of initial empirical findings in general. The model importantly highlights that the rate of false positives depends not only on the observed significance level, but also on statistical power, research priors, and the number of scholars exploring the question. Importantly, a few independent replications dramatically increase the chances that the original finding is true.},
  langid = {english},
  keywords = {Consumer Economics: Empirical Analysis,Design of Experiments: Laboratory,Example,Individual},
  file = {/Users/vincentbagilet/Zotero/storage/5YSHICAV/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf;/Users/vincentbagilet/Zotero/storage/FYHIGTDS/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf;/Users/vincentbagilet/Zotero/storage/IBL2539V/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf;/Users/vincentbagilet/Zotero/storage/S5H8VQXD/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf}
}

@article{mccloskeyStandardErrorRegressions1996,
  title = {The {{Standard Error}} of {{Regressions}}},
  author = {McCloskey, Deirdre N. and Ziliak, Stephen T.},
  year = {1996},
  journal = {Journal of Economic Literature},
  volume = {34},
  number = {1},
  eprint = {2729411},
  eprinttype = {jstor},
  pages = {97--114},
  publisher = {American Economic Association},
  issn = {0022-0515},
  urldate = {2024-01-11},
  file = {/Users/vincentbagilet/Zotero/storage/CAK7A5PR/McCloskey and Ziliak - 1996 - The Standard Error of Regressions.pdf}
}

@techreport{mcconnellGoingSimpleSample2015,
  type = {Working {{Paper}}},
  title = {Going beyond Simple Sample Size Calculations: {{A}} Practitioner's Guide},
  shorttitle = {Going beyond Simple Sample Size Calculations},
  author = {McConnell, Brendon and {Vera-Hern{\'a}ndez}, Marcos},
  year = {2015},
  number = {W15/17},
  institution = {IFS Working Papers},
  doi = {10.1920/wp.ifs.2015.1517},
  urldate = {2024-04-11},
  abstract = {Basic methods to compute the required sample size are well understood and supported by widely available software. However, the sophistication of the sample size methods commonly used has not kept pace with the complexity of the experimental designs most often employed in practice. In this paper, we compile available methods for sample size calculations for continuous and binary outcomes with and without covariates, for both clustered and non-clustered RCTs. Formulae for panel data and for unbalanced designs (where there are different numbers of treatment and control observations) are also provided. The paper includes three extensions: (1) methods to optimize the sample when costs constraints are binding; (2) simulation methods to compute the power of a complex design; and (3) methods to consider in the sample size calculation adjustments for multiple testing. The paper is provided together with spreadsheets and STATA code to implement the methods discussed.},
  copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/6UX3V77Y/McConnell and Vera-Hernández - 2015 - Going beyond simple sample size calculations A pr.pdf}
}

@misc{mckenzieSeven2023,
  title = {Seven Ways to Improve Statistical Power in Your Experiment without Increasing n},
  author = {McKenzie, David},
  year = {2023},
  month = jul,
  urldate = {2023-09-22},
  abstract = {The number of experimental units n is often limited due to budget constraints, to capacity constraints of the implementing organization, or naturally limited by the number of villages eligible for a program or number of people or firms that apply. Here are some of my main thoughts/tips on approaches to try in order to improve statistical power.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/ZUL5Y6C7/David McKenzie - 2023 - Seven ways to improve statistical power in your ex.pdf}
}

@article{mcshane_abandon_2019,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1527253},
  urldate = {2022-02-21},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm---and the p-value thresholds intrinsic to it---as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to ``ban'' p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/942AURU9/McShane et al. - 2019 - Abandon Statistical Significance.pdf}
}

@article{mellon_rain_2021,
  title = {Rain, {{Rain}}, {{Go Away}}: 176 {{Potential Exclusion-Restriction Violations}} for {{Studies Using Weather}} as an {{Instrumental Variable}}},
  author = {Mellon, Jonathan},
  year = {2021},
  month = jul,
  pages = {112},
  abstract = {Instrumental variable (IV) analysis assumes that the instrument only affects the dependent variable via its relationship with the independent variable. Other possible causal routes from the IV to the dependent variable are exclusion-restriction violations and make the instrument invalid. Weather has been widely used as an instrumental variable in social science to predict many different variables. The use of weather to instrument different independent variables represents strong prima facie evidence of exclusion violations for all studies using weather as an IV. A review of 279 studies reveals 176 variables which have been linked to weather: all of which represent potential exclusion violations. I show that the magnitude of several of these violations is sufficient overturn many existing IV results. I conclude with practical steps to systematically review existing literature to identify possible exclusion violations when using IV designs. I demonstrate how sensitivity analysis can quantify the vulnerability of a particular IV estimate to exclusion restriction violations in the literature.},
  langid = {english},
  keywords = {Critique,IV,Rainfall},
  file = {/Users/vincentbagilet/Zotero/storage/3SJLBI8H/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf;/Users/vincentbagilet/Zotero/storage/77YXQCML/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf;/Users/vincentbagilet/Zotero/storage/ABLCMAFZ/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf;/Users/vincentbagilet/Zotero/storage/V36B46KA/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf}
}

@article{middleton_bias_2016,
  title = {Bias {{Amplification}} and {{Bias Unmasking}}},
  author = {Middleton, Joel A. and Scott, Marc A. and Diakow, Ronli and Hill, Jennifer L.},
  year = {2016},
  journal = {Political Analysis},
  volume = {24},
  number = {3},
  pages = {307--323},
  publisher = {Cambridge University Press},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpw015},
  urldate = {2022-04-14},
  abstract = {In the analysis of causal effects in non-experimental studies, conditioning on observable covariates is one way to try to reduce unobserved confounder bias. However, a developing literature has shown that conditioning on certain covariates may increase bias, and the mechanisms underlying this phenomenon have not been fully explored. We add to the literature on bias-increasing covariates by first introducing a way to decompose omitted variable bias into three constituent parts: bias due to an unobserved confounder, bias due to excluding observed covariates, and bias due to amplification. This leads to two important findings. Although instruments have been the primary focus of the bias amplification literature to date, we identify the fact that the popular approach of adding group fixed effects can lead to bias amplification as well. This is an important finding because many practitioners think that fixed effects are a convenient way to account for any and all group-level confounding and are at worst harmless. The second finding introduces the concept of bias unmasking and shows how it can be even more insidious than bias amplification in some cases. After introducing these new results analytically, we use constructed observational placebo studies to illustrate bias amplification and bias unmasking with real data. Finally, we propose a way to add bias decomposition information to graphical displays for sensitivity analysis to help practitioners think through the potential for bias amplification and bias unmasking in actual applications.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/L3TXT892/Middleton et al. - 2016 - Bias Amplification and Bias Unmasking.pdf}
}

@article{millimet_fixed_2023,
  title = {Fixed {{Effects}} and {{Causal Inference}}},
  author = {Millimet, Daniel and Bellemare, Marc F.},
  year = {2023},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4467963},
  urldate = {2023-09-11},
  abstract = {Fixed Effects and Causal Inference* Across many disciplines, the fixed effects estimator of linear panel data models is the default method to estimate causal effects with nonexperimental data that are not confounded by time-invariant, unit-specific heterogeneity. One feature of the fixed effects estimator, however, is often overlooked in practice: With data over time t {$\in$} \{1,...,T\} for each unit of observation i {$\in$} \{1,...,N\}, the amount of unobserved heterogeneity the researcher can remove with unit fixed effects is weakly decreasing in T. Put differently, the set of attributes that are time-invariant is not invariant to the length of the panel. We consider several alternatives to the fixed effects estimator with T {$>$} 2 when relevant unit-specific heterogeneity is not timeinvariant, including existing estimators such as the first-difference, twice first-differenced, and interactive fixed effects estimators. We also introduce several novel algorithms based on rolling estimators. In the situations considered here, there is little to be gained and much to lose by using the fixed effects estimator. We recommend reporting the results from multiple linear panel data estimators in applied research.},
  langid = {english},
  keywords = {Fixed effects,To read},
  file = {/Users/vincentbagilet/Zotero/storage/P9YTZW8K/Millimet and Bellemare - 2023 - Fixed Effects and Causal Inference.pdf}
}

@book{miratrix_designing_2023,
  title = {Designing {{Monte Carlo Simulations}} in {{R}}},
  author = {Miratrix, Luke W. and Pustejovsky, James E.},
  year = {2023},
  month = may,
  urldate = {2023-10-18},
  abstract = {A text on designing, implementing, and reporting on Monte Carlo simulation studies},
  keywords = {Handbook,Simulations}
}

@article{morgan_counterfactuals_nodate,
  title = {Counterfactuals and {{Causal Inference}}},
  author = {Morgan, Stephen L and Winship, Christopher},
  pages = {526},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/6F7YMPU8/Morgan and Winship - Counterfactuals and Causal Inference.pdf}
}

@article{morris_causal_2022,
  title = {Causal Analyses of Existing Databases: The Importance of Understanding What Can Be Achieved with Your Data before Analysis (Commentary on {{Hern{\'a}n}})},
  shorttitle = {Causal Analyses of Existing Databases},
  author = {Morris, Tim P. and van Smeden, Maarten},
  year = {2022},
  month = feb,
  journal = {Journal of Clinical Epidemiology},
  volume = {142},
  pages = {261--263},
  publisher = {Elsevier},
  issn = {0895-4356, 1878-5921},
  doi = {10.1016/j.jclinepi.2021.09.026},
  urldate = {2022-05-17},
  langid = {english},
  pmid = {34560253},
  keywords = {Causal analysis,Meta-analysis,Planning,Power,Sample size},
  file = {/Users/vincentbagilet/Zotero/storage/ZACFZFNW/Morris and Smeden - 2022 - Causal analyses of existing databases the importa.pdf}
}

@article{morris_using_2019,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  year = {2019},
  month = may,
  journal = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074--2102},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.8086},
  urldate = {2021-01-29},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/3RJSKHJT/Morris et al. - 2019 - Using simulation studies to evaluate statistical m.pdf}
}

@misc{naqvi_difference--difference_2022,
  title = {Difference-in-{{Difference}} ({{DiD}})},
  author = {Naqvi, Asjad},
  year = {2022},
  journal = {DiD},
  urldate = {2022-05-19},
  abstract = {Welcome to the DiD revolution.},
  howpublished = {https://asjadnaqvi.github.io/DiD/},
  langid = {american}
}

@misc{noauthor_10_nodate,
  title = {10 {{Things}} to {{Know About Cluster Randomization}} -- {{EGAP}}},
  urldate = {2021-12-14},
  howpublished = {https://egap.org/resource/10-things-to-know-about-cluster-randomization/},
  keywords = {Clustering,Overall Problems}
}

@misc{noauthor_201114999_nodate,
  title = {[2011.14999] {{An Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  urldate = {2022-01-12},
  howpublished = {https://arxiv.org/abs/2011.14999}
}

@misc{noauthor_210914526_nodate,
  title = {[2109.14526] {{On}} the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  urldate = {2022-01-19},
  howpublished = {https://arxiv.org/abs/2109.14526}
}

@misc{noauthor_area-proportional_nodate,
  title = {Area-{{Proportional Euler}} and {{Venn Diagrams}} with {{Ellipses}}},
  urldate = {2022-10-20},
  abstract = {Generate area-proportional Euler diagrams     using numerical optimization. An Euler diagram is a generalization of a Venn     diagram, relaxing the criterion that all interactions need to be     represented. Diagrams may be fit with ellipses and circles via     a wide range of inputs and can be visualized in numerous ways.},
  howpublished = {https://jolars.github.io/eulerr/index.html},
  langid = {english}
}

@misc{noauthor_causal_nodate,
  title = {Causal {{Effects}} in {{Nonexperimental Studies}}: {{Reevaluating}} the {{Evaluation}} of {{Training Programs}}: {{Journal}} of the {{American Statistical Association}}: {{Vol}} 94, {{No}} 448},
  urldate = {2022-01-13},
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858}
}

@misc{noauthor_mostly_nodate,
  title = {Mostly {{Harmless Econometrics}} -- {{An Empiricist}}`s {{Companion}} : {{Angrist}}, {{Joshua D}}., {{Pischke}}, {{Jorn}}--Steffen, {{Pischke}}, {{J{\~A}}}{\P}rn--Steffen: {{Amazon}}.Fr: {{Livres}}},
  urldate = {2022-02-21},
  howpublished = {https://www.amazon.fr/Mostly-Harmless-Econometrics-Empiricist\%60s-Companion/dp/0691120358},
  file = {/Users/vincentbagilet/Zotero/storage/476ZJ84K/0691120358.html}
}

@misc{noauthor_moving_nodate,
  title = {Moving beyond the Classic Difference-in-Differences Model: A Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies {\textbar} {{BMC Medical Research Methodology}} {\textbar} {{Full Text}}},
  urldate = {2022-01-12},
  howpublished = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01471-y}
}

@book{noauthor_notitle_nodate,
  type = {Book}
}

@misc{noauthor_power_nodate,
  title = {Power Calculations},
  journal = {The Abdul Latif Jameel Poverty Action Lab (J-PAL)},
  urldate = {2023-10-05},
  abstract = {This section is intended to provide an intuitive discussion of the rationale behind power calculations, as well as practical tips and sample code for conducting power calculations using either built-in commands or simulation. It assumes some knowledge of statistics and hypothesis testing. Readers interested in more technical discussions may refer to the links at the bottom of the page, while those looking for sample code for conducting power calculations in Stata and R may refer to our GitHub page~(also linked in the "Sample code" section below)~and those already familliar with the intuition and technical aspects may refer to our Quick guide to power calculations.},
  howpublished = {https://www.povertyactionlab.org/resource/power-calculations},
  langid = {english},
  keywords = {MDE,Tutorial}
}

@article{olkenPromises2015,
  title = {Promises and {{Perils}} of {{Pre-analysis Plans}}},
  author = {Olken, Benjamin A.},
  year = {2015},
  month = sep,
  journal = {Journal of Economic Perspectives},
  volume = {29},
  number = {3},
  pages = {61--80},
  issn = {0895-3309},
  doi = {10.1257/jep.29.3.61},
  urldate = {2024-05-11},
  abstract = {The purpose of this paper is to help think through the advantages and costs of rigorous pre-specification of statistical analysis plans in economics.  A pre-analysis plan pre-specifies in a precise way the analysis to be run before examining the data.  A researcher can specify variables, data cleaning procedures, regression specifications, and so on.  If the regressions are pre-specified in advance and researchers are required to report all the results they pre-specify, data-mining problems are greatly reduced.  I begin by laying out the basics of what a statistical analysis plan actually contains so those researchers unfamiliar with it can better understand how it is done.  In so doing, I have drawn both on standards used in clinical trials, which are clearly specified by the Food and Drug Administration, as well as my own practical experience from writing these plans in economics contexts.  I then lay out some of the advantages of pre-specified analysis plans, both for the scientific community as a whole and also for the researcher.  I also explore some of the limitations and costs of such plans.  I then review a few pieces of evidence that suggest that, in many contexts, the benefits of using pre-specified analysis plans may not be as high as one might have expected initially.  For the most part, I will focus on the relatively narrow issue of pre-analysis for randomized controlled trials.},
  langid = {english},
  keywords = {Pre-analysis plan},
  file = {/Users/vincentbagilet/Zotero/storage/REUD8JC4/Olken - 2015 - Promises and Perils of Pre-analysis Plans.pdf}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aac4716},
  urldate = {2021-12-08},
  keywords = {Experiments,Power,Psychology,Replications},
  file = {/Users/vincentbagilet/Zotero/storage/4A5NV8BX/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf;/Users/vincentbagilet/Zotero/storage/7CQLXBNK/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf;/Users/vincentbagilet/Zotero/storage/UUCD7DC7/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf;/Users/vincentbagilet/Zotero/storage/VK2K5AUX/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{oster_unobservable_2019,
  title = {Unobservable {{Selection}} and {{Coefficient Stability}}: {{Theory}} and {{Evidence}}},
  shorttitle = {Unobservable {{Selection}} and {{Coefficient Stability}}},
  author = {Oster, Emily},
  year = {2019},
  month = apr,
  journal = {Journal of Business \& Economic Statistics},
  volume = {37},
  number = {2},
  pages = {187--204},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2016.1227711},
  urldate = {2022-02-01},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/2ME2TBXZ/Oster - 2019 - Unobservable Selection and Coefficient Stability .pdf}
}

@article{park_heat_2020,
  title = {Heat and {{Learning}}},
  author = {Park, R. Jisung and Goodman, Joshua and Hurwitz, Michael and Smith, Jonathan},
  year = {2020},
  month = may,
  journal = {American Economic Journal: Economic Policy},
  volume = {12},
  number = {2},
  pages = {306--339},
  issn = {1945-7731},
  doi = {10.1257/pol.20180612},
  urldate = {2024-07-29},
  abstract = {We demonstrate that heat inhibits learning and that school air conditioning may mitigate this effect. Student fixed effects models using 10 million students who retook the PSATs show that hotter school days in the years before the test was taken reduce scores, with extreme heat being particularly damaging. Weekend and summer temperatures have little impact, suggesting heat directly disrupts learning time. New nationwide, school-level measures of air conditioning penetration suggest patterns consistent with such infrastructure largely offsetting heat's effects. Without air conditioning, a 1{$^\circ$}F hotter school year reduces that year's learning by 1 percent. Hot school days disproportionately impact minority students, accounting for roughly 5 percent of the racial achievement gap.},
  langid = {english},
  keywords = {Analysis of Education Education and Inequality Economics of Minorities Races Indigenous Peoples and Immigrants,Global Warming,Natural Disasters and Their Management,Non-labor Discrimination Climate}
}

@article{park_hot_2020,
  title = {Hot {{Temperature}} and {{High Stakes Performance}}},
  author = {Park, R. Jisung},
  year = {2020},
  month = mar,
  journal = {Journal of Human Resources},
  publisher = {University of Wisconsin Press},
  issn = {0022-166X, 1548-8004},
  doi = {10.3368/jhr.57.2.0618-9535R3},
  urldate = {2024-06-12},
  abstract = {Despite the prevalence of high stakes assessments -- and the growing likelihood of heat exposure during such assessments -- the effect of temperature on performance has not yet been studied in such settings. Using student-level administrative data for the largest public school district in the United States, I provide the first estimates of temperature's impact on high-stakes exam performance and subsequent educational attainment. Hot temperature reduces performance by up to 13 percent of a standard deviation and leads to persistent impacts on high school graduation status, despite compensatory responses by teachers who selectively upward manipulate grades after hotter exams.},
  chapter = {Article},
  copyright = {{\copyright} 2020 by the Board of Regents of the University of Wisconsin System},
  langid = {english},
  keywords = {climate adaptation,human capital,I21,I26,J24,O18,Q51,Q54,Q56,temperature},
  file = {/Users/vincentbagilet/Zotero/storage/Q4RB5YH7/Park - 2020 - Hot Temperature and High Stakes Performance.pdf}
}

@article{peng_frisch_2021,
  title = {The {{Frisch}}--{{Waugh}}--{{Lovell}} Theorem for Standard Errors},
  author = {Peng, Ding},
  year = {2021},
  month = jan,
  journal = {Statistics \& Probability Letters},
  volume = {168},
  pages = {108945},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2020.108945},
  urldate = {2025-01-29},
  abstract = {The Frisch--Waugh--Lovell Theorem states the equivalence of the coefficients from the full and partial regressions. I further show the equivalence between various standard errors. Applying the new result to stratified experiments reveals the discrepancy between model-based and design-based standard errors.},
  keywords = {Autocorrelation,Clustering,Covariance estimator,FWL,Heteroskedasticity,Maths,Partial regression,Projection,Stratified experiments,Variance},
  file = {/Users/vincentbagilet/Zotero/storage/8PXRVF8A/Ding - 2021 - The Frisch–Waugh–Lovell theorem for standard errors.pdf;/Users/vincentbagilet/Zotero/storage/KBAP77ZH/S0167715220302480.html}
}

@article{peng_model_2006,
  title = {Model {{Choice}} in {{Time Series Studies}} of {{Air Pollution}} and {{Mortality}}},
  author = {Peng, Roger D. and Dominici, Francesca and Louis, Thomas A.},
  year = {2006},
  month = mar,
  journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
  volume = {169},
  number = {2},
  pages = {179--203},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/j.1467-985X.2006.00410.x},
  urldate = {2023-11-14},
  abstract = {Multicity time series studies of particulate matter and mortality and morbidity have provided evidence that daily variation in air pollution levels is associated with daily variation in mortality counts. These findings served as key epidemiological evidence for the recent review of the US national ambient air quality standards for particulate matter. As a result, methodological issues concerning time series analysis of the relationship between air pollution and health have attracted the attention of the scientific community and critics have raised concerns about the adequacy of current model formulations. Time series data on pollution and mortality are generally analysed by using log-linear, Poisson regression models for overdispersed counts with the daily number of deaths as outcome, the (possibly lagged) daily level of pollution as a linear predictor and smooth functions of weather variables and calendar time used to adjust for timevarying confounders. Investigators around the world have used different approaches to adjust for confounding, making it difficult to compare results across studies. To date, the statistical properties of these different approaches have not been comprehensively compared. To address these issues, we quantify and characterize model uncertainty and model choice in adjusting for seasonal and long-term trends in time series models of air pollution and mortality. First, we conduct a simulation study to compare and describe the properties of statistical methods that are commonly used for confounding adjustment. We generate data under several confounding scenarios and systematically compare the performance of the various methods with respect to the mean-squared error of the estimated air pollution coefficient. We find that the bias in the estimates generally decreases with more aggressive smoothing and that model selection methods which optimize prediction may not be suitable for obtaining an estimate with small bias. Second, we apply and compare the modelling approaches with the National Morbidity, Mortality, and Air Pollution Study database which comprises daily time series of several pollutants, weather variables and mortality counts covering the period 1987--2000 for the largest 100 cities in the USA. When applying these approaches to adjusting for seasonal and long-term trends we find that the Study's estimates for the national average effect of PM10 at lag 1 on mortality vary over approximately a twofold range, with 95\% posterior intervals always excluding zero risk.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/4BHD99LD/Peng et al. - 2006 - Model Choice in Time Series Studies of Air Polluti.pdf}
}

@book{peng_statistical_2008,
  title = {Statistical {{Methods}} for {{Environmental Epidemiology}} with {{R}}: {{A Case Study}} in {{Air Pollution}} and {{Health}}},
  shorttitle = {Statistical {{Methods}} for {{Environmental Epidemiology}} with {{R}}},
  author = {Peng, Roger D. and Dominici, Francesca},
  year = {2008},
  month = jul,
  edition = {2008th edition},
  publisher = {Springer},
  address = {New York ; London},
  abstract = {As an area of statistical application, environmental epidemiology and more speci cally, the estimation of health risk associated with the exposure to - vironmental agents, has led to the development of several statistical methods and software that can then be applied to other scienti c areas. The stat- tical analyses aimed at addressing questions in environmental epidemiology have the following characteristics. Often the signal-to-noise ratio in the data is low and the targets of inference are inherently small risks. These constraints typically lead to the development and use of more sophisticated (and pot- tially less transparent) statistical models and the integration of large hi- dimensional databases. New technologies and the widespread availability of powerful computing are also adding to the complexities of scienti c inves- gation by allowing researchers to t large numbers of models and search over many sets of variables. As the number of variables measured increases, so do the degrees of freedom for in uencing the association between a risk factor and an outcome of interest. We have written this book, in part, to describe our experiences developing and applying statistical methods for the estimation for air pollution health e ects. Our experience has convinced us that the application of modern s- tistical methodology in a reproducible manner can bring to bear subst- tial bene ts to policy-makers and scientists in this area. We believe that the methods described in this book are applicable to other areas of environmental epidemiology, particularly those areas involving spatial\{temporal exposures.\vphantom\}},
  isbn = {978-0-387-78166-2},
  langid = {english}
}

@article{qiu_adaptation_2022,
  title = {Adaptation and the Distributional Effects of Heat: {{Evidence}} from Professional Archery Competitions},
  shorttitle = {Adaptation and the Distributional Effects of Heat},
  author = {Qiu, Yun and Zhao, Jinhua},
  year = {2022},
  journal = {Southern Economic Journal},
  volume = {88},
  number = {3},
  pages = {1149--1177},
  issn = {2325-8012},
  doi = {10.1002/soej.12553},
  urldate = {2024-06-11},
  abstract = {We examine the distributional effects of high temperature on worker performance and the effectiveness of adaptation when capital investments such as air conditioning are not feasible. Using a longitudinal data set of 3196 professional archers in 57 competitions during 2010--2016 in China, which includes accurate performance measures at the individual-by-contest level, we show that heat causes more uneven performance distributions by hurting the bottom performers more than the top performers. More frequent heat reduces average performance and raises distributional inequality more than increases in average temperature. Gaining experience and long-term acclimatization together can mitigate 70\% of the heat impacts, demonstrating the potential of adaptation.},
  copyright = {{\copyright} 2021 The Southern Economic Association.},
  langid = {english},
  keywords = {adaptation,climate change,distributional effects,extreme heat},
  file = {/Users/vincentbagilet/Zotero/storage/4AG9F7TK/Qiu and Zhao - 2022 - Adaptation and the distributional effects of heat.pdf;/Users/vincentbagilet/Zotero/storage/HQLJYG5E/soej.html}
}

@techreport{ravallion_should_2020,
  type = {Working {{Paper}}},
  title = {Should the {{Randomistas}} ({{Continue}} to) {{Rule}}?},
  author = {Ravallion, Martin},
  year = {2020},
  month = jul,
  series = {Working {{Paper Series}}},
  number = {27554},
  institution = {National Bureau of Economic Research},
  doi = {10.3386/w27554},
  urldate = {2022-01-27},
  abstract = {The rising popularity of randomized controlled trials (RCTs) in development applications has come with continuing debates about the merits of this approach. The paper takes stock of the issues. It argues that an unconditional preference for RCTs is questionable on three main counts. First, the case for such a preference is unclear on a priori grounds. For example, with a given budget, even a biased observational study can come closer to the truth than a costly RCT. Second, the ethical objections to RCTs have not been properly addressed by advocates. Third, there is a risk of distorting the evidence-base for informing policymaking, given that an insistence on RCTs generates selection bias in what gets evaluated. Going forward, pressing knowledge gaps should drive the questions asked and how they are answered, not the methodological preferences of some researchers. The gold standard is the best method for the question at hand.},
  file = {/Users/vincentbagilet/Zotero/storage/6CQEID8D/Ravallion - 2020 - Should the Randomistas (Continue to) Rule.pdf}
}

@article{rokicki_inference_2018,
  title = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}: {{A Review}}, {{Simulation Study}}, and {{Empirical Application Using SHARE Data}}},
  shorttitle = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}},
  author = {Rokicki, Slawa and Cohen, Jessica and Fink, G{\"u}nther and Salomon, Joshua A. and Landrum, Mary Beth},
  year = {2018},
  month = jan,
  journal = {Medical Care},
  volume = {56},
  number = {1},
  pages = {97--105},
  issn = {1537-1948},
  doi = {10.1097/MLR.0000000000000830},
  abstract = {BACKGROUND: Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. METHODS: First, we review the most commonly used modeling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing, and Retirement in Europe. RESULTS: When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE, and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. CONCLUSIONS: In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.},
  langid = {english},
  pmid = {29112050},
  keywords = {DID},
  file = {/Users/vincentbagilet/Zotero/storage/47DBK8JV/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf;/Users/vincentbagilet/Zotero/storage/5NU5BD9G/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf;/Users/vincentbagilet/Zotero/storage/9U8NEN9N/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf;/Users/vincentbagilet/Zotero/storage/DH63Q2FT/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf}
}

@article{romer_praise_2020,
  title = {In {{Praise}} of {{Confidence Intervals}}},
  author = {Romer, David},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {55--60},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.20201059},
  urldate = {2022-02-21},
  abstract = {Most empirical papers in economics focus on two aspects of their results: whether the estimates are statistically significantly different from zero and the interpretation of the point estimates. This focus obscures important information about the implications of the results for economically interesting hypotheses about values of the parameters other than zero, and in some cases, about the strength of the evidence against values of zero. This limitation can be overcome by reporting confidence intervals for papers' main estimates and discussing their economic interpretation.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/JI2AAIPL/Romer - 2020 - In Praise of Confidence Intervals.pdf}
}

@book{rosenbaum_design_2020,
  title = {Design of {{Observational Studies}}},
  author = {Rosenbaum, Paul R.},
  year = {2020},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-46405-9},
  urldate = {2022-02-01},
  isbn = {978-3-030-46404-2 978-3-030-46405-9},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/HN6VH6Q5/Rosenbaum - 2020 - Design of Observational Studies.pdf}
}

@book{rosenbaum_observational_2002,
  title = {Observational {{Studies}}},
  author = {Rosenbaum, Paul R.},
  year = {2002},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-3692-2},
  urldate = {2022-02-01},
  isbn = {978-1-4419-3191-7 978-1-4757-3692-2},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/TJJUEJ7T/Rosenbaum - 2002 - Observational Studies.pdf}
}

@article{rosenthal_file_1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Publication bias,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors}
}

@incollection{roth_lets_1994,
  title = {Lets {{Keep}} the {{Con}} out of {{Experimental Econ}}.: {{A Methodological Note}}},
  shorttitle = {Lets {{Keep}} the {{Con}} out of {{Experimental Econ}}.},
  booktitle = {Experimental {{Economics}}},
  author = {Roth, Alvin E.},
  editor = {Hey, John D.},
  year = {1994},
  series = {Studies in {{Empirical Economics}}},
  pages = {99--109},
  publisher = {Physica-Verlag HD},
  address = {Heidelberg},
  doi = {10.1007/978-3-642-51179-0_6},
  urldate = {2021-11-11},
  abstract = {When Edward Learner (1983) wrote the well known critique of econometric practice whose title I have adapted and adopted, he was concerned that the credibility and utility of econometric research had suffered because of differences between the way econometric research was conducted and the way it was reported1. He wrote (p36--37): ``The econometric art as it is practiced at the computer terminal involves fitting many, perhaps thousands, of statistical models. One or several that the researcher finds pleasing are selected for reporting purposes. This searching for a model is often well intentioned, but there can be no doubt that such a specification search invalidates the traditional theories of inference. The concepts of unbiasedness, consistency, efficiency, maximum-likelihood estimation, in fact, all the concepts of traditional theory, utterly lose their meaning by the time an applied researcher pulls from the bramble of computer output the one thorn of a model he likes best, the one he chooses to portray as a rose. The consuming public is hardly fooled by this chicanery.''},
  isbn = {978-3-642-51179-0},
  langid = {english},
  keywords = {Inflated effects,To read},
  file = {/Users/vincentbagilet/Zotero/storage/36NJAYY7/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf;/Users/vincentbagilet/Zotero/storage/HDN3HJHE/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf;/Users/vincentbagilet/Zotero/storage/UL32XMDW/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf;/Users/vincentbagilet/Zotero/storage/Z7LYMBCR/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf}
}

@article{roth_pre-test_2022,
  title = {Pre-Test with {{Caution}}: {{Event-Study Estimates}} after {{Testing}} for {{Parallel Trends}}},
  shorttitle = {Pre-Test with {{Caution}}},
  author = {Roth, Jonathan},
  year = {2022},
  journal = {American Economic Review: Insights},
  doi = {10.1257/aeri.20210236},
  urldate = {2022-01-11},
  langid = {english},
  keywords = {DiD,Example,Power,Pre-trend,Simulations},
  file = {/Users/vincentbagilet/Zotero/storage/J3KD8VRK/Roth - Pre-test with Caution Event-Study Estimates after.pdf;/Users/vincentbagilet/Zotero/storage/LSDSJEUM/Roth - Pre-test with Caution Event-Study Estimates after.pdf;/Users/vincentbagilet/Zotero/storage/M6FE565U/Roth - Pre-test with Caution Event-Study Estimates after.pdf;/Users/vincentbagilet/Zotero/storage/UB49TF67/Roth - Pre-test with Caution Event-Study Estimates after.pdf}
}

@article{rubin_for_2008,
  title = {For Objective Causal Inference, Design Trumps Analysis},
  author = {Rubin, Donald B.},
  year = {2008},
  month = sep,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {3},
  issn = {1932-6157},
  doi = {10.1214/08-AOAS187},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/97JVZL3E/Rubin - 2008 - For objective causal inference, design trumps anal.pdf}
}

@article{rubin_meta-analysis_nodate,
  title = {Meta-{{Analysis}}: {{Literature Synthesis}} or {{Effect-Size Surface Estimation}}?},
  author = {Rubin, Donald B},
  pages = {12},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/N4HHKXKI/Rubin - Meta-Analysis Literature Synthesis or Effect-Size.pdf}
}

@article{rubin_using_2001,
  title = {Using {{Propensity Scores}} to {{Help Design Observational Studies}}: {{Application}} to the {{Tobacco Litigation}}},
  shorttitle = {Using {{Propensity Scores}} to {{Help Design Observational Studies}}},
  author = {Rubin, Donald B.},
  year = {2001},
  month = dec,
  journal = {Health Services and Outcomes Research Methodology},
  volume = {2},
  number = {3},
  pages = {169--188},
  issn = {1572-9400},
  doi = {10.1023/A:1020363010465},
  urldate = {2023-09-08},
  abstract = {Propensity score methodology can be used to help design observational studies in a way analogous to the way randomized experiments are designed: without seeing any answers involving outcome variables. The typical models used to analyze observational data (e.g., least squares regressions, difference of difference methods) involve outcomes, and so cannot be used for design in this sense. Because the propensity score is a function only of covariates, not outcomes, repeated analyses attempting to balance covariate distributions across treatment groups do not bias estimates of the treatment effect on outcome variables. This theme will the primary focus of this article: how to use the techniques of matching, subclassification and/or weighting to help design observational studies. The article also proposes a new diagnostic table to aid in this endeavor, which is especially useful when there are many covariates under consideration. The conclusion of the initial design phase may be that the treatment and control groups are too far apart to produce reliable effect estimates without heroic modeling assumptions. In such cases, it may be wisest to abandon the intended observational study, and search for a more acceptable data set where such heroic modeling assumptions are not necessary. The ideas and techniques will be illustrated using the initial design of an observational study for use in the tobacco litigation based on the NMES data set.},
  langid = {english},
  keywords = {Matching}
}

@article{sampford_inequalities_1953,
  title = {Some {{Inequalities}} on {{Mill}}'s {{Ratio}} and {{Related Functions}}},
  author = {Sampford, M. R.},
  year = {1953},
  journal = {The Annals of Mathematical Statistics},
  volume = {24},
  number = {1},
  eprint = {2236360},
  eprinttype = {jstor},
  pages = {130--132},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851},
  urldate = {2022-06-20},
  keywords = {Maths,Mills ratio},
  file = {/Users/vincentbagilet/Zotero/storage/YSMRG8KB/Sampford - 1953 - Some Inequalities on Mill's Ratio and Related Func.pdf}
}

@techreport{schellEvaluatingMethodsEstimate2018,
  title = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}: {{A Simulation Study}}},
  shorttitle = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}},
  author = {Schell, Terry L. and Griffin, Beth Ann and Morral, Andrew R.},
  year = {2018},
  month = dec,
  institution = {RAND Corporation},
  urldate = {2024-04-11},
  abstract = {The authors use simulations to assess the performance of a wide range of statistical models commonly used in the gun policy literature to estimate the effects of state-level gun policies on firearm deaths and to identify the most-appropriate statistical methods for producing estimates. The results suggest substantial statistical problems with many of the methods used in this field. The authors identify the best method among those assessed.},
  langid = {english},
  keywords = {Crime and Violence Prevention,Firearms,Gun Policy,Gun Violence,Public Safety Legislation,RAND-initiated,Statistical Analysis Methodology},
  file = {/Users/vincentbagilet/Zotero/storage/53KJEZ7M/Schell et al. - 2018 - Evaluating Methods to Estimate the Effect of State.pdf}
}

@article{schlenkerNonlinear2009,
  title = {Nonlinear Temperature Effects Indicate Severe Damages to {{U}}.{{S}}. Crop Yields under Climate Change},
  author = {Schlenker, Wolfram and Roberts, Michael J.},
  year = {2009},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {37},
  pages = {15594--15598},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.0906865106},
  urldate = {2023-12-12},
  abstract = {The United States produces 41\% of the world's corn and 38\% of the world's soybeans. These crops comprise two of the four largest sources of caloric energy produced and are thus critical for world food supply. We pair a panel of county-level yields for these two crops, plus cotton (a warmer-weather crop), with a new fine-scale weather dataset that incorporates the whole distribution of temperatures within each day and across all days in the growing season. We find that yields increase with temperature up to 29{$^\circ$} C for corn, 30{$^\circ$} C for soybeans, and 32{$^\circ$} C for cotton but that temperatures above these thresholds are very harmful. The slope of the decline above the optimum is significantly steeper than the incline below it. The same nonlinear and asymmetric relationship is found when we isolate either time-series or cross-sectional variations in temperatures and yields. This suggests limited historical adaptation of seed varieties or management practices to warmer temperatures because the cross-section includes farmers' adaptations to warmer climates and the time-series does not. Holding current growing regions fixed, area-weighted average yields are predicted to decrease by 30--46\% before the end of the century under the slowest (B1) warming scenario and decrease by 63--82\% under the most rapid warming scenario (A1FI) under the Hadley III model.},
  file = {/Users/vincentbagilet/Zotero/storage/7I2CNNP2/appendix_schlenker_roberts_2009.pdf;/Users/vincentbagilet/Zotero/storage/IZJPR7Y2/Schlenker and Roberts - 2009 - Nonlinear temperature effects indicate severe dama.pdf}
}

@article{schochet_statistical_2021,
  title = {Statistical {{Power}} for {{Estimating Treatment Effects Using Difference-in-Differences}} and {{Comparative Interrupted Time Series Designs}} with {{Variation}} in {{Treatment Timing}}},
  author = {Schochet, Peter Z.},
  year = {2021},
  month = oct,
  journal = {arXiv:2102.06770 [econ, stat]},
  eprint = {2102.06770},
  primaryclass = {econ, stat},
  urldate = {2021-12-13},
  abstract = {This article develops new closed-form variance expressions for power analyses for commonly used difference-in-differences (DID) and comparative interrupted time series (CITS) panel data estimators. The main contribution is to incorporate variation in treatment timing into the analysis. The power formulas also account for other key design features that arise in practice: autocorrelated errors, unequal measurement intervals, and clustering due to the unit of treatment assignment. We consider power formulas for both cross-sectional and longitudinal models and allow for covariates. An illustrative power analysis provides guidance on appropriate sample sizes. The key finding is that accounting for treatment timing increases required sample sizes. Further, DID estimators have considerably more power than standard CITS and ITS estimators. An available Shiny R dashboard performs the sample size calculations for the considered estimators.},
  archiveprefix = {arXiv},
  keywords = {DID},
  file = {/Users/vincentbagilet/Zotero/storage/5G7BVMKY/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf;/Users/vincentbagilet/Zotero/storage/UN9XBX25/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf;/Users/vincentbagilet/Zotero/storage/X9C4ENB3/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf;/Users/vincentbagilet/Zotero/storage/ZRUQZSZC/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf}
}

@article{schwartz_estimating_2015,
  title = {Estimating {{Causal Associations}} of {{Fine Particles With Daily Deaths}} in {{Boston}}},
  author = {Schwartz, Joel and Austin, Elena and Bind, Marie-Abele and Zanobetti, Antonella and Koutrakis, Petros},
  year = {2015},
  month = oct,
  journal = {American Journal of Epidemiology},
  volume = {182},
  number = {7},
  pages = {644--650},
  issn = {0002-9262},
  doi = {10.1093/aje/kwv101},
  urldate = {2023-11-14},
  abstract = {Many studies have reported associations between daily particles less than 2.5 {\textmu}m in aerodynamic diameter (PM2.5) and deaths, but they have been associational studies that did not use formal causal modeling approaches. On the basis of a potential outcome approach, we used 2 causal modeling methods with different assumptions and strengths to address whether there was a causal association between daily PM2.5 and deaths in Boston, Massachusetts (2004--2009). We used an instrumental variable approach, including back trajectories as instruments for variations in PM2.5 uncorrelated with other predictors of death. We also used propensity score as an alternative causal modeling analysis. The former protects against confounding by measured and unmeasured confounders and is based on the assumption of a valid instrument. The latter protects against confounding by all measured covariates, provides valid estimates in the case of effect modification, and is based on the assumption of no unmeasured confounders. We found a causal association of PM2.5 with mortality, with a 0.53\% (95\% confidence interval: 0.09, 0.97) and a 0.50\% (95\% confidence interval: 0.20, 0.80) increase in daily deaths using the instrumental variable and the propensity score, respectively. We failed to reject the null association with exposure after the deaths (P =0.93). Given these results, prior studies, and extensive toxicological support, the association between PM2.5 and deaths is almost certainly causal.},
  file = {/Users/vincentbagilet/Zotero/storage/UBRJK5BG/Schwartz et al. - 2015 - Estimating Causal Associations of Fine Particles W.pdf}
}

@article{schwartz_national_2018,
  title = {A {{National Multicity Analysis}} of the {{Causal Effect}} of {{Local Pollution}}, {{NO2}}, and {{PM2}}.5 on {{Mortality}}},
  author = {Schwartz, Joel and Fong, Kelvin and Zanobetti, Antonella},
  year = {2018},
  month = aug,
  journal = {Environmental Health Perspectives},
  volume = {126},
  number = {8},
  pages = {087004},
  publisher = {Environmental Health Perspectives},
  doi = {10.1289/EHP2732},
  urldate = {2023-11-08},
  abstract = {Background: Studies have long associated  PM2.5  with daily mortality, but few applied causal-modeling methods, or at low exposures. Short-term exposure to  NO2 , a marker of local traffic, has also been associated with mortality but is less studied. We previously found a causal effect between local air pollution and mortality in Boston. Objectives: We aimed to estimate the causal effects of local pollution,  PM2.5 , and  NO2  on mortality in 135 U.S. cities. Methods: We used three methods which, under different assumptions, provide causal marginal estimates of effect: a marginal structural model, an instrumental variable analysis, and a negative exposure control. The instrumental approach used planetary boundary layer, wind speed, and air pressure as instruments for concentrations of local pollutants; the marginal structural model separated the effects of  NO2  from the effects of  PM2.5 , and the negative exposure control provided protection against unmeasured confounders. Results: In 7.3 million deaths, the instrumental approach estimated that mortality increased 1.5\% [95\% confidence interval (CI): 1.1\%, 2.0\%] per {$\mu$} 10\,{$\mu$}g/m3  increase in local pollution indexed as  PM2.5 . The negative control exposure was not associated with mortality. Restricting our analysis to days with  PM2.5  below {$\mu$} 25\,{$\mu$}g/m3 , we found a 1.70\% (95\% CI 1.11\%, 2.29\%) increase. With marginal structural models, we found positive significant increases in deaths with both  PM2.5  and  NO2 . On days with  PM2.5  below {$\mu$} 25\,{$\mu$}g/m3 , we found a 0.83\% (95\% CI 0.39\%, 1.27\%) increase. Including negative exposure controls changed estimates minimally. Conclusions: Causal-modeling techniques, each subject to different assumptions, demonstrated causal effects of locally generated pollutants on daily deaths with effects at concentrations below the current EPA daily  PM2.5  standard. https://doi.org/10.1289/EHP2732},
  file = {/Users/vincentbagilet/Zotero/storage/CEV4W7IJ/Schwartz et al. - 2018 - A National Multicity Analysis of the Causal Effect.pdf}
}

@book{shadish_experimental_2002,
  title = {Experimental and {{Quasi-experimental Designs}} for {{Generalized Causal Inference}}},
  author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald Thomas},
  year = {2002},
  publisher = {Houghton Mifflin},
  abstract = {This long awaited successor of the original Cook/Campbell Quasi-Experimentation: Design and Analysis Issues for Field Settings represents updates in the field over the last two decades. The book covers four major topics in field experimentation:Theoretical matters: Experimentation, causation, and validityQuasi-experimental design: Regression discontinuity designs, interrupted time series designs, quasi-experimental designs that use both pretests and control groups, and other designsRandomized experiments: Logic and design issues, and practical problems involving ethics, recruitment, assignment, treatment implementation, and attritionGeneralized causal inference: A grounded theory of generalized causal inference, along with methods for implementing that theory in single and multiple studies},
  googlebooks = {o7jaAAAAMAAJ},
  isbn = {978-0-395-61556-0},
  langid = {english},
  keywords = {Philosophy / Epistemology,Psychology / Experimental Psychology,Psychology / General}
}

@article{shah_short_2015,
  title = {Short Term Exposure to Air Pollution and Stroke: Systematic Review and Meta-Analysis},
  shorttitle = {Short Term Exposure to Air Pollution and Stroke},
  author = {Shah, Anoop S. V. and Lee, Kuan Ken and McAllister, David A. and Hunter, Amanda and Nair, Harish and Whiteley, William and Langrish, Jeremy P. and Newby, David E. and Mills, Nicholas L.},
  year = {2015},
  month = mar,
  journal = {BMJ},
  volume = {350},
  pages = {h1295},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj.h1295},
  urldate = {2023-11-07},
  abstract = {Objective To review the evidence for the short term association between air pollution and stroke. Design Systematic review and meta-analysis of observational studies Data sources Medline, Embase, Global Health, Cumulative Index to Nursing and Allied Health Literature (CINAHL), and Web of Science searched to January 2014 with no language restrictions. Eligibility criteria Studies investigating the short term associations (up to lag of seven days) between daily increases in gaseous pollutants (carbon monoxide, sulphur dioxide, nitrogen dioxide, ozone) and particulate matter ({$<$}2.5 {\textmu}m or {$<$}10 {\textmu}m diameter (PM2.5 and PM10)), and admission to hospital for stroke or mortality. Main outcome measures Admission to hospital and mortality from stroke. Results From 2748 articles, 238 were reviewed in depth with 103 satisfying our inclusion criteria and 94 contributing to our meta-estimates. This provided a total of 6.2 million events across 28 countries. Admission to hospital for stroke or mortality from stroke was associated with an increase in concentrations of carbon monoxide (relative risk 1.015 per 1 ppm, 95\% confidence interval 1.004 to 1.026), sulphur dioxide (1.019 per 10 ppb, 1.011 to 1.027), and nitrogen dioxide (1.014 per 10 ppb, 1.009 to 1.019). Increases in PM2.5 and PM10 concentration were also associated with admission and mortality (1.011 per 10 {$\mu$}g/m3 (1.011 to 1.012) and 1.003 per 10 {\textmu}g/m3 (1.002 to 1.004), respectively). The weakest association was seen with ozone (1.001 per 10 ppb, 1.000 to 1.002). Strongest associations were observed on the day of exposure with more persistent effects observed for PM2{$\cdot$}5. Conclusion Gaseous and particulate air pollutants have a marked and close temporal association with admissions to hospital for stroke or mortality from stroke. Public and environmental health policies to reduce air pollution could reduce the burden of stroke. Systematic review registration PROSPERO-CRD42014009225.},
  chapter = {Research},
  copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions .  This is an Open Access article distributed in accordance with the Creative Commons Attribution (CC BY 4.0) license, which permits others to distribute, remix, adapt and build upon this work, for commercial use, provided the original work is properly cited. See: http://creativecommons.org/licenses/by/4.0/ .},
  langid = {english},
  pmid = {25810496},
  keywords = {Air pollution,Literature review,Stroke},
  file = {/Users/vincentbagilet/Zotero/storage/77UHYL3Z/Shah et al. - 2015 - Short term exposure to air pollution and stroke s.pdf}
}

@incollection{shaw_theoretical_2020,
  title = {A {{Theoretical Exploration}} of {{Turnout}} and {{Voting}}},
  booktitle = {The {{Turnout Myth}}},
  author = {Shaw, Daron R. and Petrocik, John R.},
  year = {2020},
  publisher = {Oxford University Press},
  address = {New York},
  doi = {10.1093/oso/9780190089450.003.0003},
  urldate = {2022-02-23},
  abstract = {This chapter addresses the fundamental question of why people vote in elections. What affects their calculus? How much do these motivations create the unequal turnout rates observed among different social and political groups? The review informs the book's initial report of how the decision to vote might (or might not) be related to partisan vote choice. Popular commentary in the media and academic literature, some of which has a reformist orientation, is assessed to provide a balanced portrait of what is known about the turnout bias. Particular attention is paid to political science research on the subject of turnout bias, including both normative and empirical works that yield little consensus.},
  isbn = {978-0-19-008945-0},
  langid = {english},
  keywords = {election laws,full turnout,partisanship,proximity model,rational choice,representativeness,utility models of voting},
  file = {/Users/vincentbagilet/Zotero/storage/GQMC7SAM/Shaw and Petrocik - 2020 - A Theoretical Exploration of Turnout and Voting.pdf}
}

@book{shaw_turnout_2020,
  title = {The {{Turnout Myth}}: {{Voting Rates}} and {{Partisan Outcomes}} in {{American National Elections}}},
  shorttitle = {The {{Turnout Myth}}},
  author = {Shaw, Daron and Petrocik, John},
  year = {2020},
  publisher = {Oxford University Press},
  address = {New York},
  doi = {10.1093/oso/9780190089450.001.0001},
  urldate = {2022-02-23},
  abstract = {This book refutes the widely held convention that high turnout in national elections advantages Democratic candidates while low turnout helps Republicans. It examines over fifty years of presidential, gubernatorial, Senate, and House election data to show there is no consistent partisan effect associated with turnout. The overall relationship between the partisan vote and turnout for these offices is uncorrelated. Most significant, there is no observable party bias to turnout when each office or seat is examined through time. In some states, across the decades, gubernatorial and senatorial contests show a pro-Democratic bias to turnout; in others an increase in turnout helps Republicans. The pattern repeats for House elections during the 1970s, 1980s, 1990s, 2000s, and through the 2010s. The analysis demonstrates that, within the range that turnout varies in American elections, it is the participation and abstention of easily influenced, less engaged citizens---peripheral voters---that move the outcome between the parties. These voters are the most influenced when the short-term forces of the election---differential candidate appeal, issues, scandals, and so forth---help the parties. Since these influences advantage Republicans as often as Democrats, the oscillation in turnout that coincides with pro-GOP and pro-Democratic forces leaves turnout rates inconsequential overall. The connections between short-term forces and the election cycle dominate the inconsistent partisan effects of turnout.},
  isbn = {978-0-19-008945-0},
  langid = {english},
  keywords = {abstention,campaigns,elections,mobilization,parties,peripheral voters,representativeness,turnout,vote switching,voting}
}

@article{sheriff_how_2019,
  title = {How {{Did Air Quality Standards Affect Employment}} at {{US Power Plants}}? {{The Importance}} of {{Timing}}, {{Geography}}, and {{Stringency}}},
  shorttitle = {How {{Did Air Quality Standards Affect Employment}} at {{US Power Plants}}?},
  author = {Sheriff, Glenn and Ferris, Ann E. and Shadbegian, Ronald J.},
  year = {2019},
  month = jan,
  journal = {Journal of the Association of Environmental and Resource Economists},
  publisher = {University of Chicago PressChicago, IL},
  issn = {2333-5955},
  doi = {10.1086/700929},
  urldate = {2023-10-21},
  abstract = {Abstract We examine fossil-fuel power plant employment impacts of new nitrogen oxides (NOx) provisions under Title I of the 1990 Clean Air Act Amendments (CAAAs). These provisions required installation of reasonably available control technology (RACT) for NOx emissions for major stationary sources in the Ozone Transport Region and in more stringently classified ozone nonattainment areas. Standard approaches using nonattainment designation to identify regulatory impacts abstract from important implementation aspects such as when regulatory changes occur, where regulations are in effect, and which specific regulations apply. Omitting these factors can introduce bias by contaminating the control group, leading to underestimation of historical employment impacts and overestimation of projected impacts from tightening regulations. Our results indicate that the new NOx RACT requirements negatively impacted power plant employment. We find no significant impacts on generation, suggesting that installation of pollution controls may have contributed to labor-saving technical change at affected units.},
  copyright = {{\copyright} 2018 by The Association of Environmental and Resource Economists. All rights reserved.},
  langid = {english},
  keywords = {Clean Air Act,Employment,Environment,Environmental Regulation,Jobs,US},
  file = {/Users/vincentbagilet/Zotero/storage/YIDLZ3IP/Sheriff et al. - 2019 - How Did Air Quality Standards Affect Employment at.pdf}
}

@article{simmons_false-positive_2011,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  urldate = {2023-03-31},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists? nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {Forking paths,Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/BAV9AR59/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf}
}

@misc{simonsohn_p-hacked_2016,
  title = {P-Hacked {{Hypotheses Are Deceivingly Robust}}},
  author = {Simonsohn, Uri},
  year = {2016},
  month = apr,
  journal = {Data Colada},
  urldate = {2023-10-25},
  abstract = {Sometimes we selectively report the analyses we run to test a hypothesis. Other times we selectively report which hypotheses we tested. One popular way to p-hack hypotheses involves subgroups. Upon realizing analyses of~the entire sample do not produce a significant effect, we check whether~analyses of various subsamples --- women, or the young, or republicans, or...},
  langid = {american},
  keywords = {Robustness checks},
  file = {/Users/vincentbagilet/Zotero/storage/GKYN2JB2/Simonsohn - 2016 - P-hacked Hypotheses Are Deceivingly Robust.pdf}
}

@misc{sloczynski_general_2018,
  title = {A {{General Weighted Average Representation}} of the {{Ordinary}} and {{Two-Stage Least Squares Estimands}}},
  author = {Sloczynski, Tymon},
  year = {2018},
  month = sep,
  number = {125},
  eprint = {125},
  publisher = {{Brandeis University, Department of Economics and International Business School}},
  urldate = {2025-01-30},
  abstract = {It is standard practice in applied work to study the effect of a binary variable ("treatment") on an outcome of interest using linear models with additive effects. In this paper I study the interpretation of the ordinary and two-stage least squares estimands in such models when treatment effects are in fact heterogeneous. I show that in both cases the coefficient on treatment is identical to a convex combination of two other parameters (different for OLS and 2SLS), which can be interpreted as the average treatment effects on the treated and controls under additional assumptions. Importantly, the OLS and 2SLS weights on these parameters are inversely related to the proportion of each group. The more units get treatment, the less weight is placed on the effect on the treated. What follows, the reliance on these implicit weights can have serious consequences for applied work. I illustrate some of these issues in four empirical applications from different fields of economics. I also develop a weighted least squares correction and simple diagnostic tools that applied researchers can use to avoid potential biases. In an important special case, my diagnostics only require the knowledge of the proportion of treated units.},
  archiveprefix = {Brandeis University, Department of Economics and International Business School},
  langid = {english},
  keywords = {empirical heterogeneity,ordinary least squares,propensity score,treatment effects,TWFE,Weights},
  file = {/Users/vincentbagilet/Zotero/storage/W5UCY4MN/Sloczynski - 2018 - A General Weighted Average Representation of the Ordinary and Two-Stage Least Squares Estimands.pdf;/Users/vincentbagilet/Zotero/storage/2RDUQ9CQ/125.html}
}

@article{sloczynski_interpreting_2022,
  title = {Interpreting {{OLS Estimands When Treatment Effects Are Heterogeneous}}: {{Smaller Groups Get Larger Weights}}},
  shorttitle = {Interpreting {{OLS Estimands When Treatment Effects Are Heterogeneous}}},
  author = {S{\l}oczy{\'n}ski, Tymon},
  year = {2022},
  month = may,
  journal = {The Review of Economics and Statistics},
  volume = {104},
  number = {3},
  pages = {501--509},
  issn = {0034-6535},
  doi = {10.1162/rest_a_00953},
  urldate = {2022-06-10},
  abstract = {Applied work often studies the effect of a binary variable (``treatment'') using linear models with additive effects. I study the interpretation of the OLS estimands in such models when treatment effects are heterogeneous. I show that the treatment coefficient is a convex combination of two parameters, which under certain conditions can be interpreted as the average treatment effects on the treated and untreated. The weights on these parameters are inversely related to the proportion of observations in each group. Reliance on these implicit weights can have serious consequences for applied work, as I illustrate with two well-known applications. I develop simple diagnostic tools that empirical researchers can use to avoid potential biases. Software for implementing these methods is available in R and Stata. In an important special case, my diagnostics require only the knowledge of the proportion of treated units.},
  keywords = {Maths,To read,Weights},
  file = {/Users/vincentbagilet/Zotero/storage/4HV62AJN/Słoczyński - 2022 - Interpreting OLS Estimands When Treatment Effects .pdf}
}

@article{somanathan_impact_2021,
  title = {The {{Impact}} of {{Temperature}} on {{Productivity}} and {{Labor Supply}}: {{Evidence}} from {{Indian Manufacturing}}},
  shorttitle = {The {{Impact}} of {{Temperature}} on {{Productivity}} and {{Labor Supply}}},
  author = {Somanathan, E. and Somanathan, Rohini and Sudarshan, Anant and Tewari, Meenu},
  year = {2021},
  month = jun,
  journal = {Journal of Political Economy},
  volume = {129},
  number = {6},
  pages = {1797--1827},
  publisher = {The University of Chicago Press},
  issn = {0022-3808},
  doi = {10.1086/713733},
  urldate = {2024-04-10},
  abstract = {Hotter years are associated with lower economic output in developing countries. We show that the effect of temperature on labor is an important part of the explanation. Using microdata from selected firms in India, we estimate reduced worker productivity and increased absenteeism on hot days. Climate control significantly mitigates productivity losses. In a national panel of Indian factories, annual plant output falls by about 2\% per degree Celsius. This response appears to be driven by a reduction in the output elasticity of labor. Our estimates are large enough to explain previously observed output losses in cross-country panels.},
  keywords = {Fixed effects,India,Labor,Productivity,Temperature,Worker level},
  file = {/Users/vincentbagilet/Zotero/storage/425SPT2C/2015376Appendix.pdf;/Users/vincentbagilet/Zotero/storage/TPVMXKPP/Somanathan et al. - 2021 - The Impact of Temperature on Productivity and Labo.pdf}
}

@article{spangler_wetbulb_2022,
  title = {Wet-{{Bulb Globe Temperature}}, {{Universal Thermal Climate Index}}, and {{Other Heat Metrics}} for {{US Counties}}, 2000--2020},
  author = {Spangler, Keith R. and Liang, Shixin and Wellenius, Gregory A.},
  year = {2022},
  month = jun,
  journal = {Scientific Data},
  volume = {9},
  number = {1},
  pages = {326},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/s41597-022-01405-3},
  urldate = {2024-06-21},
  abstract = {Epidemiologic research on extreme heat consistently finds significant impacts on human morbidity and mortality. However, most of these analyses do not use spatially explicit measures of heat (typically assessing exposures at major cities using the nearest weather station), and they frequently consider only ambient temperature or heat index. The field is moving toward more expansive analyses that use spatially resolved gridded meteorological datasets and alternative assessments of heat, such as wet-bulb globe temperature (WBGT) and universal thermal climate index (UTCI), both of which require technical geoscientific skills that may be inaccessible to many public health researchers. To facilitate research in this domain, we created a database of population-weighted, spatially explicit daily heat metrics -- including WBGT, UTCI, heat index, dewpoint temperature, net effective temperature, and humidex -- for counties in the conterminous United States derived from the ERA5-Land gridded data set and using previously validated equations and algorithms. We also provide an R package to calculate these metrics, including gold-standard algorithms for estimating WBGT and UTCI, to facilitate replication.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Climate sciences,Environmental health},
  file = {/Users/vincentbagilet/Zotero/storage/9NZDHI3U/Spangler et al. - 2022 - Wet-Bulb Globe Temperature, Universal Thermal Clim.pdf}
}

@article{staiger_instrumental_1997,
  title = {Instrumental {{Variables Regression}} with {{Weak Instruments}}},
  author = {Staiger, Douglas and Stock, James H.},
  year = {1997},
  journal = {Econometrica},
  volume = {65},
  number = {3},
  eprint = {2171753},
  eprinttype = {jstor},
  pages = {557--586},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/2171753},
  urldate = {2022-04-12},
  abstract = {This paper develops asymptotic distribution theory for single-equation instrumental variables regression when the partial correlations between the instruments and the endogenous variables are weak, here modeled as local to zero. Asymptotic representations are provided for various statistics, including two-stage least squares (TSLS) and limited information maximum likelihood (LIML) estimators, Wald statistics, and statistics testing overidentification and endogeneity. The asymptotic distributions are found to provide good approximations to sampling distributions with 10-20 observations per instrument. The theory suggests concrete guidelines for applied work, including using nonstandard methods for construction of confidence regions. These results are used to interpret Angrist and Krueger's (1991) estimates of the returns to education: whereas TSLS estimates with many instruments approach the OLS estimate of 6\%, the more reliable LIML estimates with fewer instruments fall between 8\% and 10\%, with a typical 95\% confidence interval of (5\%, 15\%).},
  keywords = {Econometrics,IV,Maths},
  file = {/Users/vincentbagilet/Zotero/storage/GCNUXVQW/Staiger and Stock - 1997 - Instrumental Variables Regression with Weak Instru.pdf}
}

@article{stevens_temperature_2017,
  title = {Temperature, {{Wages}}, and {{Agricultural Labor Productivity}}},
  author = {Stevens, Andrew},
  year = {2017},
  langid = {english},
  keywords = {Agriculture,Blueberry,Environment,Labor,Productivity,Temperature,Worker level},
  file = {/Users/vincentbagilet/Zotero/storage/DJWHN2AP/Stevens - Temperature, Wages, and Agricultural Labor Product.pdf}
}

@book{stock_introduction_2020,
  title = {Introduction to Econometrics},
  author = {Stock, James H. and Watson, Mark W.},
  year = {2020},
  series = {The {{Pearson}} Series in Economics},
  edition = {Fourth edition, global edition},
  publisher = {Pearson},
  address = {Harlow, England London New York Boston San Francisco Toronto Sydney Dubai Singapore Hong Kong Tokyo Seoul Taipei New Delhi Cape Town Sao Paulo Mexico City Madrid Amsterdam Munich Paris Milan},
  isbn = {978-1-292-26445-5},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/3QBUSDQI/Stock and Watson - 2020 - Introduction to econometrics.pdf}
}

@article{stommes_reliability_2021,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = {2021},
  month = sep,
  journal = {arXiv},
  eprint = {2109.14526},
  urldate = {2022-01-27},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it the position as a standard method in modern political science research. But identification does not necessarily imply that the causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation is particularly challenging with the RD design and investigate how these challenges manifest themselves in the empirical literature. We collect all RD-based findings published in top political science journals from 2009--2018. The findings exhibit pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher's discretion is not a major driver of these pathological features, but researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design are exaggerated, if not entirely spurious.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Political Science,RDD},
  file = {/Users/vincentbagilet/Zotero/storage/CFYJYNF8/Stommes et al. - 2021 - On the reliability of published findings using the.pdf}
}

@article{stommesReliability2023,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = {2023},
  month = apr,
  journal = {Research \& Politics},
  volume = {10},
  number = {2},
  publisher = {SAGE Publications Ltd},
  issn = {2053-1680},
  doi = {10.1177/20531680231166457},
  urldate = {2024-05-16},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it a position as a standard method in modern political science research. But identification does not necessarily imply that causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation under the RD design involves serious statistical challenges and investigate how these challenges manifest themselves in the empirical literature in political science. We collect all RD-based findings published in top political science journals in the period 2009--2018. The distribution of published results exhibits pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher discretion is not a major driver of these features. However, researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design may be exaggerated.},
  langid = {english},
  keywords = {Political Science,Power,Publication bias,RDD},
  file = {/Users/vincentbagilet/Zotero/storage/74YVA2JE/Stommes et al. - 2023 - On the reliability of published findings using the.pdf}
}

@article{sukhtankar_replications_2017,
  title = {Replications in {{Development Economics}}},
  author = {Sukhtankar, Sandip},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {32--36},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171120},
  urldate = {2021-11-11},
  abstract = {I examine replications of empirical papers in development economics published in the top-5 and next-5 general interest journals between the years 2000 through 2015. Of the 1,138 empirical papers, 71 papers (6.2 percent) were replicated in another published paper or working paper. The majority (77.5 percent) of replications involved reanalysis of the data using different econometric specifications to assess robustness. The strongest predictor of whether a paper is replicated or not is the paper's Google Scholar citation count, followed by year of publication. Papers based on randomized control trials (RCTs) appear to be replicated at a higher rate (12.5 percent).},
  langid = {english},
  keywords = {Development economics,Replications},
  file = {/Users/vincentbagilet/Zotero/storage/CRET9PU8/Sukhtankar - 2017 - Replications in Development Economics.pdf;/Users/vincentbagilet/Zotero/storage/F4WPRIVK/Sukhtankar - 2017 - Replications in Development Economics.pdf;/Users/vincentbagilet/Zotero/storage/F6IE7PPG/Sukhtankar - 2017 - Replications in Development Economics.pdf;/Users/vincentbagilet/Zotero/storage/WE9F5RRR/Sukhtankar - 2017 - Replications in Development Economics.pdf}
}

@article{suri_selection_2011,
  title = {Selection and {{Comparative Advantage}} in {{Technology Adoption}}},
  author = {Suri, Tavneet},
  year = {2011},
  journal = {Econometrica},
  volume = {79},
  number = {1},
  pages = {159--209},
  issn = {1468-0262},
  doi = {10.3982/ECTA7749},
  urldate = {2023-03-30},
  abstract = {This paper investigates an empirical puzzle in technology adoption for developing countries: the low adoption rates of technologies like hybrid maize that increase average farm profits dramatically. I offer a simple explanation for this: benefits and costs of technologies are heterogeneous, so that farmers with low net returns do not adopt the technology. I examine this hypothesis by estimating a correlated random coefficient model of yields and the corresponding distribution of returns to hybrid maize. This distribution indicates that the group of farmers with the highest estimated gross returns does not use hybrid, but their returns are correlated with high costs of acquiring the technology (due to poor infrastructure). Another group of farmers has lower returns and adopts, while the marginal farmers have zero returns and switch in and out of use over the sample period. Overall, adoption decisions appear to be rational and well explained by (observed and unobserved) variation in heterogeneous net benefits to the technology.},
  langid = {english},
  keywords = {Development,Fertilizer,Technology adoption},
  file = {/Users/vincentbagilet/Zotero/storage/CEGGLD3G/Suri - 2011 - Selection and Comparative Advantage in Technology .pdf}
}

@article{thistlethwaite_regression-discontinuity_1960,
  title = {Regression-Discontinuity Analysis: {{An}} Alternative to the Ex Post Facto Experiment},
  shorttitle = {Regression-Discontinuity Analysis},
  author = {Thistlethwaite, Donald L. and Campbell, Donald T.},
  year = {1960},
  journal = {Journal of Educational Psychology},
  volume = {51},
  number = {6},
  pages = {309--317},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2176(Electronic),0022-0663(Print)},
  doi = {10.1037/h0044319},
  abstract = {This study presents a method of testing casual hypotheses, called regression-discontinuity analysis, in situations where the investigator is unable to randomly assign Ss to experimental and control groups. The Ss were selected from near winners---5126 students who received certificates of merit and 2848 students who merely received letters of commendation. Comparison of the results obtained from the new mode of analysis with those obtained when the ex post facto design was applied to the same data. The new analysis suggested that public recognition for achievement tends to increase the likelihood that the recipient will receive a scholarship but did not support the inference that recognition affects the student's attitudes and career plans. From Psyc Abstracts 36:01:1AF09T. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Award,Education,RDD},
  file = {/Users/vincentbagilet/Zotero/storage/6QQ5453R/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/vincentbagilet/Zotero/storage/G4EIYIDL/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/vincentbagilet/Zotero/storage/LW9TV5BQ/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/vincentbagilet/Zotero/storage/VS3AC93I/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf}
}

@misc{timm_retrodesign_2019,
  title = {Retrodesign: {{Tools}} for {{Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Retrodesign},
  author = {Timm, Andrew and Gelman, Andrew and Carlin, John},
  year = {2019},
  month = mar,
  urldate = {2022-02-22},
  abstract = {Provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) {$<$}doi.org/10.1007/s001800000040{$>$} and Gelman \& Carlin (2014) {$<$}doi.org/10.1177/1745691614551642{$>$}. In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given "tolerances" for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) {$<$}doi.org/10.1111/bmsp.12132{$>$} are implemented. As of 1.0.0, this includes support only for simple research designs. See the package vignette for a fuller exposition on how Type S/M errors arise in research, and how to analyze them using the type of design analysis proposed in the above papers.},
  copyright = {MIT + file LICENSE}
}

@article{vasishth_how_2021,
  title = {How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis},
  author = {Vasishth, Shravan and Gelman, Andrew},
  year = {2021},
  month = sep,
  journal = {Linguistics},
  volume = {59},
  number = {5},
  pages = {1311--1342},
  issn = {0024-3949, 1613-396X},
  doi = {10.1515/ling-2019-0051},
  urldate = {2021-12-08},
  abstract = {The use of statistical inference in linguistics and related areas like psychology typically involves a binary decision: either reject or accept some null hypothesis using statistical significance testing. When statistical power is low, this frequentist data-analytic approach breaks down: null results are uninformative, and effect size estimates associated with significant results are overestimated. Using an example from psycholinguistics, several alternative approaches are demonstrated for reporting inconsistencies between the data and a theoretical prediction. The key here is to focus on committing to a falsifiable prediction, on quantifying uncertainty statistically, and learning to accept the fact that -- in almost all practical data analysis situations -- we can only draw uncertain conclusions from data, regardless of whether we manage to obtain statistical significance or not. A focus on uncertainty quantification is likely to lead to fewer excessively bold claims that, on closer investigation, may turn out to be not supported by the data.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/ALNZSQNJ/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf;/Users/vincentbagilet/Zotero/storage/G72L46SU/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf;/Users/vincentbagilet/Zotero/storage/P59HHMZE/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf;/Users/vincentbagilet/Zotero/storage/VJUW5ANZ/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf}
}

@article{vivalt_specification_2019,
  title = {Specification {{Searching}} and {{Significance Inflation Across Time}}, {{Methods}} and {{Disciplines}}},
  author = {Vivalt, Eva},
  year = {2019},
  journal = {Oxford Bulletin of Economics and Statistics},
  volume = {81},
  number = {4},
  pages = {797--816},
  issn = {1468-0084},
  doi = {10.1111/obes.12289},
  urldate = {2023-09-22},
  abstract = {This paper examines how significance inflation has varied across time, methods and disciplines. Leveraging a unique data set of impact evaluations on 20 kinds of development programmes, I find that results from randomized controlled trials exhibit less significance inflation than results from studies using other methods. Further, randomized controlled trials have exhibited less significance inflation over time, but quasi-experimental studies have not. There is no robust difference between results from researchers affiliated with economics departments and those from researchers affiliated with other predominantly health-related departments. Overall, the biases found appear much smaller than those previously observed in other social sciences.},
  copyright = {{\copyright} 2019 The Department of Economics, University of Oxford and John Wiley \& Sons Ltd},
  langid = {english},
  keywords = {Publication bias},
  file = {/Users/vincentbagilet/Zotero/storage/UT6TR3CH/Vivalt - 2019 - Specification Searching and Significance Inflation.pdf}
}

@article{vu_standard_2023,
  title = {Do {{Standard Error Corrections Exacerbate Publication Bias}}?},
  author = {Vu, Patrick},
  year = {2023},
  abstract = {Over the past several decades, econometrics research has devoted substantial efforts to improving the credibility of standard errors. This paper studies how such improvements interact with the selective publication process to affect the ultimate credibility of published studies. I show that adopting improved but enlarged standard errors for individual studies can lead to higher bias in the studies selected for publication. Intuitively, this is because increasing standard errors raises the bar on statistical significance, which exacerbates publication bias. Nevertheless, I show that the coverage of published confidence intervals unambiguously improves. I illustrate these phenomena using newly collected data on the adoption of clustered standard errors in the difference-in-differences literature between 2000 and 2009. Clustering is associated with a near doubling in the magnitude of published effect sizes. I estimate a model of the publication process and find that clustering led to large improvements in coverage but also sizeable increases in bias. To examine the overall impact on evidence-based policy, I develop a model of a policymaker who uses information from published studies to inform policy decisions and overestimates the precision of estimates when standard errors are unclustered. I find that clustering lowers minimax regret when policymakers exhibit sufficiently high loss aversion for mistakenly implementing an ineffective or harmful policy.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/6KPPQEL4/Vu - Do Standard Error Corrections Exacerbate Publicati.pdf}
}

@article{vu_why_2023,
  title = {Why {{Are Replication Rates So Low}}?},
  author = {Vu, Patrick},
  year = {2023},
  langid = {english},
  keywords = {Replication,To read},
  file = {/Users/vincentbagilet/Zotero/storage/AHUY63MQ/Vu - Why Are Replication Rates So Low.pdf}
}

@article{walker_environmental_2011,
  title = {Environmental {{Regulation}} and {{Labor Reallocation}}: {{Evidence}} from the {{Clean Air Act}}},
  shorttitle = {Environmental {{Regulation}} and {{Labor Reallocation}}},
  author = {Walker, W. Reed},
  year = {2011},
  journal = {The American Economic Review},
  volume = {101},
  number = {3},
  eprint = {29783786},
  eprinttype = {jstor},
  pages = {442--447},
  publisher = {American Economic Association},
  issn = {0002-8282},
  urldate = {2023-10-04},
  keywords = {CBA,Clean Air Act,Employment,Environment,Environmental Regulation,US},
  file = {/Users/vincentbagilet/Zotero/storage/A7TA92NF/Walker - 2011 - Environmental Regulation and Labor Reallocation E.pdf}
}

@article{walker_transitional_2013,
  title = {The {{Transitional Costs}} of {{Sectoral Reallocation}}: {{Evidence From}} the {{Clean Air Act}} and the {{Workforce}}*},
  shorttitle = {The {{Transitional Costs}} of {{Sectoral Reallocation}}},
  author = {Walker, W. Reed},
  year = {2013},
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {128},
  number = {4},
  pages = {1787--1835},
  issn = {0033-5533},
  doi = {10.1093/qje/qjt022},
  urldate = {2023-10-22},
  abstract = {This article uses linked worker-firm data in the United States to estimate the transitional costs associated with reallocating workers from newly regulated industries to other sectors of the economy in the context of new environmental regulations. The focus on workers rather than industries as the unit of analysis allows me to examine previously unobserved economic outcomes such as nonemployment and long-run earnings losses from job transitions, both of which are critical to understanding the reallocative costs associated with these policies. Using plant-level panel variation induced by the 1990 Clean Air Act Amendments (CAAA), I find that the reallocative costs of environmental policy are significant. Workers in newly regulated plants experienced, in aggregate, more than \$5.4 billion in forgone earnings for the years after the change in policy. Most of these costs are driven by nonemployment and lower earnings in future employment, highlighting the importance of longitudinal data for characterizing the costs and consequences of labor market adjustment. Relative to the estimated benefits of the 1990 CAAA, these one-time transitional costs are small.},
  keywords = {Clean Air Act,Employment,Environment,Environmental Regulation,Jobs,US},
  file = {/Users/vincentbagilet/Zotero/storage/T9SGU7RV/Walker - 2013 - The Transitional Costs of Sectoral Reallocation E.pdf}
}

@article{wasserstein_asa_2016,
  title = {The {{ASA Statement}} on {\emph{p}} -{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on {\emph{p}} -{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/FA9UWANV/Wasserstein and Lazar - 2016 - The ASA Statement on p -Values Context, Pr.pdf}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} `` {\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2022-02-21},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/BHELXIVT/Wasserstein et al. - 2019 - Moving to a World Beyond “ p  0.05”.pdf}
}

@article{weidmann_lurking_2021,
  title = {Lurking {{Inferential Monsters}}? {{Quantifying Selection Bias}} in {{Evaluations}} of {{School Programs}}},
  shorttitle = {Lurking {{Inferential Monsters}}?},
  author = {Weidmann, Ben and Miratrix, Luke},
  year = {2021},
  journal = {Journal of Policy Analysis and Management},
  volume = {40},
  number = {3},
  pages = {964--986},
  issn = {1520-6688},
  doi = {10.1002/pam.22236},
  urldate = {2023-11-02},
  abstract = {This study examines whether unobserved factors substantially bias education evaluations that rely on the Conditional Independence Assumption. We add 14 new within-study comparisons to the literature, all from primary schools in England. Across these 14 studies, we generate 42 estimates of selection bias using a simple approach to observational analysis. A meta-analysis of these estimates suggests that the distribution of underlying bias is centered around zero. The mean absolute value of estimated bias is 0.03{$\sigma$}, and none of the 42 estimates are larger than 0.11{$\sigma$}. Results are similar for math, reading, and writing outcomes. Overall, we find no evidence of substantial selection bias due to unobserved characteristics. These findings may not generalize easily to other settings or to more radical educational interventions, but they do suggest that non-experimental approaches could play a greater role than they currently do in generating reliable causal evidence for school education.},
  copyright = {{\copyright} 2020 by the Association for Public Policy Analysis and Management},
  langid = {english},
  keywords = {Education,Leo,OVB,To read},
  file = {/Users/vincentbagilet/Zotero/storage/K6TXXFLH/Weidmann and Miratrix - 2021 - Lurking Inferential Monsters Quantifying Selectio.pdf}
}

@article{wilms_omitted_2021,
  title = {Omitted Variable Bias: {{A}} Threat to Estimating Causal Relationships},
  shorttitle = {Omitted Variable Bias},
  author = {Wilms, R. and M{\"a}thner, E. and Winnen, L. and Lanwehr, R.},
  year = {2021},
  month = dec,
  journal = {Methods in Psychology},
  volume = {5},
  pages = {100075},
  issn = {2590-2601},
  doi = {10.1016/j.metip.2021.100075},
  urldate = {2022-10-28},
  abstract = {We aim to raise awareness of the omitted variable bias (i.e., one special form of endogeneity) and highlight its severity for causal claims. Firstly, we demonstrate via analytic proof that omitting a relevant variable from a model which explains the independent and dependent variable leads to biased estimates. Secondly, we offer an easy-to-understand visualization for the problem. Finally, we discuss two remedies, diminishing the risk of the omitted variable bias, namely the instrument variable or two-stage least squares estimator and the regression discontinuity design. We hope that our review will motivate researchers to use them more often in future research.},
  langid = {english},
  keywords = {Omitted variable bias,Sensitivity},
  file = {/Users/vincentbagilet/Zotero/storage/77M4FST8/Wilms et al. - 2021 - Omitted variable bias A threat to estimating caus.pdf}
}

@article{wooldridge_control_2015,
  title = {Control {{Function Methods}} in {{Applied Econometrics}}},
  author = {Wooldridge, Jeffrey M.},
  year = {2015},
  journal = {The Journal of Human Resources},
  volume = {50},
  number = {2},
  eprint = {24735991},
  eprinttype = {jstor},
  pages = {420--445},
  publisher = {[University of Wisconsin Press, Board of Regents of the University of Wisconsin System]},
  issn = {0022-166X},
  urldate = {2023-01-31},
  abstract = {This paper provides an overview of control function (CF) methods for solving the problem of endogenous explanatory variables (EEVs) in linear and nonlinear models. CF methods often can be justified in situations where "plug-in" approaches are known to produce inconsistent estimators of parameters and partial effects. Usually, CF approaches require fewer assumptions than maximum likelihood, and CF methods are computationally simpler. The recent focus on estimating average partial effects, along with theoretical results on nonparametric identification, suggests some simple, flexible parametric CF strategies. The CF approach for handling discrete EEVs in nonlinear models is more controversial but approximate solutions are available.},
  keywords = {Control function,Maths},
  file = {/Users/vincentbagilet/Zotero/storage/JYS855DJ/Wooldridge - 2015 - Control Function Methods in Applied Econometrics.pdf}
}

@misc{yang_answer_2015,
  title = {Answer to "{{Ratio}} of {{CDF}} to {{PDF}} Increasing?"},
  shorttitle = {Answer to "{{Ratio}} of {{CDF}} to {{PDF}} Increasing?},
  author = {Yang, Xiao},
  year = {2015},
  month = jul,
  journal = {Mathematics Stack Exchange},
  urldate = {2022-09-26},
  keywords = {Mills ratio}
}

@article{young_channelling_nodate,
  title = {{{CHANNELLING FISHER}}: {{RANDOMIZATION TESTS AND THE STATISTICAL INSIGNIFICANCE OF SEEMINGLY SIGNIFICANT EXPERIMENTAL RESULTS}}},
  author = {Young, Alwyn},
  pages = {47},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/GQ9MV9A9/Young - CHANNELLING FISHER RANDOMIZATION TESTS AND THE ST.pdf}
}

@article{young_leverage_2021,
  title = {Leverage, {{Heteroskedasticity}} and {{Instrumental Variables}} in {{Practical Application}}},
  author = {Young, Alwyn},
  year = {2021},
  month = jun,
  pages = {43},
  abstract = {I use Monte Carlo simulations, the jackknife and multiple forms of the bootstrap to study a comprehensive sample of 1359 instrumental variables regressions in 31 papers published in the journals of the American Economic Association. Monte Carlo simulations based upon published regressions show that non-iid error processes in highly leveraged regressions, both prominent features of published work, adversely affect the size and power of IV tests, while increasing the bias of IV relative to OLS. Weak instrument pre-tests based upon F-statistics are found to be largely uninformative of both size and bias. In published papers, statistically significant IV results often depend upon only one or two observations or clusters, IV has little power as, despite producing substantively different estimates, it rarely rejects the OLS point estimate or the null that OLS is unbiased, while the statistical significance of excluded instruments is exaggerated.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/N4QMT2UY/Young - Leverage, Heteroskedasticity and Instrumental Vari.pdf}
}

@article{youngConsistencyInferenceInstrumental2022,
  title = {Consistency without {{Inference}}: {{Instrumental Variables}} in {{Practical Application}}},
  shorttitle = {Consistency without {{Inference}}},
  author = {Young, Alwyn},
  year = {2022},
  month = aug,
  journal = {European Economic Review},
  volume = {147},
  pages = {104112},
  issn = {0014-2921},
  doi = {10.1016/j.euroecorev.2022.104112},
  urldate = {2024-03-26},
  abstract = {I use Monte Carlo simulations, the jackknife and multiple forms of the bootstrap to study a comprehensive sample of 1309 instrumental variables regressions in 30 papers published in the journals of the American Economic Association. Monte Carlo simulations based upon published regressions show that non-iid error processes in highly leveraged regressions, both prominent features of published work, adversely affect the size and power of IV tests, while increasing the bias and mean squared error of IV relative to OLS. Weak instrument pre-tests based upon F-statistics are found to be largely uninformative of both size and bias. In published papers IV has little power as, despite producing substantively different estimates, it rarely rejects the OLS point estimate or the null that OLS is unbiased, while the statistical significance of excluded instruments is exaggerated.},
  keywords = {Bootstrap,IV,Jackknife,Literature review,Monte-Carlo},
  file = {/Users/vincentbagilet/Zotero/storage/3ADB2MV9/CWOIOnLineAppendix.pdf;/Users/vincentbagilet/Zotero/storage/J7BIBV4W/Young - 2022 - Consistency without Inference Instrumental Variab.pdf;/Users/vincentbagilet/Zotero/storage/74PYEK9Y/S001429212200054X.html}
}

@article{zhang_distributional_2024,
  title = {Distributional Effects of the Increasing Heat Incidence on Labor Productivity},
  author = {Zhang, Jingfang and Malikov, Emir and Miao, Ruiqing},
  year = {2024},
  month = apr,
  journal = {Journal of Environmental Economics and Management},
  pages = {102998},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2024.102998},
  urldate = {2024-05-02},
  abstract = {This paper examines how temperature affects worker productivity beyond the usual ``on average'' analysis, with a particular focus on distributional impacts of the increasing heat incidence across high- and low-productivity areas. Using a recentered influence function regression approach, we estimate unconditional reduced-form effects of a location shift in the temperature distribution---consistent with climate change trends---on the labor productivity distribution across counties in the contiguous U.S. We find that labor productivity is largely insensitive to changes in the frequency of cool-to-moderate maximum daily temperatures. However, as temperatures shift above 24{$\circ$}C, the effects on productivity turn increasingly negative, albeit with their magnitudes attenuating as a county's productivity rank rises. While highly productive locations in the top 5\% are not adversely impacted even by the hottest temperatures, permanently increasing the incidence of {$\geq$}36{$\circ$}C temperatures just by a day lowers productivity at the bottom vigintile by a nontrivial 0.22\% per year, an equivalent of 10.5 h of work by a minimum-wage worker. As temperatures continue to rise, not only does worker productivity worsen on average, but the cross-county dispersion therein widens too. Given existing climate forecasts, we predict that future extreme temperatures would further deepen worker productivity inequality.},
  keywords = {Heterogeneity,Inegality,Labor,Productivity,Temperature,US},
  file = {/Users/vincentbagilet/Zotero/storage/9Q28ZAPD/appendix.pdf;/Users/vincentbagilet/Zotero/storage/K9N6MGML/Zhang et al. - 2024 - Distributional effects of the increasing heat inci.pdf;/Users/vincentbagilet/Zotero/storage/32LHEKXT/S009506962400072X.html}
}

@article{zhang_temperature_2018,
  title = {Temperature Effects on Productivity and Factor Reallocation: {{Evidence}} from a Half Million Chinese Manufacturing Plants},
  shorttitle = {Temperature Effects on Productivity and Factor Reallocation},
  author = {Zhang, Peng and Deschenes, Olivier and Meng, Kyle and Zhang, Junjie},
  year = {2018},
  month = mar,
  journal = {Journal of Environmental Economics and Management},
  volume = {88},
  pages = {1--17},
  issn = {0095-0696},
  doi = {10.1016/j.jeem.2017.11.001},
  urldate = {2024-06-14},
  abstract = {This paper uses detailed production data from a half million Chinese manufacturing plants over 1998--2007 to estimate the effects of temperature on firm-level total factor productivity (TFP), factor inputs, and output. We detect an inverted U-shaped relationship between temperature and TFP and show that it primarily drives the temperature-output effect. Both labor- and capital- intensive firms exhibit sensitivity to high temperatures. By mid 21st century, if no additional adaptation were to occur, we project that climate change will reduce Chinese manufacturing output annually by 12\%, equivalent to a loss of \$39.5 billion in 2007 dollars. This implies substantial local and global economic consequences as the Chinese manufacturing sector produces 32\% of national GDP and supplies 12\% of global exports.},
  keywords = {China,Environment,Firm level,Labor,Productivity,Temperature},
  file = {/Users/vincentbagilet/Zotero/storage/HDLARXE6/Zhang et al. - 2018 - Temperature effects on productivity and factor rea.pdf;/Users/vincentbagilet/Zotero/storage/YHCP4MWL/S0095069617304588.html}
}

@article{ziliakSizeMattersStandard2004,
  title = {Size Matters: The Standard Error of Regressions in the {{American Economic Review}}},
  shorttitle = {Size Matters},
  author = {Ziliak, Stephen T. and McCloskey, Deirdre N.},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {527--546},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.024},
  urldate = {2024-01-11},
  abstract = {Significance testing as used has no theoretical justification. Our article in the Journal of Economic Literature (1996) showed that of the 182 full-length papers published in the 1980s in the American Economic and Review 70\% did not distinguish economic from statistical significance. Since 1996 many colleagues have told us that practice has improved. We interpret their response as an empirical claim, a judgment about a fact. Our colleagues, unhappily, are mistaken: significance testing is getting worse. We find here that in the next decade, the 1990s, of the 137 papers using a test of statistical significance in the AER fully 82\% mistook a merely statistically significant finding for an economically significant finding. A super majority (81\%) believed that looking at the sign of a coefficient sufficed for science, ignoring size. The mistake is causing economic damage: losses of jobs and justice, and indeed of human lives (especially in, to mention another field enchanted with statistical significance as against substantive significance, medical science). The confusion between fit and importance is causing false hypotheses to be accepted and true hypotheses to be rejected. We propose a publication standard for the future: ``Tell me the oomph of your coefficient; and do not confuse it with merely statistical significance.''},
  keywords = {American Economic Review,Regression,Significance,Standard error,Testing},
  file = {/Users/vincentbagilet/Zotero/storage/DV7EU5H3/Ziliak and McCloskey - 2004 - Size matters the standard error of regressions in.pdf;/Users/vincentbagilet/Zotero/storage/IDXJDW9K/S1053535704000836.html}
}

@misc{zwet_explainer_,
  title = {Explainer {{Shrinkage}}},
  author = {van Zwet, Eric},
  urldate = {2024-05-21},
  howpublished = {https://statmodeling.stat.columbia.edu/wp-content/uploads/2022/12/explainer.html},
  file = {/Users/vincentbagilet/Zotero/storage/TWPB3Q5U/explainer.html}
}

@article{zwet_proposal_2021,
  title = {A {{Proposal}} for {{Informative Default Priors Scaled}} by the {{Standard Error}} of {{Estimates}}},
  author = {Zwet, Erik and Gelman, Andrew},
  year = {2021},
  month = jul,
  journal = {The American Statistician},
  pages = {1--9},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.1938225},
  urldate = {2022-02-01},
  abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization or ``shrinkage.'' To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or ``corpus'' of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/53DP6UEQ/Zwet and Gelman - 2021 - A Proposal for Informative Default Priors Scaled b.pdf;/Users/vincentbagilet/Zotero/storage/IG937C2Z/2344179.pdf;/Users/vincentbagilet/Zotero/storage/T4LG9C2B/Guido W. Imbens, Donald B. Rubin - Causal Inference for Statistics, Social, and Biomedical Sciences_ An Introduction-Cambridge University Press (2015).pdf}
}

@article{zwet_proposal_2022,
  title = {A {{Proposal}} for {{Informative Default Priors Scaled}} by the {{Standard Error}} of {{Estimates}}},
  author = {van Zwet, Erik and Gelman, Andrew},
  year = {2022},
  month = jan,
  journal = {The American Statistician},
  volume = {76},
  number = {1},
  pages = {1--9},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.1938225},
  urldate = {2022-02-21},
  abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization or ``shrinkage.'' To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or ``corpus'' of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/AP2ACJWQ/Zwet and Gelman - 2022 - A Proposal for Informative Default Priors Scaled b.pdf}
}

@article{zwet_significance_2021,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink},
  author = {Zwet, Erik W. and Cator, Eric A.},
  year = {2021},
  month = nov,
  journal = {Statistica Neerlandica},
  volume = {75},
  number = {4},
  pages = {437--452},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/stan.12241},
  urldate = {2022-02-01},
  langid = {english},
  keywords = {Maths,Power,Type M/S error},
  file = {/Users/vincentbagilet/Zotero/storage/A5D4RKFU/Statistica Neerlandica - 2021 - Zwet - The significance filter the winner s curse and the need to shrink.pdf}
}

@article{zwet_statistical_2021,
  title = {The Statistical Properties of {{RCTs}} and a Proposal for Shrinkage},
  author = {Zwet, Erik and Schwab, Simon and Senn, Stephen},
  year = {2021},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {40},
  number = {27},
  pages = {6107--6117},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.9173},
  urldate = {2022-02-01},
  abstract = {We abstract the concept of a randomized controlled trial as a triple ({$B$}, b, s), where {$B$} is the primary efficacy parameter, b the estimate, and s the standard error (s {$>$} 0). If the parameter {$B$} is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z = b/s and the signal-to-noise ratio SNR = {$B$}/s from a sample of pairs (bi, si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on ({$B$}, b, s) only through the pair (z, SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13\%. We also consider the exaggeration ratio which is the factor by which the magnitude of {$B$} is overestimated. We find that if the estimate is just significant at the 5\% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
  langid = {english},
  file = {/Users/vincentbagilet/Zotero/storage/DG9VXP5B/Zwet et al. - 2021 - The statistical properties of RCTs and a proposal .pdf}
}
