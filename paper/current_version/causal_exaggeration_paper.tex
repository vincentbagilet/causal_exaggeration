\documentclass[usletter, 12pt]{article}
 
\usepackage{VB}
\usepackage{cleveref}
\usepackage{amsthm}
\usepackage{mdframed}
\usepackage{caption}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}


%\doublespacing

\begin{document}

\lhead{\textsc{Causal Exaggeration}}

\selectlanguage{english}
	
	\title{Causal Exaggeration: \\ Unconfounded but Inflated Causal Estimates}

	\author{Vincent Bagilet
		\thanks{Columbia University, New York, USA. Email: \url{vincent.bagilet@columbia.edu}. 
			A previous version of this paper (\textit{CEEP Working Paper Series}, 20) was co-authored with Léo Zabrocki-Hallak; I cannot thank him enough for his invaluable and far-reaching contributions to the project. I am very grateful to Jeffrey Shrader for his guidance and thank Sylvain Chabé-Ferret, Clément De Chaisemartin, Jesse McDevitt-Irwin, David McKenzie, José Luis Montiel Olea, Hélène Ollivier, Suresh Naidu, Claire Palandri, Julian Reif, Stephan Thies and Roberto Zuniga Valladares for helpful comments, as well as lab members at Columbia and seminars participants at Columbia, IPWSD, the Paris School of Economics and the Toulouse School of Economics.}
	}
	
	\institution{Sustainable Development PhD Program, Columbia University}

	\date{April 16, 2024}
	
	\maketitle
	
	\begin{center}
		\large \textsc{\textbf{Abstract}}\\
	\end{center}
	
	\input{abstract/causal_exaggeration_abstract.tex}
		
			
	\begin{center}
		\href{https://vincentbagilet.github.io/causal_exaggeration/causal_exaggeration_paper.pdf}{Link to the most recent version of the paper}
	\end{center}
	
	
	\newpage
	
%-----------------------------------------------------------------------------

% INTRODUCTION

%-----------------------------------------------------------------------------
	
	\section{Introduction}
	
		\input{intro/causal_exaggeration_intro_text.tex}
	
%-----------------------------------------------------------------------------

% LIT REVIEW

%-----------------------------------------------------------------------------

	\section{Causal Exaggeration in the Literature}\label{lit_review}
	
		The trade-off presented in this paper only has concrete implications if the use of causal identification strategies yields substantial exaggeration, especially as compared to the amount of confounding bias these methods allow avoiding. 
		
		The literature on the acute health effects of air pollution, that I explore in a companion paper, provides a good illustration of potential exaggeration of causal methods \citep{bagilet_accurately_2023}. The historical literature mostly relies on associations \citep{dominici_best_2017, bind_causal_2019}. A more recent literature based on causal identification strategies confirms the adverse effects of air pollution in the short term  \citep{schwartz_estimating_2015, schwartz_national_2018, deryugina_mortality_2019}. Yet, causal estimates are substantially larger than what would have been predicted by the standard epidemiology literature, some estimates being more than 10 times larger. Even within a given setting, my literature review shows that the median of the ratio of the obtained 2SLS to their corresponding ``naive'' OLS estimates is 3.8. Similarly, the data in \cite{youngConsistencyInferenceInstrumental2022} shows that in a set of 30 instrumental variables papers published in journals from the American Economic Association, the median ratio of headline IV estimates over the corresponding OLS is 2.3 and more than 5.4 for 25\% of the estimates. What can explain that causal methods yield such large effects sizes, as compared to non-causal methods? Causal strategies could arguably remove omitted variable bias, reduce attenuation bias caused by classical measurement error in air pollution exposure or target a different causal estimand. But as I argue in this section, exaggeration and imprecision could also explain part of this difference. In the example case of studies on the short-term health effects of air pollution, causal studies often display a low relative precision, not only because effects are typically small and the data relatively coarse---at the city-day level---but also because they sometimes leverage rare exogenous shocks such as public transportation strikes, take advantage of air pollution alerts to build RDD that limit sample size or use instruments that do not strongly predict air pollution levels.
				 
%		 On the other hand, in some settings, bias caused by confounders may not be as large. 
%			For instance, \cite{youngConsistencyInferenceInstrumental2022} shows that in instrumental variables papers published in journals from the American Economic Association a lack of precision makes most of the 2SLS estimates statistically indistinguishable from the corresponding OLS. \cite{weidmann_lurking_2021} documents an absence of evidence of selection bias due to unobservables in the evaluations of school programs they investigate.
%		 In situations where confounding bias is limited but the loss in statistical power resulting from the use of a causal method likely large, a traditionally biased but more precise method may lead to less overall bias, accounting for exaggeration. Regardless, the bias resulting from the use of causal methods, even 
		 
	                \subsection{Quantifying exaggeration}
	                
	                	 	As discussed in the introduction, lack of power and exaggeration are widespread in the economics literature; \cite{ioannidis_power_2017} shows that nearly 80\% of estimates published in a wide array of empirical economic literatures are exaggerated, typically by a factor of two and one-third by a factor of four or more. To focus more specifically on the causal inference literature, I first leverage data from \cite{brodeur_methods_2020}. This paper reviews of the universe of papers published in the top-25 economics journals in 2015 and 2018, to examine the extent of selection on significance in these articles, comparing across methods. It shows that IV and to a lesser extent DID are particularly subject to publication bias---one of the two ingredients of exaggeration---while RDD and RCT seem to be less prone to this issue.
			
			The exaggeration of an estimator corresponds to the expected value of the absolute value of significant estimates. As such, it depends on the true magnitude of the estimand of interest; this true effect determines the value the estimator is centred on. Exaggeration can therefore only be calculated by hypothesizing true effect sizes. 
			%Considering the wide variety of treatments and outcomes in the literature on the acute health effects of air pollution, there is a multitude of estimands and ``true effects''. 
			To circumvent this limitation, I first evaluate the proportion of studies in \cite{brodeur_methods_2020} that would have a design reliable enough to retrieve an effect size equal to half of the obtained estimate. This is done by drawing a large number of times from a normal distribution centred on this hypothetical effect size and with a standard deviation equal to the standard error of the estimate in the study and computing the average of the draws that are 1.96 standard errors away from 0. Under this assumption, the median power would be 37\% and the median exaggeration ratio of significant estimates would be 1.6. Only 11\% of studies would have a power greater than 80\%. One quarter of the studies would, on average, exaggerate the true effect sizes by a factor greater than 2.1. These results are relatively similar across methods (IV, DID, RDD and RCT). 
			
			This hypothesis on the true effect size, despite enabling to get an overview of this wide literature, suffers from an important limitation. There is no \textit{a priori} reason to believe that the magnitude of the true effect of a specific estimation would be equal to half its point estimate. However, since \cite{ioannidis_power_2017} finds a typical exaggeration of two in the economics literature, we may expect that this assumption is reasonable, on average. Regardless,  I am not claiming that it would be the true effect but rather wonder what would be the power and exaggeration under this justifiable assumption. This approach is also to some extent conservative: hypothesized effect sizes based on exaggerated estimates will be too large and will thus minimize exaggeration.
			
			To reduce the burden of assumptions, I focus on IV designs as they allow making a less arbitrary intra-study comparison, comparing the 2SLS estimates to the ``naive'' OLS one. In the literature on the short-term health effects of air pollution, assuming the true effect is close to the naive estimate, the median power of the IVs would only be 8.4\% and median exaggeration would reach 4.5. Only 2.0\% of the IV designs would have a power of 80\% to detect an effect size of the magnitude of the OLS estimate. Significant 2SLS estimates would be greatly biased. This can be explained by the fact that the IV drastically reduces precision; the median standard error of the 2SLS estimators is 3.8 times larger than the one of the corresponding OLS. %This is suggestive of causal exaggeration: a large share of the IVs reduce precision and yield likely exaggerated estimates. 
			\cite{youngConsistencyInferenceInstrumental2022} has documented this reduction in precision more broadly, showing that published IVs papers have little power. This lack of precision makes most of the 2SLS estimates statistically indistinguishable from the corresponding OLS estimate, despite the fact that 2SLS point estimates substantially differ in magnitude from their OLS counterpart, even regularly being of a different sign.
			 
	\subsection{Illustration of the trade-off}
	
		In order to limit hypotheses and to investigate causal exaggeration further, I focus on an example study, \cite{he_straw_2020}, investigating the impact of PM2.5 air pollution on mortality. I compare the bias of the ``naive'' OLS to that of the IV by computing their distance to an estimate of the ``true effect'' they target. %\footnote{I am currently carrying out the same analysis for other studies. However, variation in the treatments and outcome considered make comparisons challenging. In addition, the number of existing studies that correspond to the necessary criteria is limited (16). Computing summary statistics on such a small sample may be irrelevant, especially since the risk of exaggeration varies a lot across these studies. I thus plan to display results study by study.} %When the IV estimate differs from this ``true effect'', I explore whether exaggeration could explain this difference.
		%The ``true effect'' is the central piece of the analysis. 
		I define this ``true effect'' in two ways. First, I use the results of a meta-analysis of epidemiological studies \citep{shah_short_2015}. By pooling a number of studies carried out in various contexts, this meta-estimate might represent the average effect one may expect from such a study.  However, the studies in this meta-analysis do not rely on canonical causal identification strategies and may be thought of as suffering from counfounding. I thus consider the result of \cite{deryugina_mortality_2019}---a precise causal study that may be less exposed to exaggeration---as an alternative estimate of the true effect. This estimate may be context specific and the ``true effect'' in %a particular study 
		\cite{he_straw_2020} may deviate from these. 
		The present discussion is conditional on this true underlying effect being close to these hypothesized true effect sizes.\\
		
		\cite{he_straw_2020} finds that a ``$10 \mu g.m^{-3}$ increase in PM2.5 increases mortality by 3.25\%'' (s.e. 1.43\%). Their corresponding OLS results suggest a 0.32\% increase (s.e. 0.23\%). 
For a similar increment in air pollution, \cite{shah_short_2015} and \cite{deryugina_mortality_2019} document a 1.1\% and 1.8\% increase in mortality respectively. The OLS estimate in \cite{he_straw_2020} is closer to the ``true effect'' based on \cite{shah_short_2015} than their 2SLS estimate. Provided that the three estimands are comparable, the bias of the IV is larger than that of the OLS. If the true effect was in fact closer to the one found by \cite{deryugina_mortality_2019}, both biases would be roughly equal and the bias of the IV still substantial.
		
		Exaggeration could explain this difference. Even if the 2SLS estimator effectively removes all conventional biases, the design in \cite{he_straw_2020} would still yield exaggerated statistical significant estimates. Figure \ref{graph_he} illustrates this point. 
				
		\begin{figure}[!h]
                   	\caption{Illustration of the Confounding-Exaggeration Trade-off in \cite{he_straw_2020}}
                        \label{graph_he}
                    	\centering
                    	\includegraphics[width=0.85\linewidth]{images/graph_he_th_annotated.pdf}
                   	 \caption*{\footnotesize \textnormal{\textit{Notes}:  %10000 draws from two normal distibutions. 
	 The distribution for the 2SLS estimator is centered on the true effect, represented by the solid line and defined as the meta-estimate found in \cite{shah_short_2015}. Its variance is equal to the one of the 2SLS estimator in \cite{he_straw_2020}. The distribution for the OLS estimator is centered on the OLS estimate found in \cite{he_straw_2020} and its variance equal to that of this same estimator. The dashed and dotted lines represent the 2SLS and OLS estimates found in \cite{he_straw_2020} respectively.}}
                \end{figure}
				
		The distribution of the 2SLS estimator represented in figure \ref{graph_he} assumes that the estimator is unbiased and thus centered on the meta-estimate found in \cite{shah_short_2015}. The variance of this distribution corresponds to the variance of the 2SLS estimator found in \cite{he_straw_2020}. Due to the lack of precision of this design, the statistically significant estimates are located in the tail of the distribution and substantially exaggerate the true effect, by a factor 3.2 on average. The 2SLS estimate found in \cite{he_straw_2020} could be one of these estimates. The distribution of the OLS estimator is the one obtained by the authors. I ignore exaggeration of the OLS for clarity but since the OLS is biased downward, inflating it would yield an estimate closer to the true effect. 
				
		This example illustrates that in a published study where a causal identification strategy substantially reduces the precision of the estimator, the resulting statistically significant estimates may be further away from the true effect than the ``naive'' OLS estimate, even if the estimator is unbiased. Note that a comparable result holds if the true effect is equal to the one found in \cite{deryugina_mortality_2019}. With this design, the average exaggeration would be 3.1 times larger than the OVB (or 1.3 if the true effect is closer to the one found in \cite{deryugina_mortality_2019}).


%-----------------------------------------------------------------------------

% MATHS

%-----------------------------------------------------------------------------
	
	\section{Mathematical derivation} \label{maths}
						
		In this section, I formally prove the existence of the confounding-exaggeration trade-off  and describe its drivers in a simple setting.%\footnote{A more detailed version of this mathematical derivation is available on the \href{https://vincentbagilet.github.io/causal_exaggeration/Maths/math_causal_exaggeration.pdf}{companion website}.} 
		To do so, I first define an exaggeration ratio  and show that it increases with the variance of normally distributed biased estimators. This leads me to computing the asymptotic distributions of a series of estimators in order to prove their normality and study drivers of their variances, and ultimately of their exaggeration ratios. Finally, I show that, for any magnitude of OVB, exaggeration can be greater when using a causal inference method than the overall bias combining exaggeration and OVB in the naive regression.
		
%%%%%%%%%%%%%%  Exagg ratio  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		
		\subsection{Properties of the exaggeration ratio}\label{general_exagg}
		
			Following  \cite{gelman_beyond_2014}, we can define the exaggeration ratio $E$, as the expectation of the absolute value of significant estimates over the absolute value of the true effect. For an estimator $\hat{\beta}$ of a true effect $\beta$, with standard deviation $\sigma$ and a two-sided hypothesis test of size $\alpha$ with threshold value $z_{\alpha}$, let
			
			%Let's define an exaggeration ratio $E$, as the expected value of significant estimates over the true effect\footnote{Note that \cite{gelman_beyond_2014} define exaggeration as the expected value of the \textit{absolute value} of the estimate: $\mathbb{E}[ | \hat{\beta}| | \beta, \sigma, | \hat{\beta} | > z_{\alpha} \sigma ] / | \beta |$. Problematically, this expression, when not conditioning on significance, \textit{i.e.} $\mathbb{E}[ | \hat{\beta}| | \beta, \sigma ] / | \beta |$, is not equal to 1 for unbiased estimators, if statistical power is low. The alternative definition I use satisfies this key condition. My exaggeration ratio is also smaller or equal to \citeauthor{gelman_beyond_2014}'s and thus more conservative.}. Formally, we have:
		 
				 \begin{equation}\label{exagg_general}
				 	E(\hat{\beta}, \sigma, \beta, z_{\alpha}) =
					 	\dfrac{\mathbb{E}\left[ | \hat{\beta} | \big| \beta, \sigma, |\hat{\beta}| > z_{\alpha} \sigma \right]}{| \beta |} 
				\end{equation}
				
			\cite{lu_note_2019} and \cite{zwet_significance_2021} showed that, for given test and true effect sizes, the exaggeration ratio increases with the variance of an unbiased normally distributed estimator. We can extend this proof to biased estimators and get that:\footnote{All the proofs of the lemma and theorems are in appendix \ref{maths_proofs}.}\label{lemma_evol_exagg}
			
			\begin{lemma}
				\label{lemma_drivers}
			
				For an estimator $\hat{\beta_{b}}  \sim \mathcal{N}(\beta + b, \sigma^{2})$ of a true effect of magnitude $\beta$ and a fixed bias $b$ of the same sign as and independent from the true effect,
				
				\begin{itemize}
					\item $E$ is a decreasing function of the Signal-to-Noise Ratio (SNR), $\frac{\beta}{\sigma}$, and only depends on $\sigma$ through this SNR. 
					\item  $\lim_{\sigma\to \infty} E(\hat{\beta_{b}}, \sigma, \beta, z_{\alpha}) = +\infty$.
				\end{itemize}
			\end{lemma}
			
				Figure \ref{fig:graph_exag} provides a clear intuition for these results in the unbiased case. Note that here, we focus on cases in which the bias is in the same direction as the true effect so that exaggeration from causal inference methods and OVB do not cancel each other.\\
				 
				 Based on lemma \ref{lemma_evol_exagg}, to study how exaggeration evolves with the IV strength in an IV setting, the number of exogenous shocks in a reduced form and the correlation between the explanatory variable of interest and the omitted variable of interest, we can show asymptotic normality and study how the variances of these estimators evolve with these parameters. This relies on the assumption that the sample size is large enough so that the sample distribution of the estimator is well approximated by their asymptotic distribution.
				 				 
		\subsection{Setting and data generating process}\label{maths_dgp}
		
			Consider a usual linear homoskedastic regression model with an omitted variable. For any individual $i \in \{1, ..., n\}$, we write:
				~
				\begin{equation}\label{maths_dgp_y}
					y_i = \beta_{0} + \beta_{1}x_{i} + \delta w_{i} + u_i
				\end{equation}
				
				where $y$ is the outcome, $x$ the explanatory variable,  $w$ an unobserved omitted variable, $u$ an unobserved error term.  $(\beta_0, \beta_1, \delta) \in \mathbb{R}^{3}$ are unknown parameters. $\beta_1$ is the parameter of interest.\\
			
			Assume homogeneous treatment effects and homoskedasticity, along with the usual OLS assumptions (\textit{i.i.d.} observations, finite second moments, positive-definiteness of $\mathbb{E}[\text{x}_i \text{x}_i']$---with $\text{x}_i = (1, x_i)'$--- and $u_{i}$ conditional mean-zero and uncorrelated with $x_i$ and $w_i$). Assume that $w_{i}$ is unobserved, correlated with $x_{i}$ and that $\delta \neq 0$. To simplify the derivations, I further assume that the unobserved variable is centered, \textit{i.e.} $\mathbb{E}[w_i] = 0$. I also assume that the variance of the component of $w_{i}$ that is orthogonal to $x_{i}$ (denoted $w_{i}^{\perp x}$) does not vary with $x_{i}$, \textit{i.e.}, $\text{Var}(w_{i}^{\perp x}|x_{i}) = \text{Var}(w_{i}^{\perp x})$. Consider the following data generating process for $x_i$:
				~
				\begin{equation}\label{maths_dgp_x}
					x_i = \mu_{x} + \gamma w_{i} + \epsilon_i
				\end{equation}
				
				where $\gamma \in \mathbb{R}^{*}$ since $x$ and $w$ are correlated. Set $\rho_{xw} = \text{corr}(x, w) =  \frac{\gamma \sigma_{w}}{\sigma_{x}}$. In the IV and reduced form sections, I further assume that there exists a valid instrumental variable $z_i$ for $x_i$, \textit{i.e.} that $\mu_x + \epsilon_{i} = \pi_0 + \pi_1 z_i + e_{i}$  where $(\pi_0, \pi_1) \in \mathbb{R}^{2}$ are unknown parameters. The existence or not of this valid instrument does not affect the results in the controlled and \textsc{OVB} cases. Since the instrument is valid, it satisfies exogeneity, \textit{ie} $\mathbb{E}[\text{z}_{i}u_{i}] = 0$ and  $\mathbb{E}[z_{i}w_{i}] = 0$, relevance, \textit{ie} $\text{rank}(\mathbb{E}[\text{z}_{i}\text{x}_i']) = 2$,  and positive-definiteness of $\mathbb{E}[\text{z}_i \text{z}_i']$. The data generating process for $x_i$ becomes:			
				~
				\begin{equation}\label{maths_dgp_x_iv}
					x_i = \pi_0 + \pi_1 z_i + \gamma w_{i} + e_{i}
				\end{equation}
				
				I assume that $e_{i}$ is uncorrelated with $z_i$ and $w_{i}$, \textit{ie} $\mathbb{E}[z_ie_{i}] = 0$ and $\mathbb{E}[w_ie_{i}] = 0$. I also assume homoskedasticity for this term, such that $\mathbb{E}[e_{i}^{2} | z_{i}, w_{i}] = \sigma_{e}^{2}$ is constant.\\ %It implies that  $\mathbb{E}[e_{i}^{2} | z_{i}] = \sigma_{e}^{2}$ (since $\mathbb{E}[e_{i}^{2} | z_{i}] =  \mathbb{E}[\mathbb{E}[e_{i}^{2} | z_{i}, w_{i}] | z_{i}] = \mathbb{E}[\sigma_{e}^{2} | z_{i}] = \sigma_{e}^{2})$.\\
				
				Overall, this DGP is close to the usual textbook one but with an additional omitted variable. The Directed Acyclic Graph (DAG) in figure \ref{DAG} represents the data generating process.
			
			 \begin{figure}[!h] 
                    			\begin{center}
                    				\caption{DAG of the data generating process}
                    				\label{DAG}
                    				\includegraphics[width=0.6\linewidth]{images/DAG_maths.png}
                                   \caption*{\footnotesize \textit{Notes}: for clarity the error terms are represented in this graph, in beige. Model parameters are noted as edge labels.}
                                    \end{center}
				\vspace{-1cm}
                    		\end{figure} 

				
%%%%%%%%%%%%  ASYMPTOTIC DISTRIB  %%%%%%%%%%%%%%%%%%%%%%%%
		
		\subsection{Asymptotic distributions of the estimators}\label{maths_asymptotics}			 
			
			I now derive the asymptotic distributions of the various estimators. For each model, the goal is to show asymptotic normality and to study the evolution of the sampling distribution variances with the value of the parameter of interest, \textit{i.e.}, a measure of the correlation between $x$ and $w$ ($\gamma$) in the controlled case, of the IV strength ($\pi_{1}$) in the IV case and of the number of exogenous shocks ($\sigma_{z}^{2}$ when $z$ is a dummy) in the reduced form case. I assume that the sampling distributions are well approximated by the asymptotic distributions. %Since variables and parameters are intertwined, modifying the value of one of them can affect the value of the others. 
			In order for the variation of one factor not to impact other factors of interest, I consider the variances of the variables ($\sigma_{y}^{2}, \sigma_{x}^{2}, \sigma_{w}^{2} \text{ and } \sigma_{z}^{2}$) as fixed but adjust for the variances of the error terms ($\sigma_{u}^{2} \text{ and } \sigma_{\epsilon}^{2}$) when varying the values of one of the parameters ($\gamma, \delta \text{ and } \pi_{1}$). This corresponds to thinking in terms of shares of the variance of $x$ and $y$ explained by ``defined'' variables (\textit{i.e.}, observed variables and $w$) \textit{versus} by residuals. Finally note that comparison between cases with and without OVB for different parameter values is only relevant if varying the parameter of interest does not affect the OVB. I thus make comparative statics analyses at bias fixed, \textit{i.e.}, as shown below, for $\gamma \delta = \kappa = cst$.
			
%%%%%%%%%%%%%%  OVB  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%

			\subsubsection{Naive regression (OVB)}\label{formal_proof_ovb}
			
%				First, consider a ``naive'' regression of $y$ on $x$ (with $w$ omitted). The formula for the bias can easily computed using textbook algebra. The intuition for the formula of the main parameter of interest, the asymptotic variance, has been discussed in the introduction: as in the standard case, the variance of the estimator is given by the ratio of the variance in $y$ that is not explained by $x$ ($\sigma^{2}_{y^{\perp x}}$), over the variance of $x$ multiplied by the number of observations. The only difference with the standard case is the presence of the omitted variable $w$. From equation \ref{maths_dgp_y}, it is clear that $\sigma^{2}_{y^{\perp x}} = \sigma^{2}_{u^{\perp x}} + \delta^{2}\sigma^{2}_{w^{\perp x}}$. A more rigorous derivation described in appendix \ref{maths_proofs} leads to the following lemma:
%				%Since, $w$ is unobserved, we consider the projection of $y$ on $X$ only, where $X = (\text{x}_{1}', ..., \text{x}_{n}')'$ with $\forall i, \text{x}_{i}' = (1, x_{i})$
%%					~
%%					\begin{equation}\label{maths_eq_ovb}
%%						y = X\bm{\beta}_{\textsc{ovb}} + u_{\textsc{ovb}}
%%					\end{equation}
%%				
%%				with, by definition of the projection, $\mathbb{E}[X'u_{\textsc{ovb}}] = 0$.
%						
%				\begin{lemma}\label{lemma_ovb}
%					Based on the data generating process described in section \ref{maths_dgp}, for $\hat{\beta}_{\textsc{ovb}}$ the OLS estimate of $\beta_{1}$ in the regression of $y$ on $x$, $\hat{\beta}_{\textsc{ovb}} \overset{d}{\to} \mathcal{N}(\beta_{1} + b_{\textsc{ovb}} , \ \sigma_{\textsc{ovb}}^{2})$, with
%					\[
%						b_{\textsc{ovb}} = \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}} 
%						\qquad \text{and} \qquad
%						\sigma_{\textsc{ovb}}^{2} = \dfrac{\sigma_{u}^{2} + \delta^{2} \sigma_{w}^{2}(1 - \rho_{xw}^{2})}{n \ \sigma_{x}^{2}}
%						%\sqrt{n}\left( \hat{\beta}_{\textsc{ovb}} - \left(\beta_{1} + \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}}\right) \right)
%						 %\overset{d}{\to} \mathcal{N}\left( 0 , \ \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} (1 - \rho_{xw}^{2} )}{\sigma_{x}^{2}} \right) 
%					\]
%				\end{lemma}
%				
%			Note that since $\sigma_{x}^{2}$ and $\sigma_{w}^{2}$ are fixed, reasoning at $b_{\textsc{ovb}} = cst$ is equivalent to considering that $\gamma \delta = \kappa = cst$. Then, noting that $\forall i, u_{i} = y_i - \beta_{0} - \beta_{1}x_{i} - \delta w_{i}$ and computing its variance, we can rewrite the variance of the estimator as a function of fixed variances and one or less varying parameter:
			
				First, let us study the benchmark against which we are going to compare our causal approaches. Consider the ``naive'' regression of $y$ on $x$ (with $w$ omitted). 
				%Since, $w$ is unobserved, we consider the projection of $y$ on $X$ only, where $X = (\text{x}_{1}', ..., \text{x}_{n}')'$ with $\forall i, \text{x}_{i}' = (1, x_{i})$
%					~
%					\begin{equation}\label{maths_eq_ovb}
%						y = X\bm{\beta}_{\textsc{ovb}} + u_{\textsc{ovb}}
%					\end{equation}
%				
%				with, by definition of the projection, $\mathbb{E}[X'u_{\textsc{ovb}}] = 0$.
						
				\begin{lemma}\label{lemma_ovb}
					Based on the data generating process described in section \ref{maths_dgp}, for $\hat{\beta}_{\textsc{ovb}}$ the OLS estimate of $\beta_{1}$ in the regression of $y$ on $x$, $\hat{\beta}_{\textsc{ovb}} \overset{d}{\to} \mathcal{N}(\beta_{1} + b_{\textsc{ovb}} , \ \sigma_{\textsc{ovb}}^{2})$, with
					\[
						b_{\textsc{ovb}} = \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}} 
						\qquad \text{and} \qquad
						\sigma_{\textsc{ovb}}^{2} = \dfrac{\sigma_{u}^{2} + \delta^{2} \sigma_{w}^{2}(1 - \rho_{xw}^{2})}{n \ \sigma_{x}^{2}}
						%\sqrt{n}\left( \hat{\beta}_{\textsc{ovb}} - \left(\beta_{1} + \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}}\right) \right)
						 %\overset{d}{\to} \mathcal{N}\left( 0 , \ \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} (1 - \rho_{xw}^{2} )}{\sigma_{x}^{2}} \right) 
					\]
				\end{lemma}
				
			The intuition for the formula of the asymptotic variance has been discussed in the introduction: $\sigma_{u}^{2} + \delta^{2} \sigma_{w}(1 - \rho_{xw}^{2})$ is the part of the variance in $y$ that is not explained by $x$ ($\sigma^{2}_{y^{\perp x}}$).\\
			
			Note that, varying the parameter of interest, $\rho_{xw}$, will change the bias and $\sigma_{u}^{2}$. Since $\sigma_{x}^{2}$ and $\sigma_{w}^{2}$ are fixed, reasoning at $b_{\textsc{ovb}} = cst$ is equivalent to considering that $\gamma \delta = \kappa = const$. Then, noting that $\forall i, u_{i} = y_i - \beta_{0} - \beta_{1}x_{i} - \delta w_{i}$ and computing its variance, we can rewrite the variance of the estimator as a function of fixed variances and one or less varying parameter:
			~
			\[
				\sigma_{\textsc{ovb}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\sigma_{x}^{2} - 2\beta_{1}\kappa\sigma_{w}^{2} - \kappa^{2}\frac{\sigma_{w}^{4}}{\sigma_{x}^{2}}}{n \ \sigma_{x}^{2}}
			\]
			
			 This expression underlines that, for a given bias,  $\sigma_{\textsc{ovb}}^{2}$ does not vary with $\gamma$, or equivalently $\delta$, the parameters of interest. Applying lemma \ref{lemma_evol_exagg} proves that $E_{\textsc{ovb}}$ does not either.

%%%%%%%%%%%%%%  CTRL  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			
			\subsubsection{Controlled regression}
			
				Next, let us turn to the ``ideal'' case in which no variable is omitted, \textit{i.e.} we control for the omitted variable $w$ and thus partial out confounders. The model considered accurately represents the DGP.
%				~
%				\begin{equation}\label{maths_eq_ovb}
%					y = X_{w}\bm{\beta}_{\textsc{ctrl}} + u_{\textsc{ctrl}}
%				\end{equation}
%				
				%with $X_{w} = (\text{x}_{w, 1}', ..., \text{x}_{w, n}')'$ with $\forall i, \text{x}_{w, i}' = (1, x_{i}, w_{i})$ and $\mathbb{E}[X_{w}'u] = 0$. 
				This corresponds to the usual OLS setting with a constant and two regressors that are uncorrelated with the error: $y$ regressed on $x$ and $w$.
						
				\begin{lemma}\label{lemma_ctrl}
					Based on the data generating process mentioned previously, for $\hat{\beta}_{\textsc{ctrl}}$ the OLS estimator of $\beta_{1}$ in the regression of $y$ on $x$ and $w$, $\hat{\beta}_{\textsc{ctrl}} \overset{d}{\to} \mathcal{N}(\beta_{1}, \ \sigma_{\textsc{ctrl}}^{2})$, with
					\[
						\sigma_{\textsc{ctrl}}^{2} = \dfrac{\sigma_{u}^{2}}{n \ \sigma_{x}^{2} (1 - \rho_{xw}^{2})}
					\]
				\end{lemma}
				
				Note that $\sigma_{x}^{2} (1 - \rho_{xw}^{2})$ is the part of the variance of $x$ that is not explained by $w$ ($\sigma^{2}_{x^{\perp w}}$) and $\sigma_{u}^{2}$ the part of the variance of $y$ that is not explained by $x$ nor $w$ ($\sigma^{2}_{y^{\perp x, w}}$); here too we retrieved a result described in introduction. For a given bias, we then rewrite $\sigma_{\textsc{ctrl}}^{2}$ as a function of fixed variances and one varying parameter, $\gamma$:
				~
				\[
					\sigma_{\textsc{ctrl}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\sigma_{x}^{2} - \frac{\kappa^{2}}{\gamma^{2}}\sigma_{w}^{2} - 2\beta_{1}\kappa \sigma_{w}^{2}}{n \ (\sigma_{x}^{2}  - \gamma^{2} \sigma_{w}^{2})}
				\]
				
				Since the numerator and denominator respectively increase and decrease with $|\gamma|$,  $\sigma_{\textsc{ctrl}}^{2}$ increases with $|\gamma|$. For a given bias, the more $w$ is correlated with $x$ (and thus roughly the less it is with $y$ since $\delta \gamma = const$), the larger the variance of the estimator. In addition, we can note that, for a given bias, the variance of the estimator can be arbitrarily large since $\lim_{\gamma^{2} \to \frac{\sigma_{x}^{2}}{\sigma_{w}^{2}}} \sigma_{\textsc{ctrl}}^{2} = + \infty$.
				
			
%%%%%%%%%%%%%%  IV   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
		
		\subsubsection{Instrumental Variables}
		
			In the previous section, we considered a case in which we removed variation that included unwanted endogenous variation. We now turn to the IV, a converse situation where we select variation we want, exogenous variation. We estimate the IV model in which we regress $y$ on $\text{x}_{i} = (1, x_{i})'$ instrumented by $\text{z}_{i} = (1, z_{i})'$. We are thus in a just-identified case and $\hat{\bm{\beta}}_{\textsc{2sls}} = \hat{\bm{\beta}}_{\textsc{iv}}$.						
			\begin{lemma}\label{lemma_iv}
				Based on the data generating process mentioned above, for $\hat{\beta}_{\textsc{iv}}$ the IV estimator of $\beta_{1}$ in the regression of $y$ on $x$ instrumented by $z$,  $\hat{\beta}_{\textsc{iv}} \overset{d}{\to} \mathcal{N}(\beta_{1}, \ \sigma_{\textsc{iv}}^{2})$, with
					\[
						\sigma_{\textsc{iv}}^{2} = \dfrac{\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}}{n \ \sigma_{x}^{2} \rho_{xz}^{2}}
					\]
			\end{lemma}
			
			Note that the numerator is $\sigma_{y \perp \hat{x}}^{2}$, the part of the variance in $y$ that is not explained by $\hat{x}$, the predicted value of $x$ in the first stage and the denominator is $\sigma_{\hat{x}}^{2}$. For a given bias, noting that $\rho_{xz} = \text{corr}(x, z) = \pi_1 \frac{\sigma_z}{\sigma_x}$ and replacing $\sigma_{u}^{2}$, we can rewrite $\sigma_{\textsc{iv}}^{2}$ as a function of fixed variances and one varying parameter, $\pi_{1}$:
			\[
				\sigma_{\textsc{iv}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\sigma_{x}^{2} - 2\beta_{1}\kappa \sigma_{w}^{2}}{n \ \pi_{1}^{2} \sigma_{z}^{2}}
			\]
			
			Clearly, the smaller $\pi_{1}$, the larger $\sigma_{\textsc{iv}}^{2}$. In addition, $\lim_{\pi_{1}\to 0} \sigma_{\textsc{iv}}^{2} = + \infty$.
			

			%We retrieve the usual result regarding the determinant of the 2SLS variance as discussed in \cite{hansen_econometrics_2022} for instance. %p354
			 %The variance of the estimator increases when $\sigma_u^{2}$, the variance of the error, increases. It decreases with  $\rho_{xz}$, the correlation between $x$ and $z$, and with $\sigma_{x}^{2}$ the variance of $x$, assuming the omitted variable fixed. 
				 
%%%%%%%%%%%%%%  Reduced form. %%%%%%%%%%%%%%%%%%	

		\subsubsection{Reduced form}
		
			Let us now assume that we want to directly estimate the effect of the instrument on the outcome of interest. Plugging equation \ref{maths_dgp_x_iv} into equation \ref{maths_dgp_y} yields:			
				\[
					y_{i} = (\beta_{0} + \beta_{1}\pi_0) + (\beta_{1}\pi_{1}) z_{i} + ((\delta + \beta_{1}\gamma) w_{i} + u_{i} + \beta_{1}e_{i})
				\]
				~
				Note that if we directly regress the outcome on the instrument, the resulting estimand will be different from that of the other models. To make them comparable, we could set $\pi_{1}$ to 1 so that an increase of 1 in the instrument causes an increase of $\beta_{1}$ in $y$. Regardless of whether we make this assumption or not, regressing $y$ on $z$ corresponds to the usual univariate, unbiased case and directly gives the following result:
				
				\begin{lemma}\label{lemma_red}
					Based on the data generating process mentioned previously, for $\hat{\beta}_{\textsc{red}}$, the OLS estimator of the reduced form regression of $y$ on $z$,  $\hat{\beta}_{\textsc{red}} \overset{d}{\to} \mathcal{N}(\beta_{1}, \ \sigma_{\textsc{red}}^{2})$, with
					\[
						\sigma_{\textsc{red}}^{2} = \dfrac{\sigma_{y}^{2} - \beta_{1}^{2}\pi_{1}^{2}\sigma_{z}^{2}}{n \ \sigma_{z}^{2}}
					\]
				\end{lemma}
				
				Note that the numerator is the part of the variance of $y$ that is not explained by $z$ ($\sigma^{2}_{y^{\perp z}}$). In addition, it is clear that the smaller $\sigma_{z}^{2}$, the larger $\sigma_{\textsc{red}}^{2}$. In addition, $\lim_{\sigma_{z}\to 0} \sigma_{\textsc{red}}^{2} = + \infty$.\\
				
				In the binary case, $\sigma_{z}^{2} = p_{1}(1-p_{1})$ with $p_{1}$ the proportion of treated observations, \textit{i.e.}, the proportion of 1 in $z$. When most observations have the same treatment status, \textit{i.e.}, $p_{1}$ close to 0 or 1, $\sigma_{z}^{2}$ tends to zero and $\sigma_{\textsc{red}}^{2}$ shoots up. There is not enough variation in the treatment status to precisely identify the effect of interest. 
				
		\subsection{Exaggeration ratios}
		
			Combining the results from lemma \ref{lemma_ovb} through \ref{lemma_red}  regarding the asymptotic distribution of the various estimators with lemma \ref{lemma_evol_exagg} stating that exaggeration increases with the variance of a normally distributed estimator yields:
			~
			\begin{theorem}
				For the data generating process described in section \ref{maths_dgp}, the exaggeration ratio of the controlled, IV and reduced form estimators, respectively $E_{\textsc{ctrl}}, E_{\textsc{iv}} \text{ and } E_{\textsc{red}}$, are such that:
					\begin{itemize}
						\item $E_{\textsc{ctrl}}$ increases with the correlation between the omitted variable and the explanatory variable of interest (\textit{i.e.} $|\gamma|$ or $|\rho_{xw}|$), for a given bias,
						\item $E_{\textsc{iv}}$ decreases with the strength of the IV (\textit{i.e.} with $|\pi_{1}|$ or $|\rho_{xz}|$),
						\item $E_{\textsc{red}}$ increases when the number of exogenous shocks decreases in the binary case
					\end{itemize}
			\end{theorem}
			
			Also using the same lemma and the limit properties of the variances described in section \ref{maths_dgp}, and since, at fixed bias, $E_{\textsc{ovb}}$ does not vary with the parameters of interest, we get:
			~
			\begin{theorem}
				For the data generating process described in section \ref{maths_dgp}, $\forall \ b_{\textsc{ovb}}$,
					\begin{itemize}
						\item $\exists \ \gamma$ s.t. $E_{\textsc{ctrl}} > E_{\textsc{ovb}}$
						\item $\exists \ \pi_{1}$ s.t. $E_{\textsc{iv}} > E_{\textsc{ovb}}$
						\item $\exists \ \sigma_{z}$ s.t. $E_{\textsc{red}} > E_{\textsc{ovb}}$
					\end{itemize}
			\end{theorem}
			
			For some parameter values, statistically significant estimates can be larger on average when using a convincing causal identification strategy that eliminates the omitted variable bias than when embracing the bias and running a naive biased regression.	
	
%-----------------------------------------------------------------------------

% SIMULATIONS

%-----------------------------------------------------------------------------	
				
	\section{Simulations} \label{simulations}
		
		 %As visible in equation \ref{exagg_general},  the exaggeration ratio depends on the true effect and can therefore only be computed when this true effect is known. Since it is never the case in real-world settings, I turn to Monte-Carlo simulations.\\
    
    		To study the drivers of exaggeration in concrete settings, I build simulations that reproduce real-world examples from economics of education for RDD, %labor economics for matching, 
		political economy for IV, health economics for exogenous shocks and environmental economics for control and fixed effects approaches. I split the simulations by identification strategy. %. While the general idea that causal inference methods discard variation to identify effects is shared across strategies, the confounding-exaggeration trade-off is mediated through a distinctive channel for each of them. 
		 Real-world settings enable clearly grasping the relationships between the different variables and to set realistic parameter values, based on existing studies. I do not reproduce a specific study but instead calibrate my simulations to emulate an archetypical study from each literature to underline that causal exaggeration is not bound to specific studies. It shows that exaggeration can arise for parameter values consistent with existing studies. To check the representativity of my simulations, I compare the Signal-to-Noise Ratios (SNR) obtained in my simulations to that of studies from the corresponding literature. As underlined by lemma \ref{lemma_drivers}, the SNR is a sufficient statistic for the exaggeration ratio; a SNR consistent with the literature will ensure the representativity of the simulations with respect to the feature of interest, exaggeration. 
		 %Since these simulations have an illustrative purpose, while I choose parameter values that are realistic, I intentionally pick values from this realistic set that fall on the side of a rather low statistical power. 
		 To limit estimation challenges and focus on power aspects, the treatment effects are linear, homogeneous, the models are correctly specified and accurately represent the data generating process, except for the omitted variable. 
    
     		For each identification strategy, I start by laying out how the method enables retrieving a causal effect. It naturally points to the key parameter through which the confounding/exaggeration trade-off is mediated. I then briefly describe the example setting considered and the simulation assumptions. I finally display the simulation outputs and discuss the implications of the trade-off that are specific to the identification strategy considered. Extensively documented code for each simulation procedure is available on the \href{https://vincentbagilet.github.io/causal_exaggeration/}{project's website}.
		
%%%%%%%%%%      SIM    RDD        %%%%%%%%%%%%%%%
		
		 \subsection{Regression Discontinuity Design}
		
     			\paragraph{Intuition.} A RDD relies on the assumption that for values close to the threshold, treatment assignment is quasi-random. It focuses on observations within a certain bandwidth around this threshold and discards observations further away. The effective sample used for causal identification is thus smaller than the total sample. A smaller bandwidth and effective sample size reduce precision and can create exaggeration. Here, the confounding-exaggeration trade-off is mediated by the size of the bandwidth. 
			%To identify a causal effect, a regression discontinuity approach also prunes units that cannot be deemed comparable enough to any units with the opposite treatment status. 
			%Under this assumption, individuals just below and just above the threshold would be comparable in terms of observed and unobserved characteristics, and only differ in their treatment status. 

			\paragraph{Case-study and simulation procedure.} To illustrate this trade-off, I consider a standard application of the sharp RD design from economics of education in which students are assigned to additional lessons based on the score they obtained on a standardized test. \cite{thistlethwaite_regression-discontinuity_1960} introduced the concept of RDD using a similar type of quasi-experiment. Students with test scores below a given threshold receive the treatment while those above do not. Since students far above and far below the threshold may differ along unobserved characteristics such as ability, a RDD estimates the effect of the treatment by comparing outcomes of students whose initial test scores are immediately below and above this threshold. 
        
        			 The simulation framework is as follows. If a student $i$ has an initial scores $Qual_{i}$ below a cutoff $C$, they must take additional lessons, making the allocation of the treatment $T$ sharp: $T_i = \mathbb{I}[Qual_{i} < C]$. Both qualification and final test scores are affected by students' unobserved ability $w$ in a non-linear (cubic) way. A high or low ability has a strong positive impact on test scores while an average one does not strongly impact test scores. The qualifying test score of student $i$ is thus: $Qual_{i} = \mu_q + \gamma f(w_i) + \epsilon_{i}$, where $f$ a non-linear function (here cubic) and $\epsilon_{i} \sim \mathcal{N}(0, \sigma_{\epsilon}^{2})$ random noise. Their final test score is: $Final_{i} = \beta_{0} + \beta_{1} T_i + \eta Qual_{i} +  \delta f(w_i) + u_{i}$, where $\beta_{1}$ is the causal parameter of interest. For ability to impact qualifying and final scores similarly, I set $\delta = \gamma (1 - \eta)$.
        
        			These simulations are built and calibrated to emulate a typical study from this literature. I derive parameters of the distribution of grades from statistics from the Department of Education,  treatment effect size is based on a meta-analysis of RCTs in economics of education  \citep{kraft_interpreting_2020}. Sample and bandwidth sizes are consistent with an existing study leveraging an RDD to explore a similar question \citep{jacob_remedial_2004}. The effect of ability is built to create a large and limited bias in estimates of the treatment effect for large and small bandwidths respectively. Further details on calibration are available on the \href{https://vincentbagilet.github.io/causal_exaggeration/RDD.html#calibration-and-baseline-parameters-values}{project's website}. Given these parameters values, I generate 1000 datasets with 60,000 observations. For each dataset, I estimate the treatment effect by regressing the final score on the treatment status and the qualifying score for different bandwidth sizes. The SNRs obtained in the simulations are aligned with SNRs observed in the literature, suggesting a realistic calibration.
				
			\paragraph{Results.}  Figure \ref{graph_RDD} displays the results of these simulations.
			For large bandwidth sizes, the distribution of estimates is far away from the true effect; there is omitted variable bias. As the bandwidth decreases, the identification strategy gets rid of the OVB and the distribution of the estimates becomes centred on the true effet. At the same time, as the bandwidth and the effective sample size decrease, the distribution widens and significant estimates represent a smaller and smaller subset of the distribution, located in the tails of the distribution, creating exaggeration. %COULD REMOVE THIS IF NEED TO SHORTEN
			While the average of all estimates gets close to the true effect as bandwidth size and thus OVB decrease, in this setting the average of statistically significant estimates never gets close to the true effect. For large bandwidths, the omitted variable biases the effect while for small bandwidths, the small effective sample size creates exaggeration issues. 
			The optimal bandwidth literature describes a similar trade-off but with different consequences \citep{imbens_optimal_2012}. They consider a bias-precision trade-off, I consider an omitted variable bias-exaggeration bias trade-off. Here, the parameter mediating the trade-off can directly be adjusted in a continuous way by the researchers and the more we reduce one of these two biases, the more we increase the other. %For other methods such as IV or exogenous shocks, the issue is more dichotomized and the use of the causal identification strategy comes with a drawback.
        
       			 \begin{figure}[!h] 
				\begin{center}
					\caption{Evolution of the Bias with Bandwidth Size in Regression Discontinuity Design, conditional on significance.}
					\label{graph_RDD}
					\includegraphics[width=0.8\linewidth]{images/main_graph_RDD_paper_annotated.pdf}
		      			\caption*{\footnotesize \textit{Notes}: 1000 simulations. Significativity level: 5\%, N = 60,000. The brown lines represent the average of significant estimates. The bandwidth size is expressed as the proportion of the total number of observations in the entire sample. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_exaggeration/RDD.html}{link}.}
				\end{center}
				\vspace{-1cm}
			\end{figure} 
		

		
		
%%%%%%%%%%      SIM    Controls        %%%%%%%%%%%%%%%
		
		 \subsection{Controlling for confounders}
		
     			\paragraph{Intuition.} To identify a causal effect and avoid the risk of confounders, an ``ideal'' approach would be to partial them out by directly controlling for them. % by including additional variables in the regression model to eliminate endogenous variation. In the ideal case, we would control for the omitted variable and for this only. 
			However, as discussed in the introduction and section \ref{maths}, controlling for an additional variable may increase the variance of the estimator if it absorbs more variation in the explanatory variable of interest than in the outcome variable.
			% ($y$). Controlling for a variable ($w$) is equivalent to partialling out this variable from both $x$ and $y$. Since the variance of the OLS estimator is given by the ratio of the variance of the residuals $\sigma_{y^{\perp x, w}}^2$ over $n$ times the variance of the explanatory variable after partialling out $w$ ($\sigma_{x^{\perp w}}^2$), if controlling absorbs more of the variance in $x$ than in $y$, it will increase the variance of the resulting estimator and create exaggeration. 
			The same reasoning applies to a cornerstone of causal identification strategies, Fixed-Effects (FEs). If including FE partials out more of the variation in $x$ than in $y$, it will increase the variance of the estimator and create exaggeration. 
			
			\paragraph{Case-study and simulation procedure.} To highlight this trade-off, I consider the extreme case in which we either perfectly control for confounders or do not control for them. %For illustration purposes, I use \textit{only arbitrary numbers for now.\footnote{I will use a concrete example, as in other simulations, in the near future.}} 
			I consider a simple setting, with one outcome variable $y$, one explanatory variable $x$ and an omitted variable $w$. The data generating process is the same as described in equations \ref{maths_dgp_y} and \ref{maths_dgp_x}. As in section \ref{maths}, I reason at bias fixed and variances of the defined variables fixed, \textit{i.e.} varying the share of the variance of $x$ and $y$ that is explained by $w$.
			%IMPORTANTLY, ADDING FIXED-EFFECTS OFTEN MODIFIES THE ESTIMAND, MAKING FAKE-DATA SIMULATIONS EVEN MORE CRUCIAL IN THIS SETTING. (CAUSE IN REAL SETTING CANNOT KNOW THE TRUE EFFECT)
        
       			 \begin{figure}[!h] 
				\begin{center}
					\caption{Evolution of the Bias with the Correlation of the omitted variable with $x$ and $y$, conditional on significance.}
					\label{graph_controls}
					\includegraphics[width=0.8\linewidth]{images/main_graph_controls_paper.pdf}
		      			\caption*{\footnotesize \textit{Notes}: The blue line indicates the average bias for estimates from the control model that are statistically significant at the 5\%. The orange line represents the bias of statistically significant estimates from the model with the omitted variable. In this simulation, N = 2,000.  For now, the simulations is calibrated with arbitrary numbers but I will modify this in a later version of the project. Details on the simulation and calibration are available at this \href{https://vincentbagilet.github.io/causal_exaggeration/controls.html}{link}.}
				\end{center}
				\vspace{-1cm}
			\end{figure} 
			
			
			\paragraph{Results.}  Figure \ref{graph_controls} displays the results of these simulations. The more the unobserved variable is linked to the explanatory variable of interest as compared to the outcome variable, \textit{i.e.}, the larger the $\gamma/\delta$ ratio, the larger the exaggeration. When this ratio is large, controlling can cause exaggeration to become larger than the OVB plus exaggeration when the variable is omitted.        

        
%%%%%%%%%%      SIM    IV        %%%%%%%%%%%%%%%
		
        		\subsection{Instrumental Variables Strategy}\label{sim_IV}
            
                		\paragraph{Intuition.} 
        			Instrumental variables strategies overcome the issue of unobserved confounding by only considering the exogenous variation in the treatment, \textit{i.e.} the variation that is explained by the instrument. Even when this exogenous fraction of the variation is limited, the instrument can successfully eliminate confounding on average. However, in such cases, the IV estimator will be imprecise and statistical power low. In the case of the IV, the confounding-exaggeration trade-off is mediated by the strength of the instrument considered. The weaker the instrument, the more inflated statistically significant estimates will be.
        		
        			\paragraph{Case-study and simulation procedure.} 
        			To illustrate this trade-off and its drivers, I consider the example of the impact of voter turnout on election results. To avoid the threat of confounding in this setting, existing studies take advantage of exogenous factors such as rainfall that affect voter turnout\footnote{I abstract from potential exclusion restriction violations of this instrument and simulate it as exogenous.}. I reproduce such setting and assume that the true data generating process for the republican vote share is such that  in location $i$, $Share_{i} = \beta_{0} + \beta_{1} Turnout_{i} + \delta w_{i} + u_{i}$, where $w$ is an unobserved variable and $u \sim \mathcal{N}(0, \sigma_{u}^{2})$ some random noise. The causal parameter of interest is $\beta_{1}$. In addition, turnout is affected by the amount of rain: $Turnout_{i} = \pi_{0} + \pi_{1} Rain_{i} + \gamma w_{i} + e_{i}$, where $Rain_{i}$ is the amount of rain in location $i$ on the day of the election 
			%\footnote{In the DiD event study simulations that reproduce this setting, $Rain_{i}$ is a dummy for whether it rained or not on the day of the election in location $i$.} 
			and $e$ some random noise drawn form $\mathcal{N}(0, \sigma_{e}^{2})$. I refer to $\pi_{1}$ as the strength of the instrumental variable. 
        			
        			To make the simulations realistic, I calibrate them on existing studies. I derive sample size, distribution parameters and effect sizes from a set of studies using similar variables \citep{gomez_republicans_2007, fujiwara_habit_2016, cooperman_randomization_2017}. Details on the calibration choices are available on the \href{https://vincentbagilet.github.io/causal_exaggeration/IV.html#calibration-and-baseline-parameters-values}{project's website}. For each value of IV strength considered, I create 1000 datasets of 30,000 observations. I run both a naive OLS and a 2SLS model to estimate the impact of voter turnout on republican vote share. SNR obtained are aligned with SNR observed in this literature.
        			
        		
                    		 \begin{figure}[!h] 
                    			\begin{center}
                    				\caption{Evolution of the Bias of Statistically Significant Estimates Against Strength of the Instrument in the IV Case.}
                    				\label{graph_IV}
                    				\includegraphics[width=0.8\linewidth]{images/main_graph_IV_paper_annotated.pdf}
                                    \caption*{\footnotesize \textit{Notes}: The blue line indicates the average bias for IV estimates that are statistically significant at the 5\%. The orange line represents the bias of statistically significant OLS estimates at the 5\% level. The strength of the instrumental variable is expressed as the value of the linear parameter linking rainfall to turnout. In these simulations, N = 30,000. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_exaggeration/IV.html}{link}.}
                                    \end{center}
				\vspace{-1cm}
                    		\end{figure} 
		
		\paragraph{Results.} 
			Figure \ref{graph_IV} displays, for different IV strengths, the average of statistically significant estimates scaled by the true effect size for both the IV and the naive regression model. When the instrument is strong, the IV will recover the true effect, contrarily to the naive regression model. Yet, when the IV strength decreases, the exaggeration of statistically significant estimates skyrockets. Even if the intensity of the omitted variable bias is large, for limited IV strengths, the exaggeration ratio can become larger than the omitted variable bias. When the only available instrument is weak, using the naive regression model would, on average, produce statistically significant estimates that are closer to the true effect size than the IV. Of interest for applied research, a large $F$-statistic does not necessarily attenuate this problem. This result is aligned with limitations around the use of first-stage $F$-statistics underlined in \cite{youngConsistencyInferenceInstrumental2022}.
			%For the parameter values considered here, this phenomenon arises even in cases for which the $F$-statistic is substantially larger than the usually recommended threshold of 10, as illustrated in the \href{https://vincentbagilet.github.io/causal_exaggeration/IV.html#f-statistic-analysis}{online supplementary materials}. 
		
% I start with the RDD and matching cases because in these settings the researcher has a hand on the parameter driving power: the bandwidth size and the stringency of the matching mechanism.



%%%%%%%%%%      SIM    EVENT        %%%%%%%%%%%%%%%

		\subsection{Exogenous shocks}\label{sim_shocks}
    
        			\paragraph{Intuition.}  Taking advantage of exogenous variation in the treatment status caused by exogenous shocks or events can also enable avoiding confounding. In many settings, while the number of observations may be large, the number of events, their duration or the proportion of individuals affected might be limited. As a consequence, the number of (un)treated observations can be small and the variation available to identify the treatment limited. As extensively discussed in the randomized controlled trial literature, statistical power is maximized when the proportion of treated observations is equal to the proportion of untreated ones and drops when one of these proportions gets close to 0. In studies using discrete exogenous shocks, a confounding-exaggeration trade-off is thus mediated by the number of treated observations. %This issue does not only concern DiD event studies but is particularly salient in this case. 
			
			\paragraph{Case-study and simulation procedure.} %To illustrate this trade-off, I reproduce the analysis described for instrumental variable strategies described in section \ref{sim_IV} but considering that $Rain$ is a dummy equals to one if it rained on the day of the election.
			To illustrate this trade-off, I simulate a study of the impact of air pollution reduction on newborn weight of babies. To avoid confounding, one can exploit exogenous shocks to air pollution such as plant closures, creation of a low emission zone or of an urban toll \citep{currie_environmental_2015, lavaine_energy_2017}. I simulate such an analysis, at the zip code and monthly levels and focus on the example of toxic plant closures. I consider that the average birth weight in zip code $z$ at time period $t$, $bw_{zt}$, depends on a zip code fixed effect $\zeta_z$, a time fixed effect $\tau_t$, and the treatment status $T_{zt}$, equal to one if a plant is closed in this period and 0 otherwise. The average birth weight in zip code $z$ at time $t$ is defined as follows: $bw_{zt} = \beta_0 + \beta_1 T_{zt} + \zeta_z + \tau_t + u_{zt}$. To further simplify the identification of the effect, I assume a non-staggered treatment allocation and constant and homogenous effects. I only vary the proportion of zip codes affected by toxic plant closings, keeping the length of the closures fixed.

       I derive parameters values from studies from the literature \citep{currie_environmental_2015, lavaine_energy_2017}. The simulations mimic the design of such studies, considering a similar number of observations (150,000), distributions of variables, treatment allocation procedure and treatment effect sizes. I generate datasets with an increasing number of treated observations, varying the proportion of treated units and estimate the correct two-way fixed effects model. The simulation procedure is described in details on the \href{https://vincentbagilet.github.io/causal_exaggeration/shocks.html}{project's website}.
        
			\paragraph{Results.} Figure \ref{graph_shocks} displays the results of these simulations. Even though the actual sample size is extremely large in this example, if the number of treated observations is small, as it can be the case in this literature, exaggeration can be substantial. A very large number of observations does not necessarily shield us from exaggeration. 

			\begin{figure}[!h] 
                    			\begin{center}
                    				\caption{Evolution of Bias With the Number of Treated Observations, for Statistically Significant Estimates, in the Exogenous Shocks Case}
                    				\label{graph_shocks}
                    				\includegraphics[width=0.8\linewidth]{images/main_graph_DID_paper.pdf}
                                    \caption*{\footnotesize \textit{Notes}: Significance level: 5\%. In this simulation, N = 150,000. 1000 iterations for each number of treated observations considered. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_exaggeration/shocks.html}{link}.}
                                    \end{center}
				\vspace{-1cm}
                    		\end{figure} 
		
		
%-----------------------------------------------------------------------------

% PRACTICAL RECOMMENDATIONS

%-----------------------------------------------------------------------------

	\section{Navigating the trade-off} \label{discussion}

		% HOOK OF THE SECTION
 		In the previous sections, I argued that that using causal identification strategies induces a trade-off between avoiding confounding and exaggerating true effects. How can we, as applied researchers using observational data, arbitrate it? Since key pieces of information such as the true effect and the effect of omitted variables are inherently unknown, we cannot directly compute the biases caused by confounders and exaggeration. In this section, I examine how we can, however, get a sense of threats from both sides of the trade-off and probe its main driver, the variation used for identification. I then discuss how changing attitudes towards statistical significance and replicating studies could limit the exaggeration issue.
	
	%Even though it does not produce uninflated estimates, reporting power calculations enables to evaluate the risk of exaggeration for a study. In this section, we present a workflow to evaluate and report the power of a study before and after its implementation. 
	
		\subsection{Gauging omitted variable bias}
	
			%"MEASURING" OVB
			On one side of the trade-off lies the widely discussed bias caused by confounders. Although it is in essence impossible to measure, tools such as sensitivity analyses are available to gauge its magnitude \citep{rosenbaum_observational_2002, middleton_bias_2016, oster_unobservable_2019, cinelli_making_2020}. For instance, the method developed in \cite{cinelli_making_2020} enables assessing how strong confounders would have to be to change the estimate of the treatment effect beyond a given level we are interested in. It offers bounds for the strength of the association between the treatment and potential omitted variables by weighting it against the measured association between the treatment and observed covariates. A typical conclusion from such an analysis would be: ``omitted variables would have to explain as much residual variance of the outcome and the treatment as the observed covariate $x$ (age for instance) to bring down the estimate to a value of $\beta_{l}$''. In addition, the authors implement graphical tools to facilitate this comparison. I suggest to use such quantitative bias analyses to evaluate the restrictiveness of the causal approach required to limit the threat of unobserved confounding to acceptable levels. In settings where bias caused by confounders is likely to be low, the
			
		\subsection{Evaluating risks of exaggeration}

			On the other side of the trade-off lies the exaggeration emerging when statistical power is low. As OVB, exaggeration and statistical power are in essence impossible to measure as their computation depends on the true effect which is always unknown. Yet, power calculations can help assess them by making hypothesizes on the magnitude of the true effect. In randomized controlled trials, such computations are not only an established practice but a requirement \citep{duflo_using_2007, mcconnell_going_2015, athey_econometrics_2016}. They are, however, rarely reported in non-experimental studies. %This can be understandable if power calculations are perceived only as a way to measure a design's ability to detect an effect. 
			Yet, taking publication bias and the threat of exaggeration into account highlights the necessity of running power calculations in non-experimental studies as well. A low power or a relatively large variance not only makes it more difficult to detect an effect or to draw clear conclusions about its magnitude when detected but it can also create a bias. To avoid this bias, I advocate to make power more central to non-experimental analyses. Currently, in causal inference textbooks, very few pages are devoted to statistical power in non-experimental studies \citep{angrist_mostly_2009, angrist_mastering_2014, imbens_causal_2015, cunningham_causal_2021}. To the best of my knowledge, only two textbooks discuss the matter in depth \citep{shadish_experimental_2002, huntington-klein_effect_2021}. Results from power and exaggeration calculations would not only be highly informative but could also be reported very concisely in the robustness section of articles. 
			
			\subsubsection{Prospective power calculations}
						
				To evaluate the statistical power of a study, the risk of exaggeration and identifying the factors driving it, one can first simulate the design of the study \citep{hill_bayesian_2011, gelman_regression_2020, black_simulated_2021}. %add 
				Simulating a data generating process from scratch requires thinking about the distribution of the variables, about their relationships and can also help underline the variation used for identification. %External information found in previous studies can help guide the simulation process to make it more realistic. 
			 I implemented such Monte Carlo simulations in \Cref{simulations}. The replication material and \verb?R? code I provide can be used as an example to implement simulations for most causal identification strategies, based on data generated from scratch. In situations where the relationships among covariates are too complex to emulate, one can also start from an existing dataset and add a known treatment effect. I implemented real-data simulations in a companion paper and describe their implementation in its \href{https://vincentbagilet.github.io/inference_pollution/}{replication material} \citep{bagilet_accurately_2023}. 
			 
			 %When simulations indicate that statistical power is low, additional data could be collected or the statistical model could be improved to increase precision. In any case, it should not stop the research project from carrying out. Simulation results rest on the way the data generation process was modeled, and it can be difficult to gauge the amount of noise present in data before actually analyzing them. The two main benefits of a prospective simulation procedure are that it allows us to consider factors that may affect the statistical power of our study and to avoid drawing misleading conclusions based solely on statistically significant estimates when statistical power is low.
				
			\subsubsection{Retrospective power calculations}
			
				Running post-analysis power calculations can also help getting a sense of the statistical power associated with a research design. Such \textit{retrospective} calculations allow evaluating whether the design of the study would produce accurate and uninflated statistically significant estimates if the true effect was in fact smaller than the observed estimate \citep{gelman_beyond_2014, ioannidis_power_2017, stommes_reliability_2021}. 
				
				%can take this out if want to make the paper shorter
				I illustrate how a retrospective analysis works by taking the example of \cite{card_using_1993} on the relationship between human capital and income. He finds that an additional year of education, instrumented by the distance of growing up near a four-year college, causes a 13.2\% average increase in wage. The associated standard error is 5.5\%. Is there a risk of exaggeration with this design? Since, as noted by the author himself, the estimate is very imprecise we could expect so. If the existing literature suggests that such effects are likely to be close close to a 10\% increase in wage, we may wonder if the design in \cite{card_using_1993} would allow detecting such an effect. We can thus compute the statistical power of the study under the hypothesis of a true effect size of 10\% and for a precision of 5.5\% equal to that obtained in \cite{card_using_1993}.\footnote{\cite{timm_retrodesign_2019} and \cite{linden_retrodesign_2019} offer \texttt{R} and \texttt{Stata} packages that enable easily running these calculations through an extremely short command: \texttt{retrodesign(10, 5.5)}.} Statistically significant estimates (at the 5\% level) would on average be roughly equal to 15\%, therefore overestimating the true effect by a factor of 1.5. Statistical power, the proportion of estimates that are significant, would only be 44\%. Conditional on a 10\% true effect size being a reasonable assumption, this study would be under-powered and exaggeration substantial.
			
				The usefulness of any retrospective power analysis lies on the assumption made regarding the true effect size. To identify a range of plausible effect sizes one can rely on results from meta-analyses or from existing studies that have a credible design (\textit{e.g.}, a large randomized controlled trial).\footnote{Note that when such meta-analyses are available, one can use a Bayesian procedure to shrink statistically significant estimates based on the corpus of estimates from prior studies \cite{zwet_proposal_2021, zwet_significance_2021, zwet_statistical_2021} .} When such information is not available, power calculations can be ran for a range of smaller but credible effect sizes. Such credible effect sizes can be derived from theoretical findings. It is also possible to evaluate whether the design of our study would be sufficient to detect smaller effects than the point estimate obtained.
				
		%\subsection{Driver of the trade-off}
		
			%We have seen that the variation used for identification was the main driver of the confounding-exaggeration trade-off. This variation determines the variance of the estimator and thus the amount of exaggeration. In this section, I first discuss the impact of estimator variance and how exaggeration leads to reinterpret the well-known bias/variance trade-off. I then propose to use a tool that enables visualizing where the variation used for identification comes from.
		
		\subsubsection{Imprecision matters after obtaining a significant estimate}
			
				In non-experimental studies, estimator variance is often important to the extent that a large variance may lead to a failure to reject the null of no effect when it is incorrect. Variance is paramount until a statistically significant estimate is obtained. Yet, exaggeration underlines that variance matters, even once a significant estimate has been obtained. 
			
				 Obtaining a statistically significant estimate from an imprecise estimator should not necessarily be interpreted as a sign of ``success'' in getting significance despite a large confidence interval. It could instead be a warning that this estimate may come from the tails of the distribution and would thus inaccurately represent the true effect. Conditional on having obtained a statistically significant estimate, a limited precision can hide a bias: exaggeration. This invites to revisit the well-known bias-variance trade-off: a larger variance can also lead to a larger bias, even in (conditional)  expectation. When combined with the existing statistical significance filter, the bias-variance trade-off is in fact a bias-bias trade-off. This paper thus invites to pay attention to the implications of our design and modelling choices on the variance of our estimators, even if a large variance did not prevent us from obtaining a statistically significant estimate.
			
		\subsection{The variation used for identification}
		
				Causal inference strategies only leverage a subset of the variation to avoid confounders. However, when this subset is too small, exaggeration arises. Identifying this variation (and the observations) actually used for estimation can help navigate the confounding-exaggeration trade-off. The measure I outline here both leverages the interpretation of causal inference methods as control approaches and builds on a procedure developed in \cite{aronow_does_2016} for a different purpose: evaluating the external validity of standard regressions.
				
				\cite{aronow_does_2016} essentially interprets the estimate of the coefficient of the treatment of interest in a simple linear non-causal regression as a weighted average of individual treatment effects. The weight $w_{i}$ of individual $i$ is simply the squared difference between its treatment status $T_{i}$ and the value of this treatment status as predicted by the other covariates $X$: $w_{i} = (T_{i} - \mathbb{E}[T_{i} | X_{i}])^{2}$. If treatment effects are heterogenous, the weighting may lead some observations to be disproportionally represented in the average effect of the treatment. %Some observations, whose treatment status is very well explained by covariates do not actually contribute to the estimation of the treatment effect. 
				In that case, the average of treatment is only representative of a subset of the individual treatments, leading to external validity issues. 
				
				The parallel with my setting directly follows from this interpretation regardless of whether the treatment is heterogenous or not: observations whose treatment status is well explained by covariates do not actually contribute to the estimation of the treatment effect. This may lead to a small \textit{effective} sample size and to exaggeration. In the control approach to causal inference strategies, the more variation in the treatment is absorbed when ``controlling'' for confoundings, the smaller the effective sample, potentially leading to exaggeration. \cite{aronow_does_2016} leverages the representativity of the effective sample for fear of external validity issues. I focus on its size for fear of exaggeration.
				
				It might then seem compelling to define this effective sample by proposing a weight value under which the associated observation does not actually contribute to  identification. Yet, considering the specificity of each analysis and that exaggeration depends on several factors, including the true effect size, I instead suggest to visualize the individual weights.\footnote{In future developments of this project, I will however develop a measure of this effective sample size. I may also need to modify the weights formula in order to account for the variation in $y$ that is absorbed when controlling.}  It allows getting a sense of where the variation comes from and which are the observations that actually contribute to the estimation. Since applied economic analyses often rely on panel data, I propose to use a heatmap as a base for visualization, with time on the \textit{x}-axis and individuals on the \textit{y}-axis. If the data is geographical, one can directly plot the weights on a map.%cross sectional, the same heatmap can be computed with individuals on the \textit{x}-axis and a length-one segment on the \textit{y}-axis.\\
				
				%\textit{I still need to develop an example. The one I have, based on \cite{deschenes_economic_2007} is actually not a good example because their effect is a composite of various effects. I need to identify another paper. In addition, I need to think again about my metric because it needs to represent the ratio of the residual variance (after partialling out fixed effects) of $y$ over that of $x$.}
				
				 %I display additional graphs and share  \verb?R? code to build on the \href{https://vincentbagilet.github.io/causal_exaggeration/}{companion website}.
				
				%talk about leverage
				
	% GENERAL RECOMMENDATIONS
	\subsection{Attitude Towards Statistical Significance and Replication}
	
		Exaggeration only arises in the presence of publication bias. As shown in the simulations, if estimates were not filtered by their statistical significance, even under-powered studies would on average recover the true effect, as long as the estimator is unbiased. The exaggeration issue could therefore be addressed by tackling publication bias. 
		
		To identify broader pathways to eliminate this filtering of significant results, it is first helpful to discuss the processes that lead to statistically significant results when power and thus the probability of obtaining a significant estimate is low. In such situations, they can be obtained either by ``chance'' or as an outcome of the garden of forking paths \citep{simmons_false-positive_2011, gelman_garden_2013, kasy_forking_2021}. Forks appear at various stages along the path of research, for instance in data preparation, regarding the inclusion of a given cofntrol variable or later, regarding whether to carry on with a research that yields non-significant results. Due to the structural flaw that favors significance, the path followed may be more likely to lead to a statistically significant result. These choices are most often not the result of bad researcher practices but instead a product of a structure that portrays significant results as the end goal of research. 
		
		The issue being structural, system level changes in scientific practices could also alleviate exaggeration and the trade-off described in this paper. First, many researchers advocate abandoning statistical significance as a measure of a study's quality \citep{mcshane_abandon_2019}. To be effective, this change should be paired with an effort to replicate studies \citep{christensen_transparency_2018}. Replications, even of low powered studies, would eventually enable building the actual distribution of the causal estimand of interest. Meta-analyzes would then reduce the uncertainty around the true value of the causal estimand by pooling estimates \citep{hernan_causal_2021}. Finally, the inflation of statistically significant estimates could be limited by interpreting confidence intervals and not point estimates and thus considering these intervals as compatibility intervals \citep{shadish_experimental_2002, amrhein_inferential_2019, romer_praise_2020}. The width of such intervals gives a range of effect sizes compatible with the data. Confidence intervals will be wide in under-powered studies signaling that point estimates should not be taken at face value, even if statistically significant.

%-----------------------------------------------------------------------------

% CONCLUSION

%-----------------------------------------------------------------------------

\section{Conclusion} \label{conclusion}

	The economic literature suffers from an extensive lack of statistical power \citep{ioannidis_power_2017} and strongly favors statistically significant findings \citep[for instance]{rosenthal_file_1979, andrews_identification_2019, abadie_statistical_2020, brodeur_methods_2020}. In such situations, estimates published from underpowered studies exaggerate true effect sizes, even when the estimators are ``unbiased'' in the usual sense of $\mathbb{E}[\hat{\beta}] = \beta$ \citep{gelmanType2000, ioannidis_why_2008, gelman_beyond_2014}. It is therefore not surprising that many estimates published in economics have been shown to be considerably exaggerated \citep{camerer_evaluating_2016, ioannidis_power_2017}, despite the extensive use of convincing causal inference methods.  However, determinants for these exaggeration and power issues have remained understudied. I argue that exaggeration is exacerbated by the foundational component of causal inference: the fact that it only leverages subsets of the variation. Although causal methods enable avoiding confounding, they also reduce statistical power and thus increase the risk of exaggeration. The same aspect that makes these methods credible can create another type of bias. A systematic reporting of statistical power calculations and analysis of the variation actually used for identification could help to avoid falling into this exaggeration trap.
	
	%Add something about the fact that the variance of an estimator matters even when a statistically significant result has been obtained because a "just significant" estimate might probably be exaggerated
	%
	
%-----------------------------------------------------------------------------

% NEXT STEPS

%%-----------------------------------------------------------------------------
%
%\section{Next steps} \label{next_steps}
%
%	Here is an ordered list of the next tasks I plan to do. I would love to get feedback on both the content and the order of this list. Are there more urgent points than others? Are there other things I need to add to this list?\\
%
%	\textit{For each task, I write down what section of the paper it pertains to (S = Simulations, M = Maths, R = Recommendations, W = Writing). Question marks mean that I am not certain that this task is necessary.}\\
%
%	\begin{enumerate}
%		\item (S) Verify the SNR in my simulations: are they similar to those in the published papers?
%		\item (S) Add more visualizations
%		\item (S) Calibrate simulations for FEs/controls
%		\item (W) Add an actual example for controls: in a real data set, compare exaggeration when a given variable is used as a control or not + try to find existing papers for which adding more and more FEs increases variance
%		\item (W) Expand the description of the simulations in the body of the paper (and move matching and exogenous shocks to the appendix) + add the simulation codes (my RMarkdown files) to the appendix
%		\item (W) Add a literature review describing the extent of exaggeration in a literature (either acute health effects of air pollution, \textit{i.e.}, directly reinjecting my other paper or adding something different  based on Leo's work on interventions aimed at reducing household energy consumption or on behavioral interventions to promote household action on climate change)
%		\item (R) Add discussion on shrinkage
%		\item (S) Redo the matching simulations
%		%\item (R) Code an actual example of how one could evaluate the trade-off for their study (\textit{e.g.}, present it as if I was Card and wanted to evaluate risks of exaggeration in my study)
%		%\item (M) Code something to compare the various potential definitions of exaggeration ($\mathbb{E}[ |\hat{\beta} | | signif]$ vs $\mathbb{E}[\hat{\beta}  | signif]$, etc) and their respective limits
%		%\item (S) For exogenous shocks, add other types of treatment allocation and thus other types of estimation methods: permanent treatment staggered (event study, TWFE), temporary (reduced form) (?)
%		\item (R) Develop the visualization tool
%	\end{enumerate}


%-----------------------------------------------------------------------------

% BIBLIO

%-----------------------------------------------------------------------------
		
	
\newpage
	
\bibliographystyle{agsm}
\bibliography{../causal_exaggeration}

\newpage

\appendix

	\section{Mathematical proofs}\label{maths_proofs}
	
%%%%%%%%%% Evol exaggeration ratio %%%%%%%%%%%%%%%%

		\subsection{Variation of the exaggeration ratio (lemma \ref{lemma_evol_exagg})}
			
			\begin{proof}
				\cite{lu_note_2019} and \cite{zwet_significance_2021} showed this in the case of $b = 0$. 
				To extend it to the biased case, consider 
				$E_{b} = \frac{\mathbb{E}\left[ |\hat{\beta}_{b}| \big| \beta_{1}, \sigma, |\hat{\beta}_{b}| > z_{\alpha} \sigma \right]}{|\beta_{1}|}$ the exaggeration ratio of interest. 
				Note that, since $\hat{\beta}_{b}$ is an unbiased estimator of $\beta_{1} + b$, $\tilde{E_{b}} = \frac{\mathbb{E}\left[ |\hat{\beta}_{b}| \big| \beta_{1}, \sigma, |\hat{\beta}_{b}| > z_{\alpha} \sigma \right]}{|\beta_{1 + b}|}$ has the properties described in the lemma. 
				Now, considering that $E_{b} = \left| \frac{\beta_{1} + b}{\beta_{1}} \right| \tilde{E_{b}}$ proves the properties when $\beta_{1}$ and $b$ have the same sign.
			\end{proof}
	
%%%%%%%%%% Distrib ovb  %%%%%%%%%%%%%%%%

\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{ovb}}$ (lemma \ref{lemma_ovb})}
			
			For readability, let us introduce the usual vector notation such that for instance $y = (y_1, ..., y_n)'$ and set $\bm{\beta} = (\beta_0, \beta_1)'$ and $\text{x}_i = (1, x_i)'$. I also use capital letters to denote matrices (for instance $X = (\text{x}_{1}', ..., \text{x}_{n}')'$).\\
			
			\begin{proof} 
				Since, we do not observe $w$, we consider the projection of $y$ on $X$ only:
						~
						\begin{equation}\label{maths_eq_ovb}
							y = X\bm{\beta}_{\textsc{ovb}} + u_{\textsc{ovb}}
						\end{equation}
				
						where by definition of the projection, $\mathbb{E}[X'u_{\textsc{ovb}}] = 0$.\\
				
				We first compute the bias of the estimator. From equation  \ref{maths_eq_ovb} we get: 
				~
				\begin{equation}\label{bias_ovb}
					\begin{aligned}
            					& X'y = X'X\bm{\beta}_{\textsc{ovb}} + X'u_{\textsc{ovb}}\\
            					\Rightarrow \quad & \mathbb{E}[X'y] = \underbrace{\mathbb{E}[X'X]}_{\text{pos. def.}}\bm{\beta}_{\textsc{ovb}} + \underbrace{\mathbb{E}[X'u_{\textsc{ovb}}]}_{0} \\
            					\Leftrightarrow \quad & \bm{\beta}_{\textsc{ovb}} = \mathbb{E}[X'X]^{-1} \mathbb{E}[X'(X\bm{\beta} + \delta w + u)] & \text{cf eq. \ref{maths_dgp_y}}\\
            					\Leftrightarrow \quad & \bm{\beta}_{\textsc{ovb}} = \bm{\beta} + \mathbb{E}[X'X]^{-1} \mathbb{E}[X'w] \delta
					\end{aligned}
				\end{equation}		
				
				We then compute the asymptotic distribution. We can write: 
				~
				\begin{align*}
					\sqrt{n}(\hat{\bm{\beta}}_{\textsc{ovb}} - \bm{\beta}_{\textsc{ovb}}) &  = \left(\dfrac{1}{n} \sum_{i = 1}^{n} \text{x}_{i} \text{x}_{i}' \right)^{-1} \sqrt{n} \left(\dfrac{1}{n} \sum_{i = 1}^{n} \text{x}_{i} u_{\textsc{ovb}, i} \right)
				\end{align*}	
				
				Applying the Weak Law of Large Numbers (WLLN), the Central Limit Theorem (CLT) and Slutsky's theorem yields:
				
				\begin{equation}\label{asympt_ovb}
					\sqrt{n}(\hat{\bm{\beta}}_{\textsc{ovb}} - \bm{\beta}_{\textsc{ovb}}) \overset{d}{\to} \mathcal{N}\left(0,  \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1}\right) 
				\end{equation}
				
				
				We are interested in the second component of $\hat{\bm{\beta}}_{\textsc{ovb}}$. To retrieve it we need to compute $\mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1}$, $ \mathbb{E}[\text{x}_{i} w_{i}] $ and $\mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}]$.  
				
				\[ 
				\mathbb{E}[\text{x}_{i}\text{x}_i'] 
				= \mathbb{E} \begin{bmatrix}
					1 & x_{i}\\ 
					x_{i} & x_{i}^{2}
				\end{bmatrix} 
				=
				\begin{bmatrix}
					1 & \mu_{x}\\ 
					\mu_{x} & \sigma_{x}^{2} + \mu_{x}^2
				\end{bmatrix} 
				\quad \Rightarrow \quad
				     \mathbb{E}[\text{x}_{i}\text{x}_i']^{-1} = \dfrac{1}{\sigma_{x}^{2}} 
				     									\begin{bmatrix}
														 \sigma_{x}^{2} + \mu_{x}^2 & -\mu_{x}\\ 
														-\mu_{x} & 1
													\end{bmatrix} 
				\]
				
				\begin{equation*} 
            				\mathbb{E}[\text{x}_{i}w_i] = 
            				\mathbb{E}
            					\begin{bmatrix}
            						w_i\\ 
            						x_iw_i
            					\end{bmatrix} 
            				=
            				\begin{bmatrix}
            				    	0\\ 
            					\mathbb{E}[x_{i}]\underbrace{\mathbb{E}[w_{i}]}_{0} + \text{cov}(x_{i}, w_{i}) 
            				\end{bmatrix} 
            				= 
            				\begin{bmatrix}
            				    	0\\ 
            					\gamma  \underbrace{\text{var}(w_{i})}_{\sigma_{w}^{2}} + \underbrace{\text{cov}(\epsilon_{i}, w_{i})}_{0} 
            				\end{bmatrix} 
            				=
            				\begin{bmatrix}
            				    	0\\ 
            					\gamma \sigma_{w}^{2}
            				\end{bmatrix} 
				\end{equation*}
				
				\begin{equation}\label{bias_ovb}
					\Rightarrow \quad
					\mathbb{E}[\text{x}_{i}\text{x}_i']^{-1}\mathbb{E}[\text{x}_{i}w_i] 
					=
					\dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}}
					\begin{bmatrix}
				    		- \mu_{x} \\
						1
					\end{bmatrix} 
				\end{equation}
				
				Note that $\mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] \overset{\textsc{lie}}{=} \mathbb{E}[\text{x}_{i}\text{x}_{i}'\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]]$. We thus first compute $\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]$, noting that:
				~
				\begin{align*}
            				u_{\textsc{ovb}, i} & =  y_i - \text{x}_i'\bm{\beta}_{\textsc{ovb}} \\
            					& = \delta w_{i} + u_i + \text{x}_i' (\bm{\beta} - \bm{\beta}_{\textsc{ovb}})\\
            					& = \delta w_{i} + u_i - \underbrace{\text{x}_{i}' \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{x}_{i}w_{i}]}_{\text{projection of $w_i$ on $x_i$}} \delta\\
            					& =  u_i + \delta \underbrace{\left(w_{i} - \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})\right)}_{\text{part of $w_i$ orthogonal to $x_i$}}\\
						& = u_i + \delta w_{i}^{\perp} & \text{where } w_{i}^{\perp} = w_{i} - \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})
				\end{align*}
				
				And thus,
				\begin{align*}
            				\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}] & = \mathbb{E}[(u_i + \delta w_{i}^{\perp})^{2}| x_{i}]\\
					& = \mathbb{E}[u_{i}^{2} | x_{i}] + 2\delta \mathbb{E}[u_{i}w_{i}^{\perp} | x_{i}] + \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]\\
					& = \sigma_{u}^{2} + 2\delta \left( \mathbb{E}[u_{i}w_{i} | x_{i}] -  \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})\underbrace{\mathbb{E}[u_{i}|x_{i}]}_{0} \right) + \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]\\
					& \overset{\textsc{lie}}{=} \sigma_{u}^{2} + 2\delta \mathbb{E}[w_{i} \underbrace{\mathbb{E}[u_{i} | x_{i}, w_{i}]}_{0} | x_{i}] + \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]\\
					& = \sigma_{u}^{2} +  \delta^{2}\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}]
				\end{align*}
				
				Notice that, by the law of total variance, $\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}] = \text{Var}(w_{i}^{\perp} | x_{i}) + \mathbb{E}[w_{i}^{\perp} | x_{i}]^{2}$. Now, since $w_{i}^{\perp}$ is the component of $w_{i}$ that is orthogonal to $x_{i}$ and by the projection interpretation of the conditional variance,  $\mathbb{E}[w_{i}^{\perp} | x_{i}] = 0$. And thus, since by assumption $\text{Var}(w_{i}^{\perp} | x_{i}) = \text{Var}(w_{i}^{\perp})$, 
				~
				\begin{align*}
					\mathbb{E}[(w_{i}^{\perp})^{2} | x_{i}] & = \text{Var}(w_{i}^{\perp} | x_{i})\\
					& = \text{Var}(w_{i}^{\perp}) \\
					& = \mathbb{E}[(w_{i}^{\perp})^{2}] - \mathbb{E}[w_{i}^{\perp}]^{2}\\
					& = \mathbb{E}\left[ \left(w_{i} - \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} (x_{i} - \mu_{x})\right)^{2} \right] - \left(\underbrace{\mathbb{E}[w_{i}]}_{0} + \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} \underbrace{\mathbb{E}[x_{i} - \mu_{x}]}_{0} \right)^{2}\\
					& = \underbrace{\mathbb{E}[w_{i}^{2}]}_{\sigma_{w}^{2}} - 2 \dfrac{\gamma\sigma_{w}^{2}}{\sigma_{x}^{2}} \left( \underbrace{\mathbb{E}[x_{i}w_{i}]}_{\gamma \sigma_{w}^{2}} - \mu_{x}\underbrace{\mathbb{E}[w_{i}]}_{0} \right) + \dfrac{\gamma^{2}\sigma_{w}^{4}}{\sigma_{x}^{4}} \underbrace{\mathbb{E}[(x_{i} - \mu_{x})^{2}]}_{\sigma_{x}^{2}}\\
					& = \sigma_{w}^{2} \left(1 - \dfrac{\gamma^{2}\sigma_{w}^{2}}{\sigma_{x}^{2}} \right)
				\end{align*}
				
				Note that this variance is well defined (positive) only if $\sigma_{x}^{2} \geq \gamma^{2}\sigma_{w}^{2}$. Under this condition, 
				~
				\begin{equation}
					\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}] =  \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} \left(1 - \dfrac{\gamma^{2}\sigma_{w}^{2}}{\sigma_{x}^{2}} \right)
				\end{equation}
				
				Thus, under our set of assumptions, $\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]$ does not depend on $x_{i}$ and $\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}] =  \mathbb{E}[u_{\textsc{ovb}, i}^{2}]$. We denote this quantity $\sigma_{u_{\textsc{ovb}}}^{2}$.\\
				
				We can now compute the variance of the estimator $\hat{\bm{\beta}}_{\textsc{ovb}}$, noting that $\mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] = \mathbb{E}[\text{x}_{i}\text{x}_{i}'\mathbb{E}[u_{\textsc{ovb}, i}^{2} | \text{x}_{i}]] =  \mathbb{E}[\text{x}_{i}\text{x}_{i}' \sigma_{u_{\textsc{ovb}}}^{2}] = \sigma_{u_{\textsc{ovb}}}^{2} \mathbb{E}[\text{x}_{i}\text{x}_{i}']$. And thus $\mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{x}_{i}\text{x}_{i}'u_{\textsc{ovb}, i}^{2}] \mathbb{E}[\text{x}_{i}\text{x}_{i}']^{-1} = \sigma_{u_{\textsc{ovb}}}^{2} \mathbb{E}[\text{x}_{i}\text{x}_{i}']$.\\
				
				Plugin this and equation \ref{bias_ovb} into equation \ref{asympt_ovb}, we get, for $\hat{\beta}_{\textsc{ovb}}$, the second component of $\hat{\bm{\beta}}_{\textsc{ovb}}$:
				~
				\begin{equation*}
						\hat{\beta}_{\textsc{ovb}} \overset{d}{\to} \mathcal{N}\left( \beta_1 + \dfrac{\delta \gamma \sigma_{w}^{2}}{\sigma_{x}^{2}}, \ \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} \left(1 - \frac{\gamma^{2}\sigma_{w}^{2}}{\sigma_{x}^{2}} \right)}{n \ \sigma_{x}^{2}} \right) 
				\end{equation*}
				
				Then, noting that $\rho_{xw} =  \text{corr}(x, w) = \frac{ \text{cov}(\mu_{x} + \gamma w + \epsilon, w)}{\sigma_{x}\sigma_{w}} = \frac{\gamma\sigma_{w}}{\sigma_{x}}$, we have:
				\[
					\sigma_{\textsc{ovb}}^{2} = \text{avar}\left(\hat{\beta}_{\textsc{ovb}}\right) = \dfrac{ \sigma_{u}^{2} +  \delta^{2} \sigma_{w}^{2} \left(1 - \rho_{xw}^{2} \right)}{n \ \sigma_{x}^{2}}
				\]
				~
			\end{proof}

	
%%%%%%%%%% Distrib CTRL  %%%%%%%%%%%%%%%%

\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{ctrl}}$ (lemma \ref{lemma_ctrl})}
	
	\begin{proof}
				The proof is the well know proof of the asymptotic distribution of the OLS. I simply compute $\mathbb{E}[x_{w, i}x_{w, i}']^{-1}$ to retrieve the variance of the parameter of interest $\bm{\beta}_{\textsc{ctrl}}$. We know that we have:
				~
				\[ \sqrt{n}(\hat{\bm{\beta}}_{\textsc{ctrl}} - \bm{\beta}_{\textsc{ctrl}}) \overset{d}{\to} 
					\mathcal{N}\left(0,  \mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1} \sigma_{u}^{2} \right) \]
			
			We are interested in the second component of $\hat{\bm{\beta}}_{\textsc{ctrl}}$. To retrieve it we need to compute $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1}$. 
			~
			\[ 
				\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}'] 
				= \mathbb{E} \begin{bmatrix}
					1 & x_{i} & w\\ 
					x_{i} & x_{i}^{2} & x_{i}w_{i}\\\
					w_{i} & x_{i}w_{i} & w_{i}^{2}
				\end{bmatrix} 
				=
%				\begin{bmatrix}
%					1 & \mu_{x} & 0\\ 
%					\mu_{x} & \sigma_{x}^{2} + \mu_{x}^{2} & \mathbb{E}[xw]\\
%					0 & \mathbb{E}[xw] & \sigma_{w}^{2}
%				\end{bmatrix} 
%				=
				\begin{bmatrix}
					1 & \mu_{x} & 0\\ 
					\mu_{x} & \sigma_{x}^{2} + \mu_{x}^{2} & \gamma\sigma_{w}^{2}\\
					0 & \gamma\sigma_{w}^{2} & \sigma_{w}^{2}
				\end{bmatrix} 
				\]
				
				Note that we have $\mathbb{E}[x_{i}w_{i}] = \mathbb{E}[x_{i}]\underbrace{\mathbb{E}[w_{i}]}_{0} + \text{cov}(x_{i}, w_{i}) = \gamma  \underbrace{\text{var}(w_{i})}_{\sigma_{w}^{2}} + \underbrace{\text{cov}(\epsilon_{i}, w_{i})}_{0} = \gamma\sigma_{w}^{2}$.\\
				
				 Now, $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1} = \dfrac{1}{\text{det}(\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}'])}~^{t}\text{C}$ with C the comatrix of $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']$. We have:
				~
				\[
					\text{det}(\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']) = (\sigma_{x}^{2} + \mu_{x}^{2})\sigma_{w}^{2} -\sigma_{w}^{2}\mu_{x}^{2} - \gamma^{2}\sigma_{w}^{4} = \sigma_{w}^{2}(\sigma_{x}^{2} - \gamma^{2}\sigma_{w}^{2})
				\]
				
				and the ``central'' component of C, $\sigma_{w}^{2}$. Thus the central component of interest of $\mathbb{E}[\text{x}_{w, i}\text{x}_{w, i}']^{-1}$ is $\frac{1}{\sigma_{x}^{2} - \gamma^{2}\sigma_{w}^{2}}$. Therefore, for $\hat{\beta}_{\textsc{ctrl}}$, the second component of $\hat{\bm{\beta}}_{\textsc{ctrl}}$, we have:
				\begin{equation}
						\hat{\beta}_{\textsc{ctrl}} \overset{d}{\to}
							 \mathcal{N}\left( \beta_1 , \ \dfrac{\sigma_{u}^{2}}{n \ (\sigma_{x}^{2} - \gamma^{2}\sigma_{w}^{2})} \right) 
				\end{equation}
				
				Then, noting that $\rho_{xw} =  \text{corr}(x, w) = \frac{ \text{cov}(\mu_{x} + \gamma w + \epsilon, w)}{\sigma_{x}\sigma_{w}} = \frac{\gamma\sigma_{w}}{\sigma_{x}}$, we have:
				\[
					\sigma_{\textsc{ctrl}}^{2} = \dfrac{\sigma_{u}^{2}}{n \ \sigma_{x}^{2}(1 - \rho_{xw}^{2})}
				\]
				~
				\end{proof}
	
%%%%%%%%%% Distrib IV  %%%%%%%%%%%%%%%%

\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{iv}}$ (lemma \ref{lemma_iv})}
	
	\begin{proof}
			Since $u_{\textsc{iv}} = u_{\textsc{ovb}} = \delta w + u$, we have $\sigma_{u_{\textsc{iv}}}^2 = \sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}$. Thus, the usual asymptotic distribution of the IV gives:
				~
				\[ \sqrt{n}(\hat{\bm{\beta}}_{\textsc{iv}} - \bm{\beta}) \overset{d}{\to} 
					\mathcal{N}\left(0,  (\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}) \mathbb{E}[\text{z}_{i}\text{x}_{i}']^{-1} \mathbb{E}[\text{z}_{i}\text{z}_{i}'] \left( \mathbb{E}[\text{z}_{i}\text{x}_{i}']^{-1}\right)' \right) \]
					
			We are interested in the second component of $\hat{\bm{\beta}}_{\textsc{iv}}$. To retrieve it we need to compute $\mathbb{E}[\text{z}_{i} \text{z}_{i}]$, $\mathbb{E}[\text{x}_{i}\text{z}_{i}']^{-1}$ and its transpose.  
			~
			\[ 
				\mathbb{E}[\text{z}_{i}\text{z}_i'] 
				= \mathbb{E} \begin{bmatrix}
					1 & z_{i}\\ 
					z_{i} & z_{i}^{2}
				\end{bmatrix} 
				=
				\begin{bmatrix}
					1 & \mu_{z}\\ 
					\mu_{z} & \sigma_{z}^{2} + \mu_{z}^2
				\end{bmatrix} 
				\]
			
			\[ 
				\mathbb{E}[\text{z}_{i}\text{x}_i'] 
				=
				\begin{bmatrix}
					1 & \mathbb{E}[x_{i}]\\ 
					\mathbb{E}[z_{i}] & \mathbb{E}[z_{i}x_{i}]
				\end{bmatrix} 
				=
				\begin{bmatrix}
					1 & \pi_0 + \pi_1 \mathbb{E}[z_{i}] + \gamma \underbrace{\mathbb{E}[w_{i}]}_{0} + \underbrace{\mathbb{E}[e_{i}]}_{0}\\ 
					\mu_{z} & \pi_0 \mathbb{E}[z_{i}] + \pi_1 \mathbb{E}[z_{i}^{2}] + \gamma \underbrace{\mathbb{E}[z_{i}w_{i}]}_{0} + \underbrace{\mathbb{E}[z_{i}e_{i}]}_{0}
				\end{bmatrix} =
				\begin{bmatrix}
					1 & \pi_0 + \pi_1 \mu_{z}\\ 
					\mu_{z} & \pi_0\mu_z + \pi_1 (\sigma_{z}^{2} + \mu_{z}^{2})
				\end{bmatrix} 
			\]
			\[
				 \Rightarrow \quad
				     \mathbb{E}[\text{z}_{i}\text{x}_i']^{-1} = \dfrac{1}{\pi_1 \sigma_{z}^{2}} 
				     			\begin{bmatrix}
								 \pi_0 \mu_z + \pi_1 (\sigma_{z}^{2} + \mu_{z}^{2})& - \pi_0 - \pi_1 \mu_{z}\\ 
								 -\mu_{z} & 1
							\end{bmatrix} 
			\]
			
		Thus, 
			\[
				     \mathbb{E}[\text{z}_{i}\text{x}_i']^{-1}\mathbb{E}[\text{z}_{i}\text{z}_i']  \left( \mathbb{E}[\text{z}_{i}\text{x}_i']^{-1} \right)' 
				     		= \dfrac{1}{\pi_1 \sigma_{z}^{2}} 
				     			\begin{bmatrix}
								2 \pi_0 \mu_z + \pi_1(\sigma_{z}^{2} + \mu_{z}^{2}) + \frac{\pi_0^2}{\pi_1}&   -\mu_{z} -  \frac{\pi_0}{\pi_1} \\ 
								  -\mu_{z} -  \frac{\pi_0}{\pi_1} & \frac{1}{\pi_1}
							\end{bmatrix} 
			\]
			
			And so, for $\hat{\beta}_{\textsc{iv}}$, the second component of $\hat{\bm{\beta}}_{\textsc{iv}}$, we have:
				\begin{equation}
					\sqrt{n}\left( \hat{\beta}_{\textsc{iv}} - \beta_{1} \right)
					\overset{d}{\to}
					\mathcal{N}\left( 0 , \ \dfrac{\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}}{n \ \pi_{1}^{2}\sigma_{z}^{2}} \right) 
				\end{equation}
				~
				Now, since $ \rho_{xz} = \text{corr}(x_{i}, z_{i}) =  \frac{\text{cov}( \pi_0 + \pi_1 z_i + \gamma w_{i} + e_{i} , z_i)}{\sigma_x \sigma_z} = \pi_1 \frac{\sigma_z}{\sigma_x}$,
				~
				\[
						\sqrt{n}\left( \hat{\beta}_{\textsc{iv}} - \beta_{1} \right)
						\overset{d}{\to}
							 \mathcal{N}\left( 0 , \ \dfrac{\sigma_{u}^{2} + \delta^{2}\sigma_{w}^{2}}{\sigma_{x}^{2} \rho_{xz}^{2}} \right) 
				\]
			\end{proof}
				
%%%%%%%%%% Distrib RED  %%%%%%%%%%%%%%%%


\subsection{Asymptotic distribution of $\hat{\beta}_{\textsc{red}}$ (lemma \ref{lemma_red})}
				
			\begin{proof}
				The proof is straightforward: this is the usual univariate, unbiased case, with and error term equal to $(\delta + \beta_{1}\gamma) w_{i} + u_{i} + \beta_{1}e_{i}$. Since $w$, $u$ and $\epsilon_{\textsc{red}}$ uncorrelated, its variance is $(\delta + \beta_{1}\gamma)^{2} \sigma_{w}^{2} + \sigma_{u}^{2} + \beta_{1}^{2}\sigma_{e}^{2}$.
			\end{proof}
			
			
			
%%%%%%%%%%      SIM    Matching        %%%%%%%%%%%%%%%		
	
 	\section{Matching simulations}
		
			\paragraph{Intuition.} Another approach to retrieve a causal effect in a situation of selection on observables is to use matching. This method defines ``counterfactuals'' for treated units by picking comparable units in the untreated pool. 
			%It will retrieve the causal effect specific to matched treated units %In the ideal case where all confounders are assumed to be observed, matching can be used to estimate the causal effect specific to matched treated units. %Contrary to multivariate regression models, this method makes the common support of the data explicit, avoids model extrapolation and non-parametrically adjusts for observed confounders \citep{ho_matching_2007}. 
			In the case of propensity score matching, treated units are matched to units that would have a similar predicted probability of taking the treatment, \textit{i.e.} couple of units with a difference in propensity score lower than a critical value called the caliper. The smaller the caliper, the more comparable units have to be to be matched and therefore the lower the risk of confounding. Yet, with a stringent caliper, some units may not find a match and be pruned, decreasing the effective sample size. This can lead to a loss in statistical power and produce statistically significant estimates that are inflated. In the case of matching, the confounding-exaggeration trade-off is therefore mediated by the value of the caliper.
                
       	 	\paragraph{Case-study and simulation procedure.} I illustrate this issue by simulating a labor training program where the treatment is not randomly allocated \citep{dehejia_causal_1999}. Individuals self-select into the training program and may therefore have different characteristics from individuals who do not choose to enroll. To emulate this, I assume that the distribution of the propensity scores differ for treated and control groups: they are drawn from $\mathcal{N}(\mu_T,\sigma_T)$ and $\mathcal{N}(\mu_C, \sigma_C)$ respectively. This can be analogous to considering that matching is done based on the value of a unique covariate. Based on how these propensity scores are created, I define the potential monthly income of each individual $i$, under the treatment or not.%: $Y_i(0)=\text{Wage}\times\text{PS}_i+\mathcal{N}(\mu_{\text{u}},\sigma_{\text{u}})$. Wage is the baseline wage, PS$_{i}$ the propensity score of individual $i$ and some noise is drawn from $\mathcal{N}(\mu_{\text{N}},\sigma_{\text{N}})$. This equation makes the potential outcomes Y(0) partly different for treated and control units, creating the required common support issue. We then simulate the potential outcomes Y$_i$(1) by adding a constant treatment effect of the training program. 
        
        		Based on this simulation framework, I generate 1000 datasets for each propensity score matching procedure with caliper values ranging from 0 to 1. Parameter values of the simulation are set to make them realistic and can be found \href{https://vincentbagilet.github.io/causal_exaggeration/Matching.html}{here}. Once units are matched, I simply regress the observed revenue on the treatment indicator.
        
         \begin{figure}[!h] 
			    \caption{Evolution of Bias with the Caliper in Propensity Score Matching, Conditional on Statistical Significance.}
				\label{graph_matching}
			 \centering\includegraphics[width=0.8\linewidth]{images/main_graph_matching_paper.pdf}
			 \caption*{\footnotesize \textit{Notes}: The green line indicates the average bias for all estimates, regardless of their statistical significance. The beige line represents the inflation of statistically significant estimates at the 5\% level. The caliper is expressed in standard deviation of the propensity score distribution. Details on the simulation are available at this \href{https://vincentbagilet.github.io/causal_exaggeration/Matching.html}{link}.}
		\end{figure} 
        
        \paragraph{Results.} Figure \ref{graph_matching} indicates that the average bias of estimates, regardless of their statistical significance, decreases with the value of the caliper as units become more comparable. For large caliper values, units are not comparable enough and confoundings bias the effect. For small caliper values, they become comparable but the sample size becomes too small to allow for a precise estimation of the treatment effect and exaggeration arises. Statistically significant estimates never get close of the true effect. This imprecision, and thus exaggeration, results from the fact that the matching procedure does not use information on outcomes that would reduce the residual variance of the model but rather focuses on reducing bias arising from covariates imbalance \citep{rubin_using_2001}. 

		
\end{document}