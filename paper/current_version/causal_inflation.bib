
@article{abadie_statistical_2020,
  title = {Statistical {{Nonsignificance}} in {{Empirical Economics}}},
  author = {Abadie, Alberto},
  year = {2020},
  month = jun,
  journal = {American Economic Review: Insights},
  volume = {2},
  number = {2},
  pages = {193--208},
  issn = {2640-205X, 2640-2068},
  doi = {10.1257/aeri.20190252},
  abstract = {Statistical significance is often interpreted as providing greater information than nonsignificance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts that are common in economics, where data-sets are large and there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. Therefore, we advocate visible reporting and discussion of nonsignificant results. (JEL C12, C90)},
  langid = {english},
  keywords = {Example,NHST,Publication bias},
  file = {/Users/vincentbagilet/Documents/zotero/storage/23UWSYFW/Abadie - 2020 - Statistical Nonsignificance in Empirical Economics.pdf}
}

@article{amrhein_inferential_2019,
  title = {Inferential {{Statistics}} as {{Descriptive Statistics}}: {{There Is No Replication Crisis}} If {{We Don}}'t {{Expect Replication}}},
  shorttitle = {Inferential {{Statistics}} as {{Descriptive Statistics}}},
  author = {Amrhein, Valentin and Trafimow, David and Greenland, Sander},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {262--270},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1543137},
  abstract = {Statistical inference often fails to replicate. One reason is that many results may be selected for drawing inference because some threshold of a statistic like the P-value was crossed, leading to biased reported effect sizes. Nonetheless, considerable non-replication is to be expected even without selective reporting, and generalizations from single studies are rarely if ever warranted. Honestly reported results must vary from replication to replication because of varying assumption violations and random variation; excessive agreement itself would suggest deeper problems, such as failure to publish results in conflict with group expectations or desires. A general perception of a ``replication crisis'' may thus reflect failure to recognize that statistical tests not only test hypotheses, but countless assumptions and the entire environment in which research takes place. Because of all the uncertain and unknown assumptions that underpin statistical inferences, we should treat inferential statistics as highly unstable local descriptions of relations between assumptions and data, rather than as providing generalizable inferences about hypotheses or models. And that means we should treat statistical results as being much more incomplete and uncertain than is currently the norm. Acknowledging this uncertainty could help reduce the allure of selective reporting: Since a small P-value could be large in a replication study, and a large P-value could be small, there is simply no need to selectively report studies based on statistical results. Rather than focusing our study reports on uncertain conclusions, we should thus focus on describing accurately how the study was conducted, what problems occurred, what data were obtained, what analysis methods were used and why, and what output those methods produced.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/5ISKITED/Amrhein et al. - 2019 - Inferential Statistics as Descriptive Statistics .pdf}
}

@article{anderson-cook_experimental_2005,
  title = {Experimental and {{Quasi-Experimental Designs}} for {{Generalized Causal Inference}}},
  author = {{Anderson-Cook}, Christine M},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {708--708},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/jasa.2005.s22},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/AM4RXJGN/Anderson-Cook - 2005 - Experimental and Quasi-Experimental Designs for Ge.pdf}
}

@article{andrews_identification_2019,
  title = {Identification of and {{Correction}} for {{Publication Bias}}},
  author = {Andrews, Isaiah and Kasy, Maximilian},
  year = {2019},
  month = aug,
  journal = {American Economic Review},
  volume = {109},
  number = {8},
  pages = {2766--2794},
  issn = {0002-8282},
  doi = {10.1257/aer.20180310},
  abstract = {Some empirical results are more likely to be published than others. Selective publication leads to biased estimates and distorted inference. We propose two approaches for identifying the conditional probability of publication as a function of a study's results, the first based on systematic replication studies and the second on meta-studies. For known conditional publication probabilities, we propose bias-corrected estimators and confidence sets. We apply our methods to recent replication studies in experimental economics and psychology, and to a meta-study on the effect of the minimum wage. When replication and meta-study data are available, we find similar results from both.},
  langid = {english},
  keywords = {and Labor Costs: Public Policy,Compensation,Design of Experiments: General,Entertainment,Estimation: General,Estimation: General; Design of Experiments: General; Higher Education,Higher Education,Labor Demand,Media,Publication bias,Research Institutions,Research Institutions; Labor Demand; Wages; Compensation; and Labor Costs: Public Policy; Entertainment,Wages},
  file = {/Users/vincentbagilet/Documents/zotero/storage/88YV27KP/Andrews and Kasy - 2019 - Identification of and Correction for Publication B.pdf}
}

@article{angrist_credibility_2010,
  title = {The {{Credibility Revolution}} in {{Empirical Economics}}: {{How Better Research Design Is Taking}} the {{Con}} out of {{Econometrics}}},
  shorttitle = {The {{Credibility Revolution}} in {{Empirical Economics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = {2010},
  month = jun,
  journal = {Journal of Economic Perspectives},
  volume = {24},
  number = {2},
  pages = {3--30},
  issn = {0895-3309},
  doi = {10.1257/jep.24.2.3},
  abstract = {Since Edward Leamer's memorable 1983 paper, "Let's Take the Con out of Econometrics," empirical microeconomics has experienced a credibility revolution. While Leamer's suggested remedy, sensitivity analysis, has played a role in this, we argue that the primary engine driving improvement has been a focus on the quality of empirical research designs. The advantages of a good research design are perhaps most easily apparent in research using random assignment. We begin with an overview of Leamer's 1983 critique and his proposed remedies. We then turn to the key factors we see contributing to improved empirical work, including the availability of more and better data, along with advances in theoretical econometric understanding, but especially the fact that research design has moved front and center in much of empirical micro. We offer a brief digression into macroeconomics and industrial organization, where progress -- by our lights -- is less dramatic, although there is work in both fields that we find encouraging. Finally, we discuss the view that the design pendulum has swung too far. Critics of design-driven studies argue that in pursuit of clean and credible research designs, researchers seek good answers instead of good questions. We briefly respond to this concern, which worries us little.},
  langid = {english},
  keywords = {Econometrics,Economic Methodology},
  file = {/Users/vincentbagilet/Documents/zotero/storage/3SRYGLIT/Angrist and Pischke - 2010 - The Credibility Revolution in Empirical Economics.pdf}
}

@book{angrist_mastering_2014,
  title = {Mastering '{{Metrics}}: {{The Path}} from {{Cause}} to {{Effect}}},
  shorttitle = {Mastering '{{Metrics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = {2014},
  month = dec,
  publisher = {{Princeton University Press}},
  abstract = {From Joshua Angrist, winner of the Nobel Prize in Economics, and J\"orn-Steffen Pischke, an accessible and fun guide to the essential tools of econometric researchApplied econometrics, known to aficionados as 'metrics, is the original data science. 'Metrics encompasses the statistical methods economists use to untangle cause and effect in human affairs. Through accessible discussion and with a dose of kung fu\textendash themed humor, Mastering 'Metrics presents the essential tools of econometric research and demonstrates why econometrics is exciting and useful.The five most valuable econometric methods, or what the authors call the Furious Five--random assignment, regression, instrumental variables, regression discontinuity designs, and differences in differences--are illustrated through well-crafted real-world examples (vetted for awesomeness by Kung Fu Panda's Jade Palace). Does health insurance make you healthier? Randomized experiments provide answers. Are expensive private colleges and selective public high schools better than more pedestrian institutions? Regression analysis and a regression discontinuity design reveal the surprising truth. When private banks teeter, and depositors take their money and run, should central banks step in to save them? Differences-in-differences analysis of a Depression-era banking crisis offers a response. Could arresting O. J. Simpson have saved his ex-wife's life? Instrumental variables methods instruct law enforcement authorities in how best to respond to domestic abuse.Wielding econometric tools with skill and confidence, Mastering 'Metrics uses data and statistics to illuminate the path from cause to effect.Shows why econometrics is importantExplains econometric research through humorous and accessible discussionOutlines empirical methods central to modern econometric practiceWorks through interesting and relevant real-world examples},
  isbn = {978-1-4008-5238-3},
  langid = {english},
  keywords = {Business \& Economics / Econometrics},
  file = {/Users/vincentbagilet/Documents/zotero/storage/YUQMFUBX/Angrist and Pischke - 2015 - Mastering 'metrics the path from cause to effect.pdf}
}

@book{angrist_mostly_2009,
  title = {Mostly {{Harmless Econometrics}}: {{An Empiricist}}'s {{Companion}}},
  shorttitle = {Mostly {{Harmless Econometrics}}},
  author = {Angrist, Joshua D. and Pischke, J{\"o}rn-Steffen},
  year = {2009},
  month = jan,
  edition = {1 edition},
  publisher = {{Princeton University Press}},
  address = {{Princeton}},
  abstract = {The core methods in today's econometric toolkit are linear regression for statistical control, instrumental variables methods for the analysis of natural experiments, and differences-in-differences methods that exploit policy changes. In the modern experimentalist paradigm, these techniques address clear causal questions such as: Do smaller classes increase learning? Should wife batterers be arrested? How much does education raise wages? Mostly Harmless Econometrics shows how the basic tools of applied econometrics allow the data to speak.  In addition to econometric essentials, Mostly Harmless Econometrics covers important new extensions--regression-discontinuity designs and quantile regression--as well as how to get standard errors right. Joshua Angrist and J\"orn-Steffen Pischke explain why fancier econometric techniques are typically unnecessary and even dangerous. The applied econometric methods emphasized in this book are easy to use and relevant for many areas of contemporary social science.  An irreverent review of econometric essentials  A focus on tools that applied researchers use most  Chapters on regression-discontinuity designs, quantile regression, and standard errors  Many empirical examples  A clear and concise resource with wide applications},
  isbn = {978-0-691-12035-5},
  langid = {english},
  keywords = {DiD,Econometrics,Handbook,IV,Methods,RDD,Theory},
  file = {/Users/vincentbagilet/Documents/zotero/storage/62ZYMP6F/recrut_econometrics.pdf}
}

@techreport{aronow_does_2015,
  type = {{{SSRN Scholarly Paper}}},
  title = {Does {{Regression Produce Representative Estimates}} of {{Causal Effects}}?},
  author = {Aronow, Peter M. and Samii, Cyrus},
  year = {2015},
  month = feb,
  number = {ID 2224964},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {With an unrepresentative sample, the estimate of a causal effect may fail to characterize how effects operate in the population of interest.  What is less well understood is that conventional estimation practices for observational studies may produce the same problem even with a representative sample.  Causal effects estimated via multiple regression differentially weight each unit's contribution.  The "effective sample'' that regression uses to generate the estimate may bear little resemblance to the population of interest, and the results may be nonrepresentative in a manner similar to what quasi-experimental methods or experiments with convenience samples produce.  There is no general external validity basis for preferring multiple regression on representative samples over quasi-experimental or experimental methods.  We show how to estimate the "multiple regression weights'' that allow one to study the effective sample. We discuss alternative approaches that, under certain conditions, recover representative average causal effects.  The requisite conditions cannot always be met.},
  langid = {english},
  keywords = {Solutions,Variations},
  file = {/Users/vincentbagilet/Documents/zotero/storage/I95XZK2R/Aronow and Samii - 2015 - Does Regression Produce Representative Estimates o.pdf}
}

@article{athey_econometrics_2016,
  title = {The {{Econometrics}} of {{Randomized Experiments}}},
  author = {Athey, Susan and Imbens, Guido},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.00698 [econ, stat]},
  eprint = {1607.00698},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  abstract = {In this chapter, we present econometric and statistical methods for analyzing randomized experiments. For basic experiments we stress randomization-based inference as opposed to sampling-based inference. In randomization-based inference, uncertainty in estimates arises naturally from the random assignment of the treatments, rather than from hypothesized sampling from a large population. We show how this perspective relates to regression analyses for randomized experiments. We discuss the analyses of stratified, paired, and clustered randomized experiments, and we stress the general efficiency gains from stratification. We also discuss complications in randomized experiments such as noncompliance. In the presence of non-compliance we contrast intention-to-treat analyses with instrumental variables analyses allowing for general treatment effect heterogeneity. We consider in detail estimation and inference for heterogenous treatment effects in settings with (possibly many) covariates. These methods allow researchers to explore heterogeneity by identifying subpopulations with different treatment effects while maintaining the ability to construct valid confidence intervals. We also discuss optimal assignment to treatment based on covariates in such settings. Finally, we discuss estimation and inference in experiments in settings with interactions between units, both in general network settings and in settings where the population is partitioned into groups with all interactions contained within these groups.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62K99,Economics - Econometrics,G.3,Statistics - Methodology},
  file = {/Users/vincentbagilet/Documents/zotero/storage/BF53F535/Athey and Imbens - 2016 - The Econometrics of Randomized Experiments.pdf}
}

@misc{baker_difference--differences_2019,
  title = {Difference-in-{{Differences Methodology}}},
  author = {Baker, Andrew C.},
  year = {2019},
  month = sep,
  journal = {Andrew Baker},
  abstract = {Introduction In this methodological section I will explain the issues with difference-in-differences (DiD) designs when there are multiple units and more than two time periods, and also the particular issues that arise when the treatment is conducted at staggered periods in time. In the canonical DiD set-up (e.g.~the Card and Kreuger minimum wage study comparing New Jersey and Pennsylvania) there are two units and two time periods, with one of the units being treated in the second period.},
  howpublished = {https://andrewcbaker.netlify.app/2019/09/25/difference-in-differences-methodology/},
  langid = {american}
}

@article{bennett_how_2020,
  title = {How {{Far}} Is {{Too Far}}? {{Estimation}} of an {{Interval}} for {{Generalization}} of a {{Regression Discontinuity Design Away}} from the {{Cutoff}}},
  author = {Bennett, Magdalena},
  year = {2020},
  month = mar,
  pages = {50},
  abstract = {Regression discontinuity designs are a commonly used approach for causal inference in observational studies. Under mild continuity assumptions, the method provides a robust estimate of the average treatment effect for observations directly at the threshold of assignment. However, it has limited external validity for populations away from the cutoff. This article proposes a strategy to overcome this limitation by identifying a wider interval around the cutoff for estimation using a Generalization of a Regression Discontinuity Design (GRD). In this interval, predictive covariates are used to explain away the relationship between the assignment score and the outcome of interest for the pre-intervention period. Under the partially-testable assumption of conditional timeinvariance in absence of the treatment, the generalization bandwidth can be applied to the post-intervention period, allowing for the estimation of average treatment effects for populations away from the cutoff. To illustrate this method, GRD is applied in the context of free higher education in Chile to estimate effects for vulnerable students.},
  langid = {english},
  keywords = {RDD},
  file = {/Users/vincentbagilet/Documents/zotero/storage/2LIVM2SG/Bennett - How Far is Too Far Estimation of an Interval for .pdf;/Users/vincentbagilet/Documents/zotero/storage/3LGFRK6F/Bennett - How Far is Too Far Estimation of an Interval for .pdf;/Users/vincentbagilet/Documents/zotero/storage/FAKVUUSD/Bennett - How Far is Too Far Estimation of an Interval for .pdf;/Users/vincentbagilet/Documents/zotero/storage/N5Q7WBP9/Bennett - How Far is Too Far Estimation of an Interval for .pdf}
}

@techreport{black_simulated_2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {Simulated {{Power Analyses}} for {{Observational Studies}}: {{An Application}} to the {{Affordable Care Act Medicaid Expansion}}},
  shorttitle = {Simulated {{Power Analyses}} for {{Observational Studies}}},
  author = {Black, Bernard S. and Hollingsworth, Alex and Nunes, Leticia and Simon, Kosali Ilayperuma},
  year = {2021},
  month = mar,
  number = {ID 3368187},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3368187},
  abstract = {Power is an important factor in assessing the likely validity of a statistical estimate.An analysis with low power is unlikely to produce convincing evidence of atreatment effect even when one exists. Of greater concern, a statistically significantestimate from a low-powered analysis is likely to overstate the magnitude of the trueeffect size, often finding estimates of the wrong sign or that are several times toolarge. Yet statistical power is rarely reported in published economics work. This is inpart because modern research designs are complex enough that power cannot alwaysbe easily ascertained using simple formulae. Power can also be difficult to estimatein observational settings where researchers may not know\textemdash and have no ability tomanipulate\textemdash the true treatment effect or other parameters of interest. Using an appliedexample\textemdash the link between gaining health insurance and mortality\textemdash we conducta simulated power analysis to outline the importance of power and ways to estimatepower in complex research settings. We find that standard difference-in-differencesand triple differences analyses of Medicaid expansions using county or state mortalitydata would need to induce reductions in population mortality of at least 2\% to be wellpowered. While there is no single, correct method for conducting a simulated poweranalysis, our manuscript outlines decisions relevant for applied researchers interestedin conducting simulations appropriate to other settings.},
  langid = {english},
  keywords = {Event study},
  file = {/Users/vincentbagilet/Documents/zotero/storage/E4I7G8CN/Black et al. - 2021 - Simulated Power Analyses for Observational Studies.pdf}
}

@article{blumberg_causal_2016,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}: {{Book Reviews}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Blumberg, Carol Joyce},
  year = {2016},
  month = apr,
  journal = {International Statistical Review},
  volume = {84},
  number = {1},
  pages = {159--159},
  issn = {03067734},
  doi = {10.1111/insr.12170},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/G8CKK59D/Blumberg - 2016 - Causal Inference for Statistics, Social, and Biome.pdf}
}

@article{brewer_inference_2018,
  title = {Inference with {{Difference-in-Differences Revisited}}},
  author = {Brewer, Mike and Crossley, Thomas F. and Joyce, Robert},
  year = {2018},
  month = jan,
  journal = {Journal of Econometric Methods},
  volume = {7},
  number = {1},
  publisher = {{De Gruyter}},
  issn = {2156-6674},
  doi = {10.1515/jem-2017-0005},
  abstract = {A growing literature on inference in difference-in-differences (DiD) designs has been pessimistic about obtaining hypothesis tests of the correct size, particularly with few groups. We provide Monte Carlo evidence for four points: (i) it is possible to obtain tests of the correct size even with few groups, and in many settings very straightforward methods will achieve this; (ii) the main problem in DiD designs with grouped errors is instead low power to detect real effects; (iii) feasible GLS estimation combined with robust inference can increase power considerably whilst maintaining correct test size \textendash{} again, even with few groups, and (iv) using OLS with robust inference can lead to a perverse relationship between power and panel length.},
  langid = {english},
  keywords = {DID},
  file = {/Users/vincentbagilet/Documents/zotero/storage/BEJDCC8L/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf;/Users/vincentbagilet/Documents/zotero/storage/RTTNS8XE/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf;/Users/vincentbagilet/Documents/zotero/storage/TUGNRABU/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf;/Users/vincentbagilet/Documents/zotero/storage/X7WWSE7N/Brewer et al. - 2018 - Inference with Difference-in-Differences Revisited.pdf}
}

@article{broderick_automatic_2021,
  title = {An {{Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  shorttitle = {An {{Automatic Finite-Sample Robustness Metric}}},
  author = {Broderick, Tamara and Giordano, Ryan and Meager, Rachael},
  year = {2021},
  month = nov,
  journal = {arXiv:2011.14999 [econ, stat]},
  eprint = {2011.14999},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  abstract = {We propose a method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data. Manually checking the influence of all possible small subsets is computationally infeasible, so we provide an approximation to find the most influential subset. Our metric, the "Approximate Maximum Influence Perturbation," is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several economics papers can be overturned by removing less than 1\% of the sample.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/Users/vincentbagilet/Documents/zotero/storage/KRAKUDI9/Broderick et al. - 2021 - An Automatic Finite-Sample Robustness Metric When.pdf}
}

@article{brodeur_methods_2020,
  title = {Methods {{Matter}}: P-{{Hacking}} and {{Publication Bias}} in {{Causal Analysis}} in {{Economics}}},
  shorttitle = {Methods {{Matter}}},
  author = {Brodeur, Abel and Cook, Nikolai and Heyes, Anthony},
  year = {2020},
  month = nov,
  journal = {American Economic Review},
  volume = {110},
  number = {11},
  pages = {3634--3660},
  issn = {0002-8282},
  doi = {10.1257/aer.20190687},
  abstract = {The credibility revolution in economics has promoted causal identification using randomized control trials (RCT), difference-in-differences (DID), instrumental variables (IV) and regression discontinuity design (RDD). Applying multiple approaches to over 21,000 hypothesis tests published in 25 leading economics journals, we find that the extent of p-hacking and publication bias varies greatly by method. IV (and to a lesser extent DID) are particularly problematic. We find no evidence that (i) papers published in the Top 5 journals are different to others; (ii) the journal "revise and resubmit" process mitigates the problem; (iii) things are improving through time.},
  langid = {english},
  keywords = {and Selection,Hypothesis Testing: General,Model Evaluation,Sociology of Economics,Validation},
  file = {/Users/vincentbagilet/Documents/zotero/storage/D6VNN7BM/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf;/Users/vincentbagilet/Documents/zotero/storage/JA32AEG7/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf;/Users/vincentbagilet/Documents/zotero/storage/JDQPIVBV/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf;/Users/vincentbagilet/Documents/zotero/storage/YHKENYEL/Brodeur et al. - 2020 - Methods Matter p-Hacking and Publication Bias in .pdf}
}

@article{bunnenberg_size_nodate,
  title = {Size and Power of Difference-in-Differences Studies in Financial Economics: {{An}} Approximate Permutation Test},
  author = {Bunnenberg, Sebastian and Meyer, Steffen},
  pages = {26},
  abstract = {Researchers use difference-in-differences models to evaluate the causal effects of policy changes. As the empirical correlation across firms and time is usually unknown, estimating consistent standard errors is difficult and statistical inferences may be biased. We suggest an approximate permutation test using simulated interventions to reveal the empirical error distribution of estimated policy effects. In contrast to existing econometric corrections, such as single- or double-clustering, our approach does not impose any parametric form on the data. In comparison to alternative parametric tests, our procedure maintains correct size with simulated and real-world interventions. Simultaneously, it improves power.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/YI66AXPC/Bunnenberg and Meyer - Size and power of diﬀerence-in-diﬀerences studies .pdf}
}

@article{button_power_2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  shorttitle = {Power Failure},
  author = {Button, Katherine S. and Ioannidis, John P. A. and Mokrysz, Claire and Nosek, Brian A. and Flint, Jonathan and Robinson, Emma S. J. and Munaf{\`o}, Marcus R.},
  year = {2013},
  month = may,
  journal = {Nature Reviews Neuroscience},
  volume = {14},
  number = {5},
  pages = {365--376},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0048},
  doi = {10.1038/nrn3475},
  abstract = {Low statistical power undermines the purpose of scientific research; it reduces the chance of detecting a true effect.Perhaps less intuitively, low power also reduces the likelihood that a statistically significant result reflects a true effect.Empirically, we estimate the median statistical power of studies in the neurosciences is between {$\sim$}8\% and {$\sim$}31\%.We discuss the consequences of such low statistical power, which include overestimates of effect size and low reproducibility of results.There are ethical dimensions to the problem of low power; unreliable research is inefficient and wasteful.Improving reproducibility in neuroscience is a key priority and requires attention to well-established, but often ignored, methodological principles.We discuss how problems associated with low power can be addressed by adopting current best-practice and make clear recommendations for how to achieve this.},
  copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Molecular neuroscience},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Molecular neuroscience Subject\_term\_id: molecular-neuroscience},
  file = {/Users/vincentbagilet/Documents/zotero/storage/HTNY2I7W/Button et al. - 2013 - Power failure why small sample size undermines th.pdf}
}

@article{camerer_evaluating_2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aaf0918},
  keywords = {Experiments,Replications},
  file = {/Users/vincentbagilet/Documents/zotero/storage/8UPTEKMG/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/vincentbagilet/Documents/zotero/storage/QIQBYX9F/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/vincentbagilet/Documents/zotero/storage/V3VUUYJ6/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf;/Users/vincentbagilet/Documents/zotero/storage/XB9QNCGT/Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf}
}

@techreport{card_using_1993,
  type = {Working {{Paper}}},
  title = {Using {{Geographic Variation}} in {{College Proximity}} to {{Estimate}} the {{Return}} to {{Schooling}}},
  author = {Card, David},
  year = {1993},
  month = oct,
  series = {Working {{Paper Series}}},
  number = {4483},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w4483},
  abstract = {A convincing analysis of the causal link between schooling and earnings requires an exogenous source of variation in education outcomes. This paper explores the use of college proximity as an exogenous determinant of schooling. Analysis of the NLS Young Men Cohort reveals that men who grew up in local labor markets with a nearby college have significantly higher education and earnings than other men. The education and earnings gains are concentrated among men with poorly-educated parents \textendash{} men who would otherwise stop schooling at relatively low levels. When college proximity is taken as an exogenous determinant of schooling the implied instrumental variables estimates of the return to schooling are 25-60\% higher than conventional ordinary least squares estimates. Since the effect of a nearby college on schooling attainment varies by family background it is possible to test whether college proximity is a legitimately exogenous determinant of schooling. The results affirm that marginal returns to education among children of less-educated parents are as high and perhaps much higher than the rates of return estimated by conventional methods.},
  file = {/Users/vincentbagilet/Documents/zotero/storage/UPNNYWVP/Card - 1993 - Using Geographic Variation in College Proximity to.pdf}
}

@article{cattaneo_power_2019,
  title = {Power Calculations for Regression-Discontinuity Designs},
  author = {Cattaneo, Matias D. and Titiunik, Roc{\'i}o and {Vazquez-Bare}, Gonzalo},
  year = {2019},
  month = mar,
  journal = {The Stata Journal: Promoting communications on statistics and Stata},
  volume = {19},
  number = {1},
  pages = {210--245},
  issn = {1536-867X, 1536-8734},
  doi = {10.1177/1536867X19830919},
  abstract = {In this article, we introduce two commands, rdpow and rdsampsi, that conduct power calculations and survey sample selection when using local polynomial estimation and inference methods in regression-discontinuity designs. rdpow conducts power calculations using modern robust bias-corrected local polynomial inference procedures and allows for new hypothetical sample sizes and bandwidth selections, among other features. rdsampsi uses power calculations to compute the minimum sample size required to achieve a desired level of power, given estimated or user-supplied bandwidths, biases, and variances. Together, these commands are useful when devising new experiments or surveys in regression-discontinuity designs, which will later be analyzed using modern local polynomial techniques for estimation, inference, and falsification. Because our commands use the communitycontributed (and R) package rdrobust for the underlying bandwidths, biases, and variances estimation, all the options currently available in rdrobust can also be used for power calculations and sample-size selection, including preintervention covariate adjustment, clustered sampling, and many bandwidth selectors. Finally, we also provide companion R functions with the same syntax and capabilities.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/NPAY4N4H/Cattaneo et al. - 2019 - Power calculations for regression-discontinuity de.pdf}
}

@article{chang_is_2022,
  title = {Is {{Economics Research Replicable}}? {{Sixty Published Papers From Thirteen Journals Say}} ``{{Often Not}}''},
  shorttitle = {Is {{Economics Research Replicable}}?},
  author = {Chang, Andrew C. and Li, Phillip},
  year = {2022},
  month = jul,
  journal = {Critical Finance Review},
  volume = {11},
  publisher = {{Now Publishers, Inc.}},
  issn = {2164-5744, 2164-5760},
  doi = {10.1561/104.00000053},
  abstract = {Is Economics Research Replicable? Sixty Published Papers From Thirteen Journals Say ``Often Not''},
  langid = {english}
}

@article{christensen_transparency_2018,
  title = {Transparency, {{Reproducibility}}, and the {{Credibility}} of {{Economics Research}}},
  author = {Christensen, Garret and Miguel, Edward},
  year = {2018},
  month = sep,
  journal = {Journal of Economic Literature},
  volume = {56},
  number = {3},
  pages = {920--980},
  issn = {0022-0515},
  doi = {10.1257/jel.20171350},
  abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
  langid = {english},
  keywords = {Market for Economists; Methodological Issues: General; Higher Education,Research Institutions,Role of Economics,Role of Economists},
  file = {/Users/vincentbagilet/Documents/zotero/storage/L82UKTMB/Christensen and Miguel - 2018 - Transparency, Reproducibility, and the Credibility.pdf}
}

@article{cinelli_making_2020,
  title = {Making Sense of Sensitivity: Extending Omitted Variable Bias},
  shorttitle = {Making Sense of Sensitivity},
  author = {Cinelli, Carlos and Hazlett, Chad},
  year = {2020},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {82},
  number = {1},
  pages = {39--67},
  issn = {1467-9868},
  doi = {10.1111/rssb.12348},
  abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t-values, as well as `extreme scenarios'. Finally, we describe problems with a common `benchmarking' practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
  copyright = {\textcopyright{} 2019 Royal Statistical Society},
  langid = {english},
  keywords = {Omitted variable bias,Statistics},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12348}
}

@article{cochran_planning_2022,
  title = {The {{Planning}} of {{Observational Studies}} of {{Human Populations}}},
  author = {Cochran, W G},
  year = {2022},
  pages = {34},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/IQ56NYZX/Cochran - 2022 - The Planning of Observational Studies of Human Pop.pdf}
}

@article{cook_waiting_2008,
  title = {``{{Waiting}} for {{Life}} to {{Arrive}}'': {{A}} History of the Regression-Discontinuity Design in {{Psychology}}, {{Statistics}} and {{Economics}}},
  shorttitle = {``{{Waiting}} for {{Life}} to {{Arrive}}''},
  author = {Cook, Thomas D.},
  year = {2008},
  month = feb,
  journal = {Journal of Econometrics},
  series = {The Regression Discontinuity Design: {{Theory}} and Applications},
  volume = {142},
  number = {2},
  pages = {636--654},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2007.05.002},
  abstract = {This paper reviews the history of the regression discontinuity design in three academic disciplines. It describes the design's birth and subsequent demise in Psychology even though most problems with it had been solved there. It further describes the scant interest shown in the design by scholars formally trained in Statistics, and the design's poor reception in Economics from 1972 until about 1995, when its profile and acceptance changed. Reasons are given for this checkered history that is characterized as waiting for life to arrive.},
  langid = {english},
  keywords = {History,RDD}
}

@article{cooperman_randomization_2017,
  title = {Randomization {{Inference}} with {{Rainfall Data}}: {{Using Historical Weather Patterns}} for {{Variance Estimation}}},
  shorttitle = {Randomization {{Inference}} with {{Rainfall Data}}},
  author = {Cooperman, Alicia Dailey},
  year = {2017},
  month = jul,
  journal = {Political Analysis},
  volume = {25},
  number = {3},
  pages = {277--288},
  publisher = {{Cambridge University Press}},
  issn = {1047-1987, 1476-4989},
  doi = {10.1017/pan.2017.17},
  abstract = {Many recent papers in political science and economics use rainfall as a strategy to facilitate causal inference. Rainfall shocks are as-if randomly assigned, but the assignment of rainfall by county is highly correlated across space. Since clustered assignment does not occur within well-defined boundaries, it is challenging to estimate the variance of the effect of rainfall on political outcomes. I propose using randomization inference with historical weather patterns from 73 years as potential randomizations. I replicate the influential work on rainfall and voter turnout in presidential elections in the United States by Gomez, Hansford, and Krause (2007) and compare the estimated average treatment effect (ATE) to a sampling distribution of estimates under the sharp null hypothesis of no effect. The alternate randomizations are random draws from national rainfall patterns on election and would-be election days, which preserve the clustering in treatment assignment and eliminate the need to simulate weather patterns or make assumptions about unit boundaries for clustering. I find that the effect of rainfall on turnout is subject to greater sampling variability than previously estimated using conventional standard errors.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/AZQQ3762/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf;/Users/vincentbagilet/Documents/zotero/storage/GUD7T7L7/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf;/Users/vincentbagilet/Documents/zotero/storage/HRXBHHJP/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf;/Users/vincentbagilet/Documents/zotero/storage/VZKAHT2K/Cooperman - 2017 - Randomization Inference with Rainfall Data Using .pdf}
}

@book{cunningham_causal_2021,
  title = {Causal {{Inference}}: {{The Mixtape}}},
  shorttitle = {Causal {{Inference}}},
  author = {Cunningham, Scott},
  year = {2021},
  month = jan,
  publisher = {{Yale University Press}},
  doi = {10.2307/j.ctv1c29t27},
  isbn = {978-0-300-25588-1 978-0-300-25168-5},
  langid = {english},
  keywords = {Causal inference,Handbook,Simulations,Statistics},
  file = {/Users/vincentbagilet/Documents/zotero/storage/G8I9US2N/Cunningham - 2021 - Causal Inference The Mixtape.pdf;/Users/vincentbagilet/Documents/zotero/storage/HMURX9VR/Cunningham - 2021 - Causal Inference The Mixtape.pdf;/Users/vincentbagilet/Documents/zotero/storage/URKCJFMN/Cunningham - 2021 - Causal Inference The Mixtape.pdf;/Users/vincentbagilet/Documents/zotero/storage/YYGUW32E/Cunningham - 2021 - Causal Inference The Mixtape.pdf}
}

@article{currie_environmental_2015,
  title = {Environmental {{Health Risks}} and {{Housing Values}}: {{Evidence}} from 1,600 {{Toxic Plant Openings}} and {{Closings}}},
  shorttitle = {Environmental {{Health Risks}} and {{Housing Values}}},
  author = {Currie, Janet and Davis, Lucas and Greenstone, Michael and Walker, Reed},
  year = {2015},
  month = feb,
  journal = {American Economic Review},
  volume = {105},
  number = {2},
  pages = {678--709},
  issn = {0002-8282},
  doi = {10.1257/aer.20121656},
  abstract = {Regulatory oversight of toxic emissions from industrial plants and understanding about these emissions' impacts are in their infancy. Applying a research design based on the openings and closings of 1,600 industrial plants to rich data on housing markets and infant health, we find that: toxic air emissions affect air quality only within 1 mile of the plant; plant openings lead to 11 percent declines in housing values within 0.5 mile or a loss of about \$4.25 million for these households; and a plant's operation is associated with a roughly 3 percent increase in the probability of low birthweight within 1 mile. (JEL I12, L60, Q52, Q53, Q58, R23, R31)},
  langid = {english},
  keywords = {Distributional Effects,Employment Effects; Air Pollution,Hazardous Waste,Health Behavior; Industry Studies: Manufacturing: General; Pollution Control Adoption and Costs,Neighborhood Characteristics; Housing Supply and Markets,Noise,Population,Recycling; Environmental Economics: Government Policy; Urban; Rural; Regional; Real Estate; and Transportation Economics: Regional Migration,Regional Labor Markets,Solid Waste,Water Pollution},
  file = {/Users/vincentbagilet/Documents/zotero/storage/A4RAKRRD/Currie et al. - 2015 - Environmental Health Risks and Housing Values Evi.pdf}
}

@techreport{de_chaisemartin_two-way_2022,
  type = {Working {{Paper}}},
  title = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}: {{A Survey}}},
  shorttitle = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = jan,
  series = {Working {{Paper Series}}},
  number = {29691},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29691},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate policies' effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been show that those regressions may produce misleading estimates, if the policy's effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue, and that proposes alternative estimators robust to heterogeneous effects.},
  file = {/Users/vincentbagilet/Documents/zotero/storage/Z7NS79JN/de Chaisemartin and D'Haultfoeuille - 2022 - Two-Way Fixed Effects and Differences-in-Differenc.pdf}
}

@article{deaton_understanding_2018,
  title = {Understanding and Misunderstanding Randomized Controlled Trials},
  author = {Deaton, Angus and Cartwright, Nancy},
  year = {2018},
  month = aug,
  journal = {Social Science \& Medicine},
  series = {Randomized {{Controlled Trials}} and {{Evidence-based Policy}}: {{A Multidisciplinary Dialogue}}},
  volume = {210},
  pages = {2--21},
  issn = {0277-9536},
  doi = {10.1016/j.socscimed.2017.12.005},
  abstract = {Randomized Controlled Trials (RCTs) are increasingly popular in the social sciences, not only in medicine. We argue that the lay public, and sometimes researchers, put too much trust in RCTs over other methods of investigation. Contrary to frequent claims in the applied literature, randomization does not equalize everything other than the treatment in the treatment and control groups, it does not automatically deliver a precise estimate of the average treatment effect (ATE), and it does not relieve us of the need to think about (observed or unobserved) covariates. Finding out whether an estimate was generated by chance is more difficult than commonly believed. At best, an RCT yields an unbiased estimate, but this property is of limited practical value. Even then, estimates apply only to the sample selected for the trial, often no more than a convenience sample, and justification is required to extend the results to other groups, including any population to which the trial sample belongs, or to any individual, including an individual in the trial. Demanding `external validity' is unhelpful because it expects too much of an RCT while undervaluing its potential contribution. RCTs do indeed require minimal assumptions and can operate with little prior knowledge. This is an advantage when persuading distrustful audiences, but it is a disadvantage for cumulative scientific progress, where prior knowledge should be built upon, not discarded. RCTs can play a role in building scientific knowledge and useful predictions but they can only do so as part of a cumulative program, combining with other methods, including conceptual and theoretical development, to discover not `what works', but `why things work'.},
  langid = {english},
  keywords = {Precision,RCT,Variance/bias trade off},
  file = {/Users/vincentbagilet/Documents/zotero/storage/JUVKRPBY/Deaton and Cartwright - 2018 - Understanding and misunderstanding randomized cont.pdf}
}

@article{dehejia_causal_1999,
  title = {Causal {{Effects}} in {{Nonexperimental Studies}}: {{Reevaluating}} the {{Evaluation}} of {{Training Programs}}},
  shorttitle = {Causal {{Effects}} in {{Nonexperimental Studies}}},
  author = {Dehejia, Rajeev H. and Wahba, Sadek},
  year = {1999},
  journal = {Journal of the American Statistical Association},
  volume = {94},
  number = {448},
  pages = {1053--1062},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2669919},
  abstract = {This article uses propensity score methods to estimate the treatment impact of the National Supported Work (NSW) Demonstration, a labor training program, on postintervention earnings. We use data from Lalonde's evaluation of nonexperimental methods that combine the treated units from a randomized evaluation of the NSW with nonexperimental comparison units drawn from survey datasets. We apply propensity score methods to this composite dataset and demonstrate that, relative to the estimators that Lalonde evaluates, propensity score estimates of the treatment impact are much closer to the experimental benchmark estimate. Propensity score methods assume that the variables associated with assignment to treatment are observed (referred to as ignorable treatment assignment, or selection on observables). Even under this assumption, it is difficult to control for differences between the treatment and comparison groups when they are dissimilar and when there are many preintervention variables. The estimated propensity score (the probability of assignment to treatment, conditional on preintervention variables) summarizes the preintervention variables. This offers a diagnostic on the comparability of the treatment and comparison groups, because one has only to compare the estimated propensity score across the two groups. We discuss several methods (such as stratification and matching) that use the propensity score to estimate the treatment impact. When the range of estimated propensity scores of the treatment and comparison groups overlap, these methods can estimate the treatment impact for the treatment group. A sensitivity analysis shows that our estimates are not sensitive to the specification of the estimated propensity score, but are sensitive to the assumption of selection on observables. We conclude that when the treatment and comparison groups overlap, and when the variables determining assignment to treatment are observed, these methods provide a means to estimate the treatment impact. Even though propensity score methods are not always applicable, they offer a diagnostic on the quality of nonexperimental comparison groups in terms of observable preintervention variables.},
  file = {/Users/vincentbagilet/Documents/zotero/storage/XQ2JHVXI/Dehejia and Wahba - 1999 - Causal Effects in Nonexperimental Studies Reevalu.pdf}
}

@incollection{duflo_using_2007,
  title = {Using {{Randomization}} in {{Development Economics Research}}: {{A Toolkit}}},
  shorttitle = {Chapter 61 {{Using Randomization}} in {{Development Economics Research}}},
  booktitle = {Handbook of {{Development Economics}}},
  author = {Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  editor = {Schultz, T. Paul and Strauss, John A.},
  year = {2007},
  month = jan,
  volume = {4},
  pages = {3895--3962},
  publisher = {{Elsevier}},
  doi = {10.1016/S1573-4471(07)04061-2},
  abstract = {This paper is a practical guide (a toolkit) for researchers, students and practitioners wishing to introduce randomization as part of a research design in the field. It first covers the rationale for the use of randomization, as a solution to selection bias and a partial solution to publication biases. Second, it discusses various ways in which randomization can be practically introduced in a field settings. Third, it discusses designs issues such as sample size requirements, stratification, level of randomization and data collection methods. Fourth, it discusses how to analyze data from randomized evaluations when there are departures from the basic framework. It reviews in particular how to handle imperfect compliance and externalities. Finally, it discusses some of the issues involved in drawing general conclusions from randomized evaluations, including the necessary use of theory as a guide when designing evaluations and interpreting results.},
  langid = {english},
  keywords = {development,experiments,program evaluation,randomized evaluations}
}

@article{duflo_using_nodate,
  title = {{{USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEARCH}}: {{A TOOLKIT}}},
  author = {Duflo, Esther and Glennerster, Rachel and Kremer, Michael},
  pages = {92},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/R5ZEKHVL/Duflo et al. - USING RANDOMIZATION IN DEVELOPMENT ECONOMICS RESEA}
}

@article{ferman_inference_2019,
  title = {Inference in {{Differences-in-Differences}} with {{Few Treated Groups}} and {{Heteroskedasticity}}},
  author = {Ferman, Bruno and Pinto, Cristine},
  year = {2019},
  month = jul,
  journal = {The Review of Economics and Statistics},
  volume = {101},
  number = {3},
  pages = {452--467},
  issn = {0034-6535},
  doi = {10.1162/rest_a_00759},
  abstract = {We derive an inference method that works in differences-in-differences settings with few treated and many control groups in the presence of heteroskedasticity. As a leading example, we provide theoretical justification and empirical evidence that heteroskedasticity generated by variation in group sizes can invalidate existing inference methods, even in data sets with a large number of observations per group. In contrast, our inference method remains valid in this case. Our test can also be combined with feasible generalized least squares, providing a safeguard against misspecification of the serial correlation.},
  keywords = {DID},
  file = {/Users/vincentbagilet/Documents/zotero/storage/C9R8BSER/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf;/Users/vincentbagilet/Documents/zotero/storage/DQSVLHSF/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf;/Users/vincentbagilet/Documents/zotero/storage/HZAZC6ZC/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf;/Users/vincentbagilet/Documents/zotero/storage/INFNPQXB/Ferman and Pinto - 2019 - Inference in Differences-in-Differences with Few T.pdf}
}

@article{fowler_electoral_2013,
  title = {Electoral and {{Policy Consequences}} of {{Voter Turnout}}: {{Evidence}} from {{Compulsory Voting}} in {{Australia}}},
  shorttitle = {Electoral and {{Policy Consequences}} of {{Voter Turnout}}},
  author = {Fowler, Anthony},
  year = {2013},
  journal = {Quarterly Journal of Political Science},
  volume = {8},
  number = {2},
  pages = {159--182},
  publisher = {{now publishers}},
  abstract = {Despite extensive research on voting, there is little evidence connecting turnout to tangible outcomes. Would election results and public policy be different if everyone voted? The adoption of compulsory voting in Australia provides a rare opportunity to address this question. First, I collect two novel data sources to assess the extent of turnout inequality in Australia before compulsory voting. Overwhelmingly, wealthy citizens voted more than their working-class counterparts. Next, exploiting the differential adoption of compulsory voting across states, I find that the policy increased voter turnout by 24 percentage points which in turn increased the vote shares and seat shares of the Labor Party by 7\textendash 10 percentage points. Finally, comparing across OECD countries, I find that Australia's adoption of compulsory voting significantly increased turnout and pension spending at the national level. Results suggest that democracies with voluntary voting do not represent the preferences of all citizens. Instead, increased voter turnout can dramatically alter election outcomes and resulting public policies.},
  file = {/Users/vincentbagilet/Documents/zotero/storage/VC5CNH3Z/Fowler - 2013 - Electoral and Policy Consequences of Voter Turnout.pdf}
}

@article{freeman_power_2013,
  title = {Power and Sample Size Calculations for {{Mendelian}} Randomization Studies Using One Genetic Instrument},
  author = {Freeman, G. and Cowling, B. J. and Schooling, C. M.},
  year = {2013},
  month = aug,
  journal = {International Journal of Epidemiology},
  volume = {42},
  number = {4},
  pages = {1157--1163},
  issn = {0300-5771, 1464-3685},
  doi = {10.1093/ije/dyt110},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/BYCVRB6U/Freeman et al. - 2013 - Power and sample size calculations for Mendelian r.pdf}
}

@article{fujiwara_habit_2016,
  title = {Habit {{Formation}} in {{Voting}}: {{Evidence}} from {{Rainy Elections}}},
  shorttitle = {Habit {{Formation}} in {{Voting}}},
  author = {Fujiwara, Thomas and Meng, Kyle and Vogl, Tom},
  year = {2016},
  month = oct,
  journal = {American Economic Journal: Applied Economics},
  volume = {8},
  number = {4},
  pages = {160--188},
  issn = {1945-7782},
  doi = {10.1257/app.20140533},
  abstract = {We estimate habit formation in voting--the effect of past on current turnout--by exploiting transitory voting cost shocks. Using county-level data on US presidential elections from 1952-2012, we find that rainfall on current and past election days reduces voter turnout. Our estimates imply that a 1-point decrease in past turnout lowers current turnout by 0.6-1.0 points. Further analyses suggest that habit formation operates by reinforcing the direct consumption value of voting and that our estimates may be amplified by social spillovers.},
  langid = {english},
  keywords = {Elections,IV,Rainfall,Turnout},
  file = {/Users/vincentbagilet/Documents/zotero/storage/DX8X7CVE/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf;/Users/vincentbagilet/Documents/zotero/storage/DXI85JKA/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf;/Users/vincentbagilet/Documents/zotero/storage/PAPLP8RE/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf;/Users/vincentbagilet/Documents/zotero/storage/UJ7R46KS/Fujiwara et al. - 2016 - Habit Formation in Voting Evidence from Rainy Ele.pdf}
}

@article{gelman_beyond_2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {6},
  pages = {641--651},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1177/1745691614551642},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  langid = {english},
  keywords = {p-values,Power,Statistics,Type S/M errors}
}

@book{gelman_regression_2020,
  title = {{Regression and Other Stories}},
  author = {Gelman, Andrew},
  year = {2020},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore}},
  isbn = {978-1-107-67651-0},
  langid = {Anglais}
}

@misc{goldfeld_generating_nodate,
  title = {Generating Data to Explore the Myriad Causal Effects That Can Be Estimated in Observational Data Analysis},
  author = {Goldfeld, Keith},
  journal = {ouR data generation},
  abstract = {I've been inspired by two recent talks describing the challenges of using instrumental variable (IV) methods. IV methods are used to estimate the causal effects of an exposure or intervention when there is unmeasured confounding. This estimated causal effect is very specific: the complier average causal effect (CACE). But, the CACE is just one of several possible causal estimands that we might be interested in. For example, there's the average causal effect (ACE) that represents a population average (not just based the subset of compliers).},
  langid = {english},
  keywords = {Simulations}
}

@article{goldsmith-pinkham_bartik_2020,
  title = {Bartik {{Instruments}}: {{What}}, {{When}}, {{Why}}, and {{How}}},
  shorttitle = {Bartik {{Instruments}}},
  author = {{Goldsmith-Pinkham}, Paul and Sorkin, Isaac and Swift, Henry},
  year = {2020},
  month = aug,
  journal = {American Economic Review},
  volume = {110},
  number = {8},
  pages = {2586--2624},
  issn = {0002-8282},
  doi = {10.1257/aer.20181047},
  abstract = {The Bartik instrument is formed by interacting local industry shares and national industry growth rates. We show that the typical use of a Bartik instrument assumes a pooled exposure research design, where the shares measure differential exposure to common shocks, and identification is based on exogeneity of the shares. Next, we show how the Bartik instrument weights each of the exposure designs. Finally, we discuss how to assess the plausibility of the research design. We illustrate our results through two applications: estimating the elasticity of labor supply, and estimating the elasticity of substitution between immigrants and natives.},
  langid = {english},
  keywords = {and Immigrants,and Transportation Economics: Regional Migration,Economics of Minorities,Empirical Studies of Trade,Indigenous Peoples,Industry Studies: Manufacturing: General,Model Construction and Estimation,Model Construction and Estimation; Empirical Studies of Trade; Economics of Minorities; Races; Indigenous Peoples; and Immigrants,Neighborhood Characteristics,Neighborhood Characteristics; Other Spatial Production and Pricing Analysis,Non-labor Discrimination,Non-labor Discrimination; Time Allocation and Labor Supply; Industry Studies: Manufacturing: General; Urban; Rural; Regional; Real Estate; and Transportation Economics: Regional Migration,Other Spatial Production and Pricing Analysis,Population,Races,Real Estate,Regional,Regional Labor Markets,Rural,Time Allocation and Labor Supply,Urban},
  file = {/Users/vincentbagilet/Documents/zotero/storage/223PSFJU/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf;/Users/vincentbagilet/Documents/zotero/storage/7UCS4JRK/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf;/Users/vincentbagilet/Documents/zotero/storage/JRE776A4/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf;/Users/vincentbagilet/Documents/zotero/storage/JW8VUECR/Goldsmith-Pinkham et al. - 2020 - Bartik Instruments What, When, Why, and How.pdf}
}

@article{gomez_republicans_2007,
  title = {The {{Republicans Should Pray}} for {{Rain}}: {{Weather}}, {{Turnout}}, and {{Voting}} in {{U}}.{{S}}. {{Presidential Elections}}},
  shorttitle = {The {{Republicans Should Pray}} for {{Rain}}},
  author = {Gomez, Brad T. and Hansford, Thomas G. and Krause, George A.},
  year = {2007},
  journal = {The Journal of Politics},
  volume = {69},
  number = {3},
  pages = {649--663},
  publisher = {{[The University of Chicago Press, Southern Political Science Association]}},
  issn = {0022-3816},
  doi = {10.1111/j.1468-2508.2007.00565.x},
  abstract = {The relationship between bad weather and lower levels of voter turnout is widely espoused by media, political practitioners, and, perhaps, even political scientists. Yet, there is virtually no solid empirical evidence linking weather to voter participation. This paper provides an extensive test of the claim. We examine the effect of weather on voter turnout in 14 U.S. presidential elections. Using GIS interpolations, we employ meteorological data drawn from over 22,000 U.S. weather stations to provide election day estimates of rain and snow for each U.S. county. We find that, when compared to normal conditions, rain significantly reduces voter participation by a rate of just less than 1\% per inch, while an inch of snowfall decreases turnout by almost .5\%. Poor weather is also shown to benefit the Republican party's vote share. Indeed, the weather may have contributed to two Electoral College outcomes, the 1960 and 2000 presidential elections.},
  file = {/Users/vincentbagilet/Documents/zotero/storage/HWA85Y4P/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf;/Users/vincentbagilet/Documents/zotero/storage/I54JJBNE/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf;/Users/vincentbagilet/Documents/zotero/storage/SI5CHN2E/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf;/Users/vincentbagilet/Documents/zotero/storage/ZSWKFZ5D/Gomez et al. - 2007 - The Republicans Should Pray for Rain Weather, Tur.pdf}
}

@article{griffin_moving_2021,
  title = {Moving beyond the Classic Difference-in-Differences Model: {{A}} Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies},
  shorttitle = {Moving beyond the Classic Difference-in-Differences Model},
  author = {Griffin, Beth Ann and Schuler, Megan S. and Stuart, Elizabeth A. and Patrick, Stephen and McNeer, Elizabeth and Smart, Rosanna and Powell, David and Stein, Bradley D. and Schell, Terry and Pacula, Rosalie L.},
  year = {2021},
  month = jun,
  journal = {arXiv:2003.12008 [stat]},
  eprint = {2003.12008},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {State-level policy evaluations commonly employ a difference-in-differences (DID) study design; yet within this framework, statistical model specification varies notably across studies. Motivated by applied state-level opioid policy evaluations, this simulation study compares statistical performance of multiple variations of two-way fixed effect models traditionally used for DID under a range of simulation conditions. While most linear models resulted in minimal bias, non-linear models and population-weighted versions of classic linear two-way fixed effect and linear GEE models yielded considerable bias (60 to 160\%). Further, root mean square error is minimized by linear AR models when examining crude mortality rates and by negative binomial models when examining raw death counts. In the context of frequentist hypothesis testing, many models yielded high Type I error rates and very low rates of correctly rejecting the null hypothesis ({$<$} 10\%), raising concerns of spurious conclusions about policy effectiveness. When considering performance across models, the linear autoregressive models were optimal in terms of directional bias, root mean squared error, Type I error, and correct rejection rates. These findings highlight notable limitations of traditional statistical models commonly used for DID designs, designs widely used in opioid policy studies and in state policy evaluations more broadly.},
  archiveprefix = {arXiv},
  keywords = {DID},
  file = {/Users/vincentbagilet/Documents/zotero/storage/2Z9C83TR/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf;/Users/vincentbagilet/Documents/zotero/storage/6E65YRZL/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf;/Users/vincentbagilet/Documents/zotero/storage/8W959THM/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf;/Users/vincentbagilet/Documents/zotero/storage/WMTE6KXJ/Griffin et al. - 2021 - Moving beyond the classic difference-in-difference.pdf}
}

@book{hernan_causal_2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A and Robins, James M},
  year = {2020},
  edition = {Boca Raton: Chapman \& Hall/CRC},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/6PAWZAPU/Hernán and Robins - Causal Inference What If.pdf}
}

@article{hernan_causal_2021,
  title = {Causal Analyses of Existing Databases: No Power Calculations Required},
  shorttitle = {Causal Analyses of Existing Databases},
  author = {Hern{\'a}n, Miguel A.},
  year = {2021},
  month = aug,
  journal = {Journal of Clinical Epidemiology},
  pages = {S0895435621002730},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2021.08.028},
  abstract = {Observational databases are often used to study causal questions. Before being granted access to data or funding, researchers may need to prove that ``the statistical power of their analysis will be high''. Analyses expected to have low power, and hence result in imprecise estimates, will not be approved. This restrictive attitude towards observational analyses is misguided.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/5GKZ3ANX/Hernán - 2021 - Causal analyses of existing databases no power ca.pdf}
}

@article{hill_bayesian_2011,
  title = {Bayesian {{Nonparametric Modeling}} for {{Causal Inference}}},
  author = {Hill, Jennifer L.},
  year = {2011},
  month = jan,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {20},
  number = {1},
  pages = {217--240},
  issn = {1061-8600, 1537-2715},
  doi = {10.1198/jcgs.2010.08162},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/TN4HL7CV/Hill - 2011 - Bayesian Nonparametric Modeling for Causal Inferen.pdf}
}

@article{hoxby_effects_2000,
  title = {The {{Effects}} of {{Class Size}} on {{Student Achievement}}: {{New Evidence}} from {{Population Variation}}*},
  shorttitle = {The {{Effects}} of {{Class Size}} on {{Student Achievement}}},
  author = {Hoxby, Caroline M.},
  year = {2000},
  month = nov,
  journal = {The Quarterly Journal of Economics},
  volume = {115},
  number = {4},
  pages = {1239--1285},
  issn = {0033-5533},
  doi = {10.1162/003355300555060},
  abstract = {I identify the effects of class size on student achievement using longitudinal variation in the population associated with each grade in 649 elementary schools. I use variation in class size driven by idiosyncratic variation in the population. I also use discrete jumps in class size that occur when a small change in enrollment triggers a maximum or minimum class size rule. The estimates indicate that class size does not have a statistically significant effect on student achievement. I rule out even modest effects (2 to 4 percent of a standard deviation in scores for a 10 percent reduction in class size).},
  keywords = {Class size,Education},
  file = {/Users/vincentbagilet/Documents/zotero/storage/86LLSHR4/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf;/Users/vincentbagilet/Documents/zotero/storage/B9R9XSTB/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf;/Users/vincentbagilet/Documents/zotero/storage/JF6ZI3RW/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf;/Users/vincentbagilet/Documents/zotero/storage/P2E2NJLX/Hoxby - 2000 - The Effects of Class Size on Student Achievement .pdf}
}

@book{huntington-klein_effect_2021,
  title = {The {{Effect}}: {{An Introduction}} to {{Research Design}} and {{Causality}}},
  shorttitle = {The {{Effect}}},
  author = {{Huntington-Klein}, Nick},
  year = {2021},
  month = nov,
  edition = {First},
  publisher = {{Chapman and Hall/CRC}},
  address = {{Boca Raton}},
  doi = {10.1201/9781003226055},
  isbn = {978-1-00-322605-5},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/WC6YEAMY/Huntington-Klein - 2021 - The Effect An Introduction to Research Design and.pdf}
}

@book{imbens_causal_2015,
  title = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}: {{An Introduction}}},
  shorttitle = {Causal {{Inference}} for {{Statistics}}, {{Social}}, and {{Biomedical Sciences}}},
  author = {Imbens, Guido W. and Rubin, Donald B.},
  year = {2015},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139025751},
  abstract = {Most questions in social and biomedical sciences are causal in nature: what would happen to individuals, or to groups, if part of their environment were changed? In this groundbreaking text, two world-renowned experts present statistical methods for studying such questions. This book starts with the notion of potential outcomes, each corresponding to the outcome that would be realized if a subject were exposed to a particular treatment or regime. In this approach, causal effects are comparisons of such potential outcomes. The fundamental problem of causal inference is that we can only observe one of the potential outcomes for a particular subject. The authors discuss how randomized experiments allow us to assess causal effects and then turn to observational studies. They lay out the assumptions needed for causal inference and describe the leading analysis methods, including matching, propensity-score methods, and instrumental variables. Many detailed applications are included, with special focus on practical aspects for the empirical researcher.},
  isbn = {978-0-521-88588-1}
}

@article{imbens_optimal_2012,
  title = {Optimal {{Bandwidth Choice}} for the {{Regression Discontinuity Estimator}}},
  author = {Imbens, Guido and Kalyanaraman, Karthik},
  year = {2012},
  journal = {The Review of Economic Studies},
  volume = {79},
  number = {3},
  pages = {933--959},
  publisher = {{[Oxford University Press, Review of Economic Studies, Ltd.]}},
  issn = {0034-6527},
  abstract = {We investigate the choice of the bandwidth for the regression discontinuity estimator. We focus on estimation by local linear regression, which was shown to have attractive properties (Porter, J. 2003, "Estimation in the Regression Discontinuity Model" (unpublished, Department of Economics, University of Wisconsin, Madison)). We derive the asymptotically optimal bandwidth under squared error loss. This optimal bandwidth depends on unknown functionals of the distribution of the data and we propose simple and consistent estimators for these functionals to obtain a fully data-driven bandwidth algorithm. We show that this bandwidth estimator is optimal according to the criterion of Li (1987, "Asymptotic Optimality for C p , C L , Cross-validation and Generalized Cross-validation: Discrete Index Set", Annals of Statistics, 15, 958\textendash 975), although it is not unique in the sense that alternative consistent estimators for the unknown functionals would lead to bandwidth estimators with the same optimality properties. We illustrate the proposed bandwidth, and the sensitivity to the choices made in our algorithm, by applying the methods to a data set previously analysed by Lee (2008, "Randomized Experiments from Non-random Selection in U.S. House Elections", Journal of Econometrics, 142, 675\textendash 697) as well as by conducting a small simulation study.},
  keywords = {Optimal bandwith,Precision,RDD},
  file = {/Users/vincentbagilet/Documents/zotero/storage/HMB6DNG8/Imbens and Kalyanaraman - 2012 - Optimal Bandwidth Choice for the Regression Discon.pdf}
}

@article{imbens_statistical_2021,
  title = {Statistical {{Significance}}, {{Values}}, and the {{Reporting}} of {{Uncertainty}}},
  author = {Imbens, Guido W.},
  year = {2021},
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {157--174},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.157},
  abstract = {The use of statistical significance and p-values has become a matter of substantial controversy in various fields using statistical methods. This has gone as far as some journals banning the use of indicators for statistical significance, or even any reports of p-values, and, in one case, any mention of confidence intervals. I discuss three of the issues that have led to these often-heated debates. First, I argue that in many cases, p-values and indicators of statistical significance do not answer the questions of primary interest. Such questions typically involve making (recommendations on) decisions under uncertainty. In that case, point estimates and measures of uncertainty in the form of confidence intervals or even better, Bayesian intervals, are often more informative summary statistics. In fact, in that case, the presence or absence of statistical significance is essentially irrelevant, and including them in the discussion may confuse the matter at hand. Second, I argue that there are also cases where testing null hypotheses is a natural goal and where p-values are reasonable and appropriate summary statistics. I conclude that banning them in general is counterproductive. Third, I discuss that the overemphasis in empirical work on statistical significance has led to abuse of p-values in the form of p-hacking and publication bias. The use of pre-analysis plans and replication studies, in combination with lowering the emphasis on statistical significance may help address these problems.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/FPVKDF76/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf;/Users/vincentbagilet/Documents/zotero/storage/JXJMHQVG/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf;/Users/vincentbagilet/Documents/zotero/storage/T8N7FZX4/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf;/Users/vincentbagilet/Documents/zotero/storage/UHE67LM9/Imbens - 2021 - Statistical Significance, p -Values, and th.pdf}
}

@article{ioannidis_power_2017,
  title = {The {{Power}} of {{Bias}} in {{Economics Research}}},
  author = {Ioannidis, John P. A. and Stanley, T. D. and Doucouliagos, Hristos},
  year = {2017},
  month = oct,
  journal = {The Economic Journal},
  volume = {127},
  number = {605},
  pages = {F236-F265},
  publisher = {{Oxford Academic}},
  issn = {0013-0133},
  doi = {10.1111/ecoj.12461},
  abstract = {Abstract.  We investigate two critical dimensions of the credibility of empirical economics research: statistical power and bias. We survey 159 empirical econom},
  langid = {english},
  keywords = {Economics,Metaanalysis,Power,Statistics},
  file = {/Users/vincentbagilet/Documents/zotero/storage/CJP5KNRI/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf;/Users/vincentbagilet/Documents/zotero/storage/GI6V8NR7/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf;/Users/vincentbagilet/Documents/zotero/storage/GQD48CQ9/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf;/Users/vincentbagilet/Documents/zotero/storage/ZD2VLDAZ/Ioannidis et al. - 2017 - The Power of Bias in Economics Research.pdf}
}

@article{ioannidis_why_2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  year = {2008},
  journal = {Epidemiology},
  volume = {19},
  number = {5},
  pages = {640--648},
  langid = {english},
  keywords = {Inflated effects,To read},
  file = {/Users/vincentbagilet/Documents/zotero/storage/TSNPGPJS/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf;/Users/vincentbagilet/Documents/zotero/storage/U6M3UN2G/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf;/Users/vincentbagilet/Documents/zotero/storage/VJAU9HAA/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf;/Users/vincentbagilet/Documents/zotero/storage/XEI7JJPE/Ioannidis - 2008 - Why Most Discovered True Associations Are Inflated.pdf}
}

@article{jacob_remedial_2004,
  title = {Remedial {{Education}} and {{Student Achievement}}: {{A Regression-Discontinuity Analysis}}},
  shorttitle = {Remedial {{Education}} and {{Student Achievement}}},
  author = {Jacob, Brian A. and Lefgren, Lars},
  year = {2004},
  month = feb,
  journal = {The Review of Economics and Statistics},
  volume = {86},
  number = {1},
  pages = {226--244},
  issn = {0034-6535},
  doi = {10.1162/003465304323023778},
  abstract = {As standards and accountability have become increasingly prominent features of the educational landscape, educators have relied more on remedial programs such as summer school and grade retention to help low-achieving students meet minimum academic standards. Yet the evidence on the effectiveness of such programs is mixed, and prior research suffers from selection bias. However, recent school reform efforts in Chicago provide an opportunity to examine the causal impact of these remedial education programs. In 1996, the Chicago Public Schools instituted an accountability policy that tied summer school and promotional decisions to performance on standardized tests, which resulted in a highly nonlinear relationship between current achievement and the probability of attending summer school or being retained. Using a regression discontinuity design, we find that the net effect of these programs was to substantially increase academic achievement among third-graders, but not sixth-graders. In addition, contrary to conventional wisdom and prior research, we find that retention increases achievement for third-grade students and has little effect on math achievement for sixth-grade students.},
  keywords = {Chicago,Education,Grade retention,RDD,Summer schools,US},
  file = {/Users/vincentbagilet/Documents/zotero/storage/8XCDEVTB/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf;/Users/vincentbagilet/Documents/zotero/storage/A38G3UBJ/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf;/Users/vincentbagilet/Documents/zotero/storage/FKYNJNY2/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf;/Users/vincentbagilet/Documents/zotero/storage/ZRVUJZ34/Jacob and Lefgren - 2004 - Remedial Education and Student Achievement A Regr.pdf}
}

@article{kamenica_bayesian_2011,
  title = {Bayesian {{Persuasion}}},
  author = {Kamenica, Emir and Gentzkow, Matthew},
  year = {2011},
  month = oct,
  journal = {American Economic Review},
  volume = {101},
  number = {6},
  pages = {2590--2615},
  issn = {0002-8282},
  doi = {10.1257/aer.101.6.2590},
  abstract = {When is it possible for one person to persuade another to change her action? We consider a symmetric information model where a sender chooses a signal to reveal to a receiver, who then takes a noncontractible action that affects the welfare of both players. We derive necessary and sufficient conditions for the existence of a signal that strictly benefits the sender. We characterize sender-optimal signals. We examine comparative statics with respect to the alignment of the sender's and the receiver's preferences. Finally, we apply our results to persuasion by litigators, lobbyists, and salespeople. (JEL D72, D82, D83, K40, M31)},
  langid = {english},
  keywords = {and Illegal Behavior: General,and Voting Behavior,Asymmetric and Private Information,Belief,Belief; Legal Procedure; the Legal System; and Illegal Behavior: General; Marketing,Communication,Elections,Information and Knowledge,Learning,Legal Procedure,Legislatures,Lobbying,Marketing,Political Processes: Rent-seeking,Political Processes: Rent-seeking; Lobbying; Elections; Legislatures; and Voting Behavior; Asymmetric and Private Information; Search,Search,the Legal System},
  file = {/Users/vincentbagilet/Documents/zotero/storage/4C2KHES5/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf;/Users/vincentbagilet/Documents/zotero/storage/8XJ8XGAE/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf;/Users/vincentbagilet/Documents/zotero/storage/W346WDLT/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf;/Users/vincentbagilet/Documents/zotero/storage/WPDK5SNC/Kamenica and Gentzkow - 2011 - Bayesian Persuasion.pdf}
}

@article{kasy_forking_2021,
  title = {Of {{Forking Paths}} and {{Tied Hands}}: {{Selective Publication}} of {{Findings}}, and {{What Economists Should Do}} about {{It}}},
  shorttitle = {Of {{Forking Paths}} and {{Tied Hands}}},
  author = {Kasy, Maximilian},
  year = {2021},
  month = aug,
  journal = {Journal of Economic Perspectives},
  volume = {35},
  number = {3},
  pages = {175--192},
  issn = {0895-3309},
  doi = {10.1257/jep.35.3.175},
  abstract = {A key challenge for interpreting published empirical research is the fact that published findings might be selected by researchers or by journals. Selection might be based on criteria such as significance, consistency with theory, or the surprisingness of findings or their plausibility. Selection leads to biased estimates, reduced coverage of confidence intervals, and distorted posterior beliefs. I review methods for detecting and quantifying selection based on the distribution of p-values, systematic replication studies, and meta-studies. I then discuss the conflicting recommendations regarding selection result ing from alternative objectives, in particular, the validity of inference versus the relevance of findings for decision-makers. Based on this discussion, I consider various reform proposals, such as deemphasizing significance, pre-analysis plans, journals for null results and replication studies, and a functionally differentiated publication system. In conclusion, I argue that we need alternative foundations of statistics that go beyond the single-agent model of decision theory.},
  langid = {english},
  keywords = {Forking paths,Statistics},
  file = {/Users/vincentbagilet/Documents/zotero/storage/3ZCKSJ87/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf;/Users/vincentbagilet/Documents/zotero/storage/JXR4TXGH/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf;/Users/vincentbagilet/Documents/zotero/storage/NLZ6XQIB/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf;/Users/vincentbagilet/Documents/zotero/storage/YP4YI6TE/Kasy - 2021 - Of Forking Paths and Tied Hands Selective Publica.pdf}
}

@article{klaauw_estimating_2002,
  title = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}: {{A Regression}}\textendash{{Discontinuity Approach}}*},
  shorttitle = {Estimating the {{Effect}} of {{Financial Aid Offers}} on {{College Enrollment}}},
  author = {Klaauw, Wilbert Van Der},
  year = {2002},
  journal = {International Economic Review},
  volume = {43},
  number = {4},
  pages = {1249--1287},
  issn = {1468-2354},
  doi = {10.1111/1468-2354.t01-1-00055},
  abstract = {An important problem faced by colleges and universities, that of evaluating the effect of their financial aid offers on student enrollment decisions, is complicated by the likely endogeneity of the aid offer variable in a student enrollment equation. This article shows how discontinuities in an East Coast college's aid assignment rule can be exploited to obtain credible estimates of the aid effect without having to rely on arbitrary exclusion restrictions and functional form assumptions. Semiparametric estimates based on a regression\textendash discontinuity (RD) approach affirm the importance of financial aid as an effective instrument in competing with other colleges for students.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-2354.t01-1-00055},
  file = {/Users/vincentbagilet/Documents/zotero/storage/DGA7N68X/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf;/Users/vincentbagilet/Documents/zotero/storage/DXGSID8N/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf;/Users/vincentbagilet/Documents/zotero/storage/SL9ZPSEX/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf;/Users/vincentbagilet/Documents/zotero/storage/TAAQRRCI/Klaauw - 2002 - Estimating the Effect of Financial Aid Offers on C.pdf}
}

@techreport{lal_how_2021,
  type = {{{SSRN Scholarly Paper}}},
  title = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}? {{Practical Advice}} Based on {{Over}} 60 {{Replicated Studies}}},
  shorttitle = {How {{Much Should We Trust Instrumental Variable Estimates}} in {{Political Science}}?},
  author = {Lal, Apoorva and Lockhart, Mackenzie William and Xu, Yiqing and Zu, Ziwen},
  year = {2021},
  month = aug,
  number = {ID 3905329},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3905329},
  abstract = {Instrumental variable (IV) strategies are commonly used in political science to establish causal relationships, yet the identifying assumptions required by an IV design are demanding and it remains challenging for researchers to evaluate their plausibility. We replicate 61 papers published in three top journals in political science from the past decade (2011-2020) and document several troubling patterns: (1) researchers often miscalculate the first-stage F statistics, overestimating the strength of their IVs; (2) most researchers rely on classical asymptotic standard errors, which often severely underestimate the uncertainties around the two-stage-least-squared (2SLS) estimates; (3) in the majority of the replicated studies, the 2SLS estimates are much bigger than the ordinary-least-squared estimates, and their ratio is negatively correlated with the strength of the IVs in studies where the IVs are not experimentally generated, suggesting potential violations of the exclusion restriction; such a relationship is much weaker with experimentally generated IVs. To improve practice, we provide a checklist for researchers to avoid these pitfalls and recommend a zero-first-stage test and a local-to-zero procedure to guard against failure of the identifying assumptions.},
  langid = {english},
  keywords = {exclusion restriction,instrumental variables,replications,two-stage-least-squared,weak IV,zero-first-stage test},
  file = {/Users/vincentbagilet/Documents/zotero/storage/4HE2KCI2/Lal et al. - 2021 - How Much Should We Trust Instrumental Variable Est.pdf;/Users/vincentbagilet/Documents/zotero/storage/8JBVYNCS/Lal et al. - 2021 - How Much Should We Trust Instrumental Variable Est.pdf;/Users/vincentbagilet/Documents/zotero/storage/MPZABSPD/Lal et al. - 2021 - How Much Should We Trust Instrumental Variable Est.pdf;/Users/vincentbagilet/Documents/zotero/storage/TNASG478/Lal et al. - 2021 - How Much Should We Trust Instrumental Variable Est.pdf}
}

@article{lalonde_evaluating_1986,
  title = {Evaluating the {{Econometric Evaluations}} of {{Training Programs}} with {{Experimental Data}}},
  author = {LaLonde, Robert J.},
  year = {1986},
  journal = {The American Economic Review},
  volume = {76},
  number = {4},
  pages = {604--620},
  publisher = {{American Economic Association}},
  issn = {0002-8282},
  abstract = {This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric procedures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.}
}

@article{lavaine_energy_2017,
  title = {Energy {{Production}} and {{Health Externalities}}: {{Evidence}} from {{Oil Refinery Strikes}} in {{France}}},
  shorttitle = {Energy {{Production}} and {{Health Externalities}}},
  author = {Lavaine, Emmanuelle and Neidell, Matthew},
  year = {2017},
  month = jun,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {4},
  number = {2},
  pages = {447--477},
  publisher = {{The University of Chicago Press}},
  issn = {2333-5955},
  doi = {10.1086/691554},
  abstract = {This paper examines the effect of energy production on health using a recent strike that affected oil refineries in France as a natural experiment. First, we show that the temporary reduction in refining led to a significant reduction in sulfur dioxide (SO2) concentrations. Second, this shock significantly increased birth weight and gestational age of newborns, particularly for those exposed to the strike during the first and third trimesters of pregnancy, and decreased asthma and bronchitis admissions. Back-of-the-envelope calculations suggest that a 1-unit (or 26\%) decline in monthly SO2 leads to an \texteuro 89 million increase in lifetime earnings per birth-year cohort. This externality from oil refineries should be an important part of policy discussions surrounding the production of energy.},
  keywords = {Air pollution,Birth outcomes,Difference in difference,Q40,Q51,Q53},
  file = {/Users/vincentbagilet/Documents/zotero/storage/M25UKNLA/Lavaine and Neidell - 2017 - Energy Production and Health Externalities Eviden.pdf}
}

@article{leamer_lets_2021,
  title = {Let's {{Take}} the {{Con Out}} of {{Econometrics}}},
  author = {Leamer, Edward E},
  year = {2021},
  pages = {14},
  langid = {english},
  keywords = {Inflated effects,To read},
  file = {/Users/vincentbagilet/Documents/zotero/storage/56KTMW3G/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf;/Users/vincentbagilet/Documents/zotero/storage/5KN2N38Z/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf;/Users/vincentbagilet/Documents/zotero/storage/DSKNDMX2/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf;/Users/vincentbagilet/Documents/zotero/storage/E9LN3TCU/Leamer - 2021 - Let's Take the Con Out of Econometrics.pdf}
}

@article{lee_regression_2010,
  title = {Regression {{Discontinuity Designs}} in {{Economics}}},
  author = {Lee, David S. and Lemieux, Thomas},
  year = {2010},
  journal = {Journal of Economic Literature},
  volume = {48},
  number = {2},
  pages = {281--355},
  publisher = {{American Economic Association}},
  issn = {0022-0515},
  abstract = {This paper provides an introduction and "user guide" to Regression Discontinuity (RD) designs for empirical researchers. It presents the basic theory behind the research design, details when RD is likely to be valid or invalid given economic incentives, explains why it is considered a "quasi-experimental" design, and summarizes different ways (with their advantages and disadvantages) of estimating RD designs and the limitations of interpreting these estimates. Concepts are discussed using examples drawn from the growing body of empirical research using RD.},
  keywords = {Literature review,RDD},
  file = {/Users/vincentbagilet/Documents/zotero/storage/MA5BFJJ6/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf;/Users/vincentbagilet/Documents/zotero/storage/QFVT48EJ/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf;/Users/vincentbagilet/Documents/zotero/storage/VXR9ZRWJ/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf;/Users/vincentbagilet/Documents/zotero/storage/Z7VRRZE9/Lee and Lemieux - 2010 - Regression Discontinuity Designs in Economics.pdf}
}

@misc{linden_retrodesign_2019,
  title = {{{RETRODESIGN}}: {{Stata}} Module to Compute Type-{{S}} ({{Sign}}) and Type-{{M}} ({{Magnitude}}) Errors},
  shorttitle = {{{RETRODESIGN}}},
  author = {Linden, Ariel},
  year = {2019},
  month = oct,
  journal = {Statistical Software Components},
  abstract = {retrodesign computes power, type-S, and type-M errors for one or more specified effect sizes. A type-S (sign) error indicates the probability of an effect size estimate being in the wrong direction, and a type-M (magnitude) error indicates the factor by which the magnitude of an effect might be overestimated -- given that the test statistic is statistically significant (Gelman and Carlin 2014). Gelman and Carlin (2014) propose computing the type-M error using the Student's t distribution while Lu, Qiu, and Deng (2019) propose a closed form solution for computing the type-M error. Both methods are implemented in retrodesign. retrodesign produces results identical to those computed in the retrodesign package for R.},
  howpublished = {Boston College Department of Economics},
  langid = {english},
  keywords = {design calculation,power analysis,replication crisis,Stata,statistical significance,type M error,type S error}
}

@article{lu_note_2019,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  year = {2019},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {72},
  number = {1},
  pages = {1--17},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12132},
  abstract = {Motivated by the recent replication and reproducibility crisis, Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641) advocated focusing on controlling for Type S/M errors, instead of the classic Type I/II errors, when conducting hypothesis testing. In this paper, we aim to fill several theoretical gaps in the methodology proposed by Gelman and Carlin (2014, Perspect. Psychol. Sci., 9, 641). In particular, we derive the closed-form expression for the expected Type M error, and study the mathematical properties of the probability of Type S error as well as the expected Type M error, such as monotonicity. We demonstrate the advantages of our results through numerical and empirical examples.},
  copyright = {\textcopyright{} 2018 The British Psychological Society},
  langid = {english},
  keywords = {Maths},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bmsp.12132},
  file = {/Users/vincentbagilet/Documents/zotero/storage/3FMY695Y/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Documents/zotero/storage/8AJLYKCW/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Documents/zotero/storage/NU6J9YSV/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Documents/zotero/storage/R63NKECQ/Lu et al. - 2019 - A note on Type SM errors in hypothesis testing.pdf;/Users/vincentbagilet/Documents/zotero/storage/ULCHMHF4/Peña et al. - 2019 - A novel imputation method for missing values in ai.pdf}
}

@article{maniadis_one_2014,
  title = {One {{Swallow Doesn}}'t {{Make}} a {{Summer}}: {{New Evidence}} on {{Anchoring Effects}}},
  shorttitle = {One {{Swallow Doesn}}'t {{Make}} a {{Summer}}},
  author = {Maniadis, Zacharias and Tufano, Fabio and List, John A.},
  year = {2014},
  month = jan,
  journal = {American Economic Review},
  volume = {104},
  number = {1},
  pages = {277--290},
  issn = {0002-8282},
  doi = {10.1257/aer.104.1.277},
  abstract = {Some researchers have argued that anchoring in economic valuations casts doubt on the assumption of consistent and stable preferences. We present new evidence that explores the strength of certain anchoring results. We then present a theoretical framework that provides insights into why we should be cautious of initial empirical findings in general. The model importantly highlights that the rate of false positives depends not only on the observed significance level, but also on statistical power, research priors, and the number of scholars exploring the question. Importantly, a few independent replications dramatically increase the chances that the original finding is true.},
  langid = {english},
  keywords = {Consumer Economics: Empirical Analysis,Design of Experiments: Laboratory,Example,Individual},
  file = {/Users/vincentbagilet/Documents/zotero/storage/5YSHICAV/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf;/Users/vincentbagilet/Documents/zotero/storage/FYHIGTDS/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf;/Users/vincentbagilet/Documents/zotero/storage/IBL2539V/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf;/Users/vincentbagilet/Documents/zotero/storage/S5H8VQXD/Maniadis et al. - 2014 - One Swallow Doesn't Make a Summer New Evidence on.pdf}
}

@techreport{mcconnell_going_2015,
  title = {Going beyond Simple Sample Size Calculations: A Practitioner's Guide},
  shorttitle = {Going beyond Simple Sample Size Calculations},
  author = {McConnell, Brendon and {Vera-Hernandez}, Marcos},
  year = {2015},
  month = sep,
  institution = {{Institute for Fiscal Studies}},
  doi = {10.1920/wp.ifs.2015.1517},
  abstract = {Basic methods to compute required sample sizes are well understood and supported by widely available software. However, the sophistication of the methods commonly used has not kept pace with the complexity of commonly employed experimental designs. We compile available methods for sample size calculations for continuous and binary outcomes with and without covariates, for both clustered and non-clustered RCTs. Formulae for both panel data and unbalanced designs are provided. Extensions include methods to: (1) optimise the sample when costs constraints are binding, (2) compute the power of a complex design by simulation, and (3) adjust calculations for multiple testing.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/FDQLJ2IE/McConnell and Vera-Hernandez - 2015 - Going beyond simple sample size calculations a pr.pdf}
}

@article{mcshane_abandon_2019,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1527253},
  abstract = {We discuss problems the null hypothesis significance testing (NHST) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the NHST paradigm\textemdash and the p-value thresholds intrinsic to it\textemdash as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to ``ban'' p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/942AURU9/McShane et al. - 2019 - Abandon Statistical Significance.pdf}
}

@article{mellon_rain_2021,
  title = {Rain, {{Rain}}, {{Go Away}}: 176 {{Potential Exclusion-Restriction Violations}} for {{Studies Using Weather}} as an {{Instrumental Variable}}},
  author = {Mellon, Jonathan},
  year = {2021},
  month = jul,
  pages = {112},
  abstract = {Instrumental variable (IV) analysis assumes that the instrument only affects the dependent variable via its relationship with the independent variable. Other possible causal routes from the IV to the dependent variable are exclusion-restriction violations and make the instrument invalid. Weather has been widely used as an instrumental variable in social science to predict many different variables. The use of weather to instrument different independent variables represents strong prima facie evidence of exclusion violations for all studies using weather as an IV. A review of 279 studies reveals 176 variables which have been linked to weather: all of which represent potential exclusion violations. I show that the magnitude of several of these violations is sufficient overturn many existing IV results. I conclude with practical steps to systematically review existing literature to identify possible exclusion violations when using IV designs. I demonstrate how sensitivity analysis can quantify the vulnerability of a particular IV estimate to exclusion restriction violations in the literature.},
  langid = {english},
  keywords = {Critique,IV,Rainfall},
  file = {/Users/vincentbagilet/Documents/zotero/storage/3SJLBI8H/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf;/Users/vincentbagilet/Documents/zotero/storage/77YXQCML/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf;/Users/vincentbagilet/Documents/zotero/storage/ABLCMAFZ/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf;/Users/vincentbagilet/Documents/zotero/storage/V36B46KA/Mellon - Rain, Rain, Go Away 176 Potential Exclusion-Restr.pdf}
}

@article{morgan_counterfactuals_nodate,
  title = {Counterfactuals and {{Causal Inference}}},
  author = {Morgan, Stephen L and Winship, Christopher},
  pages = {526},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/6F7YMPU8/Morgan and Winship - Counterfactuals and Causal Inference.pdf}
}

@misc{noauthor_10_nodate,
  title = {10 {{Things}} to {{Know About Cluster Randomization}} \textendash{} {{EGAP}}},
  howpublished = {https://egap.org/resource/10-things-to-know-about-cluster-randomization/},
  keywords = {Clustering,Overall Problems}
}

@misc{noauthor_201114999_nodate,
  title = {[2011.14999] {{An Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  howpublished = {https://arxiv.org/abs/2011.14999}
}

@misc{noauthor_210914526_nodate,
  title = {[2109.14526] {{On}} the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  howpublished = {https://arxiv.org/abs/2109.14526}
}

@misc{noauthor_causal_nodate,
  title = {Causal {{Effects}} in {{Nonexperimental Studies}}: {{Reevaluating}} the {{Evaluation}} of {{Training Programs}}: {{Journal}} of the {{American Statistical Association}}: {{Vol}} 94, {{No}} 448},
  howpublished = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1999.10473858}
}

@misc{noauthor_mostly_nodate,
  title = {Mostly {{Harmless Econometrics}} \textendash{} {{An Empiricist}}`s {{Companion}} : {{Angrist}}, {{Joshua D}}., {{Pischke}}, {{Jorn}}\textendash Steffen, {{Pischke}}, {{J\~A}}\textparagraph rn\textendash Steffen: {{Amazon}}.Fr: {{Livres}}},
  howpublished = {https://www.amazon.fr/Mostly-Harmless-Econometrics-Empiricist\%60s-Companion/dp/0691120358},
  file = {/Users/vincentbagilet/Documents/zotero/storage/476ZJ84K/0691120358.html}
}

@misc{noauthor_moving_nodate,
  title = {Moving beyond the Classic Difference-in-Differences Model: A Simulation Study Comparing Statistical Methods for Estimating Effectiveness of State-Level Policies | {{BMC Medical Research Methodology}} | {{Full Text}}},
  howpublished = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01471-y}
}

@book{noauthor_notitle_nodate,
  type = {Book}
}

@article{open_science_collaboration_estimating_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.aac4716},
  keywords = {Experiments,Power,Psychology,Replications},
  file = {/Users/vincentbagilet/Documents/zotero/storage/4A5NV8BX/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf;/Users/vincentbagilet/Documents/zotero/storage/7CQLXBNK/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf;/Users/vincentbagilet/Documents/zotero/storage/UUCD7DC7/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf;/Users/vincentbagilet/Documents/zotero/storage/VK2K5AUX/OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{oster_unobservable_2019,
  title = {Unobservable {{Selection}} and {{Coefficient Stability}}: {{Theory}} and {{Evidence}}},
  shorttitle = {Unobservable {{Selection}} and {{Coefficient Stability}}},
  author = {Oster, Emily},
  year = {2019},
  month = apr,
  journal = {Journal of Business \& Economic Statistics},
  volume = {37},
  number = {2},
  pages = {187--204},
  issn = {0735-0015, 1537-2707},
  doi = {10.1080/07350015.2016.1227711},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/2ME2TBXZ/Oster - 2019 - Unobservable Selection and Coefficient Stability .pdf}
}

@techreport{ravallion_should_2020,
  type = {Working {{Paper}}},
  title = {Should the {{Randomistas}} ({{Continue}} to) {{Rule}}?},
  author = {Ravallion, Martin},
  year = {2020},
  month = jul,
  series = {Working {{Paper Series}}},
  number = {27554},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w27554},
  abstract = {The rising popularity of randomized controlled trials (RCTs) in development applications has come with continuing debates about the merits of this approach. The paper takes stock of the issues. It argues that an unconditional preference for RCTs is questionable on three main counts. First, the case for such a preference is unclear on a priori grounds. For example, with a given budget, even a biased observational study can come closer to the truth than a costly RCT. Second, the ethical objections to RCTs have not been properly addressed by advocates. Third, there is a risk of distorting the evidence-base for informing policymaking, given that an insistence on RCTs generates selection bias in what gets evaluated. Going forward, pressing knowledge gaps should drive the questions asked and how they are answered, not the methodological preferences of some researchers. The gold standard is the best method for the question at hand.},
  file = {/Users/vincentbagilet/Documents/zotero/storage/6CQEID8D/Ravallion - 2020 - Should the Randomistas (Continue to) Rule.pdf}
}

@article{rokicki_inference_2018,
  title = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}: {{A Review}}, {{Simulation Study}}, and {{Empirical Application Using SHARE Data}}},
  shorttitle = {Inference {{With Difference-in-Differences With}} a {{Small Number}} of {{Groups}}},
  author = {Rokicki, Slawa and Cohen, Jessica and Fink, G{\"u}nther and Salomon, Joshua A. and Landrum, Mary Beth},
  year = {2018},
  month = jan,
  journal = {Medical Care},
  volume = {56},
  number = {1},
  pages = {97--105},
  issn = {1537-1948},
  doi = {10.1097/MLR.0000000000000830},
  abstract = {BACKGROUND: Difference-in-differences (DID) estimation has become increasingly popular as an approach to evaluate the effect of a group-level policy on individual-level outcomes. Several statistical methodologies have been proposed to correct for the within-group correlation of model errors resulting from the clustering of data. Little is known about how well these corrections perform with the often small number of groups observed in health research using longitudinal data. METHODS: First, we review the most commonly used modeling solutions in DID estimation for panel data, including generalized estimating equations (GEE), permutation tests, clustered standard errors (CSE), wild cluster bootstrapping, and aggregation. Second, we compare the empirical coverage rates and power of these methods using a Monte Carlo simulation study in scenarios in which we vary the degree of error correlation, the group size balance, and the proportion of treated groups. Third, we provide an empirical example using the Survey of Health, Ageing, and Retirement in Europe. RESULTS: When the number of groups is small, CSE are systematically biased downwards in scenarios when data are unbalanced or when there is a low proportion of treated groups. This can result in over-rejection of the null even when data are composed of up to 50 groups. Aggregation, permutation tests, bias-adjusted GEE, and wild cluster bootstrap produce coverage rates close to the nominal rate for almost all scenarios, though GEE may suffer from low power. CONCLUSIONS: In DID estimation with a small number of groups, analysis using aggregation, permutation tests, wild cluster bootstrap, or bias-adjusted GEE is recommended.},
  langid = {english},
  pmid = {29112050},
  keywords = {DID},
  file = {/Users/vincentbagilet/Documents/zotero/storage/47DBK8JV/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf;/Users/vincentbagilet/Documents/zotero/storage/5NU5BD9G/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf;/Users/vincentbagilet/Documents/zotero/storage/9U8NEN9N/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf;/Users/vincentbagilet/Documents/zotero/storage/DH63Q2FT/Rokicki et al. - 2018 - Inference With Difference-in-Differences With a Sm.pdf}
}

@article{romer_praise_2020,
  title = {In {{Praise}} of {{Confidence Intervals}}},
  author = {Romer, David},
  year = {2020},
  month = may,
  journal = {AEA Papers and Proceedings},
  volume = {110},
  pages = {55--60},
  issn = {2574-0768, 2574-0776},
  doi = {10.1257/pandp.20201059},
  abstract = {Most empirical papers in economics focus on two aspects of their results: whether the estimates are statistically significantly different from zero and the interpretation of the point estimates. This focus obscures important information about the implications of the results for economically interesting hypotheses about values of the parameters other than zero, and in some cases, about the strength of the evidence against values of zero. This limitation can be overcome by reporting confidence intervals for papers' main estimates and discussing their economic interpretation.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/9YDGV5UA/Romer - 2020 - In Praise of Confidence Intervals.pdf}
}

@book{rosenbaum_design_2020,
  title = {Design of {{Observational Studies}}},
  author = {Rosenbaum, Paul R.},
  year = {2020},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-46405-9},
  isbn = {978-3-030-46404-2 978-3-030-46405-9},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/HN6VH6Q5/Rosenbaum - 2020 - Design of Observational Studies.pdf}
}

@book{rosenbaum_observational_2002,
  title = {Observational {{Studies}}},
  author = {Rosenbaum, Paul R.},
  year = {2002},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-3692-2},
  isbn = {978-1-4419-3191-7 978-1-4757-3692-2},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/TJJUEJ7T/Rosenbaum - 2002 - Observational Studies.pdf}
}

@article{rosenthal_file_1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  year = {1979},
  journal = {Psychological Bulletin},
  volume = {86},
  number = {3},
  pages = {638--641},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimentation,Scientific Communication,Statistical Probability,Statistical Tests,Type I Errors}
}

@incollection{roth_lets_1994,
  title = {Lets {{Keep}} the {{Con}} out of {{Experimental Econ}}.: {{A Methodological Note}}},
  shorttitle = {Lets {{Keep}} the {{Con}} out of {{Experimental Econ}}.},
  booktitle = {Experimental {{Economics}}},
  author = {Roth, Alvin E.},
  editor = {Hey, John D.},
  year = {1994},
  series = {Studies in {{Empirical Economics}}},
  pages = {99--109},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-642-51179-0_6},
  abstract = {When Edward Learner (1983) wrote the well known critique of econometric practice whose title I have adapted and adopted, he was concerned that the credibility and utility of econometric research had suffered because of differences between the way econometric research was conducted and the way it was reported1. He wrote (p36\textendash 37): ``The econometric art as it is practiced at the computer terminal involves fitting many, perhaps thousands, of statistical models. One or several that the researcher finds pleasing are selected for reporting purposes. This searching for a model is often well intentioned, but there can be no doubt that such a specification search invalidates the traditional theories of inference. The concepts of unbiasedness, consistency, efficiency, maximum-likelihood estimation, in fact, all the concepts of traditional theory, utterly lose their meaning by the time an applied researcher pulls from the bramble of computer output the one thorn of a model he likes best, the one he chooses to portray as a rose. The consuming public is hardly fooled by this chicanery.''},
  isbn = {978-3-642-51179-0},
  langid = {english},
  keywords = {Inflated effects,To read},
  file = {/Users/vincentbagilet/Documents/zotero/storage/36NJAYY7/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf;/Users/vincentbagilet/Documents/zotero/storage/HDN3HJHE/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf;/Users/vincentbagilet/Documents/zotero/storage/UL32XMDW/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf;/Users/vincentbagilet/Documents/zotero/storage/Z7LYMBCR/Roth - 1994 - Lets Keep the Con out of Experimental Econ. A Met.pdf}
}

@article{roth_pre-test_2022,
  title = {Pre-Test with {{Caution}}: {{Event-Study Estimates}} after {{Testing}} for {{Parallel Trends}}},
  shorttitle = {Pre-Test with {{Caution}}},
  author = {Roth, Jonathan},
  year = {2022},
  journal = {American Economic Review: Insights},
  doi = {10.1257/aeri.20210236},
  langid = {english},
  keywords = {Example},
  file = {/Users/vincentbagilet/Documents/zotero/storage/J3KD8VRK/Roth - Pre-test with Caution Event-Study Estimates after.pdf;/Users/vincentbagilet/Documents/zotero/storage/LSDSJEUM/Roth - Pre-test with Caution Event-Study Estimates after.pdf;/Users/vincentbagilet/Documents/zotero/storage/M6FE565U/Roth - Pre-test with Caution Event-Study Estimates after.pdf;/Users/vincentbagilet/Documents/zotero/storage/UB49TF67/Roth - Pre-test with Caution Event-Study Estimates after.pdf}
}

@article{rubin_for_2008,
  title = {For Objective Causal Inference, Design Trumps Analysis},
  author = {Rubin, Donald B.},
  year = {2008},
  month = sep,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {3},
  issn = {1932-6157},
  doi = {10.1214/08-AOAS187},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/97JVZL3E/Rubin - 2008 - For objective causal inference, design trumps anal.pdf}
}

@article{rubin_meta-analysis_nodate,
  title = {Meta-{{Analysis}}: {{Literature Synthesis}} or {{Effect-Size Surface Estimation}}?},
  author = {Rubin, Donald B},
  pages = {12},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/N4HHKXKI/Rubin - Meta-Analysis Literature Synthesis or Effect-Size.pdf}
}

@techreport{schell_evaluating_2018,
  title = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}: {{A Simulation Study}}},
  shorttitle = {Evaluating {{Methods}} to {{Estimate}} the {{Effect}} of {{State Laws}} on {{Firearm Deaths}}},
  author = {Schell, Terry L. and Griffin, Beth Ann and Morral, Andrew R.},
  year = {2018},
  month = dec,
  institution = {{RAND Corporation}},
  abstract = {The authors use simulations to assess the performance of a wide range of statistical models commonly used in the gun policy literature to estimate the effects of state-level gun policies on firearm deaths and to identify the most-appropriate statistical methods for producing estimates. The results suggest substantial statistical problems with many of the methods used in this field. The authors identify the best method among those assessed.},
  langid = {english},
  keywords = {Event study},
  file = {/Users/vincentbagilet/Documents/zotero/storage/2T7H52U5/Schell et al. - 2018 - Evaluating Methods to Estimate the Effect of State.pdf}
}

@article{schochet_statistical_2021,
  title = {Statistical {{Power}} for {{Estimating Treatment Effects Using Difference-in-Differences}} and {{Comparative Interrupted Time Series Designs}} with {{Variation}} in {{Treatment Timing}}},
  author = {Schochet, Peter Z.},
  year = {2021},
  month = oct,
  journal = {arXiv:2102.06770 [econ, stat]},
  eprint = {2102.06770},
  eprinttype = {arxiv},
  primaryclass = {econ, stat},
  abstract = {This article develops new closed-form variance expressions for power analyses for commonly used difference-in-differences (DID) and comparative interrupted time series (CITS) panel data estimators. The main contribution is to incorporate variation in treatment timing into the analysis. The power formulas also account for other key design features that arise in practice: autocorrelated errors, unequal measurement intervals, and clustering due to the unit of treatment assignment. We consider power formulas for both cross-sectional and longitudinal models and allow for covariates. An illustrative power analysis provides guidance on appropriate sample sizes. The key finding is that accounting for treatment timing increases required sample sizes. Further, DID estimators have considerably more power than standard CITS and ITS estimators. An available Shiny R dashboard performs the sample size calculations for the considered estimators.},
  archiveprefix = {arXiv},
  keywords = {DID},
  file = {/Users/vincentbagilet/Documents/zotero/storage/5G7BVMKY/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf;/Users/vincentbagilet/Documents/zotero/storage/UN9XBX25/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf;/Users/vincentbagilet/Documents/zotero/storage/X9C4ENB3/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf;/Users/vincentbagilet/Documents/zotero/storage/ZRUQZSZC/Schochet - 2021 - Statistical Power for Estimating Treatment Effects.pdf}
}

@book{shadish_experimental_2002,
  title = {Experimental and {{Quasi-experimental Designs}} for {{Generalized Causal Inference}}},
  author = {Shadish, William R. and Cook, Thomas D. and Campbell, Donald Thomas},
  year = {2002},
  publisher = {{Houghton Mifflin}},
  abstract = {This long awaited successor of the original Cook/Campbell Quasi-Experimentation: Design and Analysis Issues for Field Settings represents updates in the field over the last two decades. The book covers four major topics in field experimentation:Theoretical matters: Experimentation, causation, and validityQuasi-experimental design: Regression discontinuity designs, interrupted time series designs, quasi-experimental designs that use both pretests and control groups, and other designsRandomized experiments: Logic and design issues, and practical problems involving ethics, recruitment, assignment, treatment implementation, and attritionGeneralized causal inference: A grounded theory of generalized causal inference, along with methods for implementing that theory in single and multiple studies},
  googlebooks = {o7jaAAAAMAAJ},
  isbn = {978-0-395-61556-0},
  langid = {english},
  keywords = {Philosophy / Epistemology,Psychology / Experimental Psychology,Psychology / General}
}

@article{stommes_reliability_2021,
  title = {On the Reliability of Published Findings Using the Regression Discontinuity Design in Political Science},
  author = {Stommes, Drew and Aronow, P. M. and S{\"a}vje, Fredrik},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.14526 [stat]},
  eprint = {2109.14526},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it the position as a standard method in modern political science research. But identification does not necessarily imply that the causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation is particularly challenging with the RD design and investigate how these challenges manifest themselves in the empirical literature. We collect all RD-based findings published in top political science journals from 2009\textendash 2018. The findings exhibit pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher's discretion is not a major driver of these pathological features, but researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design are exaggerated, if not entirely spurious.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {RDD},
  file = {/Users/vincentbagilet/Documents/zotero/storage/CFYJYNF8/Stommes et al. - 2021 - On the reliability of published findings using the.pdf}
}

@article{sukhtankar_replications_2017,
  title = {Replications in {{Development Economics}}},
  author = {Sukhtankar, Sandip},
  year = {2017},
  month = may,
  journal = {American Economic Review},
  volume = {107},
  number = {5},
  pages = {32--36},
  issn = {0002-8282},
  doi = {10.1257/aer.p20171120},
  abstract = {I examine replications of empirical papers in development economics published in the top-5 and next-5 general interest journals between the years 2000 through 2015. Of the 1,138 empirical papers, 71 papers (6.2 percent) were replicated in another published paper or working paper. The majority (77.5 percent) of replications involved reanalysis of the data using different econometric specifications to assess robustness. The strongest predictor of whether a paper is replicated or not is the paper's Google Scholar citation count, followed by year of publication. Papers based on randomized control trials (RCTs) appear to be replicated at a higher rate (12.5 percent).},
  langid = {english},
  keywords = {Development economics,Replications},
  file = {/Users/vincentbagilet/Documents/zotero/storage/CRET9PU8/Sukhtankar - 2017 - Replications in Development Economics.pdf;/Users/vincentbagilet/Documents/zotero/storage/F4WPRIVK/Sukhtankar - 2017 - Replications in Development Economics.pdf;/Users/vincentbagilet/Documents/zotero/storage/F6IE7PPG/Sukhtankar - 2017 - Replications in Development Economics.pdf;/Users/vincentbagilet/Documents/zotero/storage/WE9F5RRR/Sukhtankar - 2017 - Replications in Development Economics.pdf}
}

@article{thistlethwaite_regression-discontinuity_1960,
  title = {Regression-Discontinuity Analysis: {{An}} Alternative to the Ex Post Facto Experiment},
  shorttitle = {Regression-Discontinuity Analysis},
  author = {Thistlethwaite, Donald L. and Campbell, Donald T.},
  year = {1960},
  journal = {Journal of Educational Psychology},
  volume = {51},
  number = {6},
  pages = {309--317},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-2176(Electronic),0022-0663(Print)},
  doi = {10.1037/h0044319},
  abstract = {This study presents a method of testing casual hypotheses, called regression-discontinuity analysis, in situations where the investigator is unable to randomly assign Ss to experimental and control groups. The Ss were selected from near winners\textemdash 5126 students who received certificates of merit and 2848 students who merely received letters of commendation. Comparison of the results obtained from the new mode of analysis with those obtained when the ex post facto design was applied to the same data. The new analysis suggested that public recognition for achievement tends to increase the likelihood that the recipient will receive a scholarship but did not support the inference that recognition affects the student's attitudes and career plans. From Psyc Abstracts 36:01:1AF09T. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Award,Education,RDD},
  file = {/Users/vincentbagilet/Documents/zotero/storage/6QQ5453R/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/vincentbagilet/Documents/zotero/storage/G4EIYIDL/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/vincentbagilet/Documents/zotero/storage/LW9TV5BQ/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf;/Users/vincentbagilet/Documents/zotero/storage/VS3AC93I/Thistlethwaite and Campbell - 1960 - Regression-discontinuity analysis An alternative .pdf}
}

@misc{timm_retrodesign_2019,
  title = {Retrodesign: {{Tools}} for {{Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Retrodesign},
  author = {Timm, Andrew and Gelman, Andrew and Carlin, John},
  year = {2019},
  month = mar,
  abstract = {Provides tools for working with Type S (Sign) and Type M (Magnitude) errors, as proposed in Gelman and Tuerlinckx (2000) {$<$}doi.org/10.1007/s001800000040{$>$} and Gelman \& Carlin (2014) {$<$}doi.org/10.1177/1745691614551642{$>$}. In addition to simply calculating the probability of Type S/M error, the package includes functions for calculating these errors across a variety of effect sizes for comparison, and recommended sample size given "tolerances" for Type S/M errors. To improve the speed of these calculations, closed forms solutions for the probability of a Type S/M error from Lu, Qiu, and Deng (2018) {$<$}doi.org/10.1111/bmsp.12132{$>$} are implemented. As of 1.0.0, this includes support only for simple research designs. See the package vignette for a fuller exposition on how Type S/M errors arise in research, and how to analyze them using the type of design analysis proposed in the above papers.},
  copyright = {MIT + file LICENSE}
}

@article{vasishth_how_2021,
  title = {How to Embrace Variation and Accept Uncertainty in Linguistic and Psycholinguistic Data Analysis},
  author = {Vasishth, Shravan and Gelman, Andrew},
  year = {2021},
  month = sep,
  journal = {Linguistics},
  volume = {59},
  number = {5},
  pages = {1311--1342},
  issn = {0024-3949, 1613-396X},
  doi = {10.1515/ling-2019-0051},
  abstract = {The use of statistical inference in linguistics and related areas like psychology typically involves a binary decision: either reject or accept some null hypothesis using statistical significance testing. When statistical power is low, this frequentist data-analytic approach breaks down: null results are uninformative, and effect size estimates associated with significant results are overestimated. Using an example from psycholinguistics, several alternative approaches are demonstrated for reporting inconsistencies between the data and a theoretical prediction. The key here is to focus on committing to a falsifiable prediction, on quantifying uncertainty statistically, and learning to accept the fact that \textendash{} in almost all practical data analysis situations \textendash{} we can only draw uncertain conclusions from data, regardless of whether we manage to obtain statistical significance or not. A focus on uncertainty quantification is likely to lead to fewer excessively bold claims that, on closer investigation, may turn out to be not supported by the data.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/ALNZSQNJ/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf;/Users/vincentbagilet/Documents/zotero/storage/G72L46SU/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf;/Users/vincentbagilet/Documents/zotero/storage/P59HHMZE/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf;/Users/vincentbagilet/Documents/zotero/storage/VJUW5ANZ/Vasishth and Gelman - 2021 - How to embrace variation and accept uncertainty in.pdf}
}

@article{wasserstein_asa_2016,
  title = {The {{ASA Statement}} on {\emph{p}} -{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on {\emph{p}} -{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1154108},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/FA9UWANV/Wasserstein and Lazar - 2016 - The ASA Statement on p -Values Context, Pr.pdf}
}

@article{wasserstein_moving_2019,
  title = {Moving to a {{World Beyond}} `` {\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/BHELXIVT/Wasserstein et al. - 2019 - Moving to a World Beyond “ p  0.05”.pdf}
}

@article{young_channelling_nodate,
  title = {{{CHANNELLING FISHER}}: {{RANDOMIZATION TESTS AND THE STATISTICAL INSIGNIFICANCE OF SEEMINGLY SIGNIFICANT EXPERIMENTAL RESULTS}}},
  author = {Young, Alwyn},
  pages = {47},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/GQ9MV9A9/Young - CHANNELLING FISHER RANDOMIZATION TESTS AND THE ST.pdf}
}

@article{young_leverage_2021,
  title = {Leverage, {{Heteroskedasticity}} and {{Instrumental Variables}} in {{Practical Application}}},
  author = {Young, Alwyn},
  year = {2021},
  month = jun,
  pages = {43},
  abstract = {I use Monte Carlo simulations, the jackknife and multiple forms of the bootstrap to study a comprehensive sample of 1359 instrumental variables regressions in 31 papers published in the journals of the American Economic Association. Monte Carlo simulations based upon published regressions show that non-iid error processes in highly leveraged regressions, both prominent features of published work, adversely affect the size and power of IV tests, while increasing the bias of IV relative to OLS. Weak instrument pre-tests based upon F-statistics are found to be largely uninformative of both size and bias. In published papers, statistically significant IV results often depend upon only one or two observations or clusters, IV has little power as, despite producing substantively different estimates, it rarely rejects the OLS point estimate or the null that OLS is unbiased, while the statistical significance of excluded instruments is exaggerated.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/N4QMT2UY/Young - Leverage, Heteroskedasticity and Instrumental Vari.pdf}
}

@article{zwet_proposal_2021,
  title = {A {{Proposal}} for {{Informative Default Priors Scaled}} by the {{Standard Error}} of {{Estimates}}},
  author = {Zwet, Erik and Gelman, Andrew},
  year = {2021},
  month = jul,
  journal = {The American Statistician},
  pages = {1--9},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.1938225},
  abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization or ``shrinkage.'' To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or ``corpus'' of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/53DP6UEQ/Zwet and Gelman - 2021 - A Proposal for Informative Default Priors Scaled b.pdf;/Users/vincentbagilet/Documents/zotero/storage/IG937C2Z/2344179.pdf;/Users/vincentbagilet/Documents/zotero/storage/T4LG9C2B/Guido W. Imbens, Donald B. Rubin - Causal Inference for Statistics, Social, and Biomedical Sciences_ An Introduction-Cambridge University Press (2015).pdf}
}

@article{zwet_proposal_2022,
  title = {A {{Proposal}} for {{Informative Default Priors Scaled}} by the {{Standard Error}} of {{Estimates}}},
  author = {van Zwet, Erik and Gelman, Andrew},
  year = {2022},
  month = jan,
  journal = {The American Statistician},
  volume = {76},
  number = {1},
  pages = {1--9},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2021.1938225},
  abstract = {If we have an unbiased estimate of some parameter of interest, then its absolute value is positively biased for the absolute value of the parameter. This bias is large when the signal-to-noise ratio (SNR) is small, and it becomes even larger when we condition on statistical significance; the winner's curse. This is a frequentist motivation for regularization or ``shrinkage.'' To determine a suitable amount of shrinkage, we propose to estimate the distribution of the SNR from a large collection or ``corpus'' of similar studies and use this as a prior distribution. The wider the scope of the corpus, the less informative the prior, but a wider scope does not necessarily result in a more diffuse prior. We show that the estimation of the prior simplifies if we require that posterior inference is equivariant under linear transformations of the data. We demonstrate our approach with corpora of 86 replication studies from psychology and 178 phase 3 clinical trials. Our suggestion is not intended to be a replacement for a prior based on full information about a particular problem; rather, it represents a familywise choice that should yield better long-term properties than the current default uniform prior, which has led to systematic overestimates of effect sizes and a replication crisis when these inflated estimates have not shown up in later studies.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/AP2ACJWQ/Zwet and Gelman - 2022 - A Proposal for Informative Default Priors Scaled b.pdf}
}

@article{zwet_significance_2021,
  title = {The Significance Filter, the Winner's Curse and the Need to Shrink},
  author = {Zwet, Erik W. and Cator, Eric A.},
  year = {2021},
  month = nov,
  journal = {Statistica Neerlandica},
  volume = {75},
  number = {4},
  pages = {437--452},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/stan.12241},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/Z7PIKK8N/Zwet and Cator - 2021 - The significance filter, the winner's curse and th.pdf}
}

@article{zwet_statistical_2021,
  title = {The Statistical Properties of {{RCTs}} and a Proposal for Shrinkage},
  author = {Zwet, Erik and Schwab, Simon and Senn, Stephen},
  year = {2021},
  month = nov,
  journal = {Statistics in Medicine},
  volume = {40},
  number = {27},
  pages = {6107--6117},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.9173},
  abstract = {We abstract the concept of a randomized controlled trial as a triple ({$\mathsl{B}$}, b, s), where {$\mathsl{B}$} is the primary efficacy parameter, b the estimate, and s the standard error (s {$>$} 0). If the parameter {$\mathsl{B}$} is either a difference of means, a log odds ratio or a log hazard ratio, then it is reasonable to assume that b is unbiased and normally distributed. This then allows us to estimate the joint distribution of the z-value z = b/s and the signal-to-noise ratio SNR = {$\mathsl{B}$}/s from a sample of pairs (bi, si). We have collected 23 551 such pairs from the Cochrane database. We note that there are many statistical quantities that depend on ({$\mathsl{B}$}, b, s) only through the pair (z, SNR). We start by determining the estimated distribution of the achieved power. In particular, we estimate the median achieved power to be only 13\%. We also consider the exaggeration ratio which is the factor by which the magnitude of {$\mathsl{B}$} is overestimated. We find that if the estimate is just significant at the 5\% level, we would expect it to overestimate the true effect by a factor of 1.7. This exaggeration is sometimes referred to as the winner's curse and it is undoubtedly to a considerable extent responsible for disappointing replication results. For this reason, we believe it is important to shrink the unbiased estimator, and we propose a method for doing so. We show that our shrinkage estimator successfully addresses the exaggeration. As an example, we re-analyze the ANDROMEDA-SHOCK trial.},
  langid = {english},
  file = {/Users/vincentbagilet/Documents/zotero/storage/DG9VXP5B/Zwet et al. - 2021 - The statistical properties of RCTs and a proposal .pdf}
}

@article{ho2007matching,
  title={Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference},
  author={Ho, Daniel E and Imai, Kosuke and King, Gary and Stuart, Elizabeth A},
  journal={Political analysis},
  volume={15},
  number={3},
  pages={199--236},
  year={2007},
  publisher={Cambridge University Press}
}

@article{rubin2001using,
  title={Using propensity scores to help design observational studies: application to the tobacco litigation},
  author={Rubin, Donald B},
  journal={Health Services and Outcomes Research Methodology},
  volume={2},
  number={3},
  pages={169--188},
  year={2001},
  publisher={Springer}
}

@article{kraft2020interpreting,
  title={Interpreting effect sizes of education interventions},
  author={Kraft, Matthew A},
  journal={Educational Researcher},
  volume={49},
  number={4},
  pages={241--253},
  year={2020},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{fujiwara2016habit,
  title={Habit formation in voting: Evidence from rainy elections},
  author={Fujiwara, Thomas and Meng, Kyle and Vogl, Tom},
  journal={American Economic Journal: Applied Economics},
  volume={8},
  number={4},
  pages={160--88},
  year={2016}
}

@article{cooperman2017randomization,
  title={Randomization inference with rainfall data: Using historical weather patterns for variance estimation},
  author={Cooperman, Alicia Dailey},
  journal={Political Analysis},
  volume={25},
  number={3},
  pages={277--288},
  year={2017},
  publisher={Cambridge University Press}
}

@article{gomez2007republicans,
  title={The Republicans should pray for rain: Weather, turnout, and voting in US presidential elections},
  author={Gomez, Brad T and Hansford, Thomas G and Krause, George A},
  journal={The Journal of Politics},
  volume={69},
  number={3},
  pages={649--663},
  year={2007},
  publisher={Cambridge University Press New York, USA}
}

@article{currie2015environmental,
  title={Environmental health risks and housing values: evidence from 1,600 toxic plant openings and closings},
  author={Currie, Janet and Davis, Lucas and Greenstone, Michael and Walker, Reed},
  journal={American Economic Review},
  volume={105},
  number={2},
  pages={678--709},
  year={2015}
}